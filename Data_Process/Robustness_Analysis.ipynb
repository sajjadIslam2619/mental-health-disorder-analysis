{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd9d742",
   "metadata": {},
   "source": [
    "\n",
    "#### Robustness eval (no sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80532c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nipua\\AppData\\Local\\anaconda3\\envs\\py312_gpu_2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a244b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# config (edit this list!)\n",
    "# ----------------------------\n",
    "model_dirs = [\n",
    "    \"Data_Warehouse/mental_health_splits_with_stress/all_roberta_large_v1_with_stress/best\",\n",
    "    \"Data_Warehouse/mental_health_splits_with_stress/mental_roberta_base_with_stress/best\",\n",
    "    \"Data_Warehouse/mental_health_splits_no_stress/all_roberta_large_v1_multiclass/best\",\n",
    "    \"Data_Warehouse/mental_health_splits_no_stress/mental_roberta_base_no_stress/best\",\n",
    "]\n",
    "# Point this to the dataset you want to evaluate (e.g., 50/50 sample)\n",
    "#dataset_csv = \"Data_Warehouse/erisk_task2_sample_50_50.csv\"\n",
    "#dataset_csv = \"Data_Warehouse/erisk_task2_sample_100_100.csv\"\n",
    "#dataset_csv = \"Data_Warehouse/erisk_task2_sample_200_200.csv\"\n",
    "#dataset_csv = \"Data_Warehouse/erisk_task2_sample_50_250.csv\"\n",
    "dataset_csv = \"Data_Warehouse/erisk_task2_userlevel_50_50.csv\"\n",
    "batch_size = 16\n",
    "max_length = 512\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d758741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: 100 rows | class counts:\n",
      "label\n",
      "depression        50\n",
      "non depression    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Evaluating: Data_Warehouse/mental_health_splits_with_stress/all_roberta_large_v1_with_stress/best\n",
      "accuracy=0.6300  precision(dep)=0.5890  recall(dep)=0.8600  f1(dep)=0.6992  auc(dep)=0.7639999999999999\n",
      "confusion matrix rows [dep, non]:\n",
      "[[43  7]\n",
      " [30 20]]\n",
      "\n",
      "Label-wise metrics (per class):\n",
      "                precision  recall      f1  support  roc_auc\n",
      "non depression     0.7407    0.40  0.5195     50.0    0.764\n",
      "depression         0.5890    0.86  0.6992     50.0    0.764\n",
      "\n",
      "Overall metrics:\n",
      "macro  - precision=0.6649 recall=0.6300 f1=0.6093\n",
      "weighted - precision=0.6649 recall=0.6300 f1=0.6093\n",
      "balanced_accuracy=0.6300\n",
      "\n",
      "Evaluating: Data_Warehouse/mental_health_splits_with_stress/mental_roberta_base_with_stress/best\n",
      "accuracy=0.5800  precision(dep)=0.5513  recall(dep)=0.8600  f1(dep)=0.6719  auc(dep)=0.716\n",
      "confusion matrix rows [dep, non]:\n",
      "[[43  7]\n",
      " [35 15]]\n",
      "\n",
      "Label-wise metrics (per class):\n",
      "                precision  recall      f1  support  roc_auc\n",
      "non depression     0.6818    0.30  0.4167     50.0    0.716\n",
      "depression         0.5513    0.86  0.6719     50.0    0.716\n",
      "\n",
      "Overall metrics:\n",
      "macro  - precision=0.6166 recall=0.5800 f1=0.5443\n",
      "weighted - precision=0.6166 recall=0.5800 f1=0.5443\n",
      "balanced_accuracy=0.5800\n",
      "\n",
      "Evaluating: Data_Warehouse/mental_health_splits_no_stress/all_roberta_large_v1_multiclass/best\n",
      "accuracy=0.5900  precision(dep)=0.5542  recall(dep)=0.9200  f1(dep)=0.6917  auc(dep)=0.7792\n",
      "confusion matrix rows [dep, non]:\n",
      "[[46  4]\n",
      " [37 13]]\n",
      "\n",
      "Label-wise metrics (per class):\n",
      "                precision  recall      f1  support  roc_auc\n",
      "non depression     0.7647    0.26  0.3881     50.0   0.7792\n",
      "depression         0.5542    0.92  0.6917     50.0   0.7792\n",
      "\n",
      "Overall metrics:\n",
      "macro  - precision=0.6595 recall=0.5900 f1=0.5399\n",
      "weighted - precision=0.6595 recall=0.5900 f1=0.5399\n",
      "balanced_accuracy=0.5900\n",
      "\n",
      "Evaluating: Data_Warehouse/mental_health_splits_no_stress/mental_roberta_base_no_stress/best\n",
      "accuracy=0.5900  precision(dep)=0.5556  recall(dep)=0.9000  f1(dep)=0.6870  auc(dep)=0.7216\n",
      "confusion matrix rows [dep, non]:\n",
      "[[45  5]\n",
      " [36 14]]\n",
      "\n",
      "Label-wise metrics (per class):\n",
      "                precision  recall      f1  support  roc_auc\n",
      "non depression     0.7368    0.28  0.4058     50.0   0.7216\n",
      "depression         0.5556    0.90  0.6870     50.0   0.7216\n",
      "\n",
      "Overall metrics:\n",
      "macro  - precision=0.6462 recall=0.5900 f1=0.5464\n",
      "weighted - precision=0.6462 recall=0.5900 f1=0.5464\n",
      "balanced_accuracy=0.5900\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, roc_auc_score, classification_report, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# data utilities\n",
    "# ----------------------------\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = list(texts)\n",
    "        self.tok = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(\n",
    "            str(self.texts[i]),\n",
    "            truncation=True,\n",
    "            padding=False,   # dynamic padding via collator\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    return \"depression\" if str(s).strip().lower() == \"depression\" else \"non depression\"\n",
    "\n",
    "def to_binary_int(lbl: str) -> int:\n",
    "    return 1 if normalize_label(lbl) == \"depression\" else 0\n",
    "\n",
    "# ----------------------------\n",
    "# inference utilities\n",
    "# ----------------------------\n",
    "def find_depression_index(model) -> int:\n",
    "    id2label = getattr(model.config, \"id2label\", None) or {i: f\"label_{i}\" for i in range(model.config.num_labels)}\n",
    "    id2label_norm = {int(k): str(v).strip().lower() for k, v in id2label.items()}\n",
    "    for i, name in id2label_norm.items():\n",
    "        if name == \"depression\":\n",
    "            return int(i)\n",
    "    if model.config.num_labels > 1:\n",
    "        print(\"warning: 'depression' not found in id2label; falling back to index 1\")\n",
    "        return 1\n",
    "    raise ValueError(f\"Could not resolve 'depression' class from id2label: {id2label}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(model, tokenizer, texts, batch_size=16, max_length=512, device=\"cpu\"):\n",
    "    ds = PostDataset(texts, tokenizer, max_length=max_length)\n",
    "    collator = DataCollatorWithPadding(\n",
    "        tokenizer,\n",
    "        pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "\n",
    "    logits_list = []\n",
    "    model.eval()\n",
    "    for batch in dl:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        logits_list.append(out.logits.detach().cpu())\n",
    "\n",
    "    logits = torch.cat(logits_list, dim=0).numpy()\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    preds = probs.argmax(axis=1)\n",
    "    return preds, probs\n",
    "\n",
    "def evaluate_binary(y_true, y_pred, p_dep=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0]).astype(int)\n",
    "    auc = None\n",
    "    if p_dep is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, p_dep)\n",
    "        except Exception:\n",
    "            auc = None\n",
    "    return acc, prec, rec, f1, auc, cm\n",
    "\n",
    "def overall_metrics(y_true, y_pred):\n",
    "    \"\"\"Macro/weighted P/R/F1 and balanced accuracy.\"\"\"\n",
    "    out = {}\n",
    "    for avg in [\"macro\", \"weighted\"]:\n",
    "        p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average=avg, zero_division=0)\n",
    "        out[f\"precision_{avg}\"] = p\n",
    "        out[f\"recall_{avg}\"] = r\n",
    "        out[f\"f1_{avg}\"] = f\n",
    "    out[\"balanced_accuracy\"] = balanced_accuracy_score(y_true, y_pred)\n",
    "    return out\n",
    "\n",
    "def labelwise_report(y_true, y_pred, p_dep=None):\n",
    "    \"\"\"\n",
    "    Returns a small dataframe with per-class precision, recall, f1, support,\n",
    "    and (if probs provided) per-class ROC-AUC.\n",
    "    Classes shown: ['non depression', 'depression'] in that order.\n",
    "    \"\"\"\n",
    "    target_names = ['non depression', 'depression']  # map 0 -> non dep, 1 -> dep\n",
    "    rep = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=target_names,\n",
    "        digits=4, zero_division=0, output_dict=True\n",
    "    )\n",
    "    df_rep = pd.DataFrame(rep).T.loc[target_names, [\"precision\", \"recall\", \"f1-score\", \"support\"]]\n",
    "    df_rep = df_rep.rename(columns={\"f1-score\": \"f1\"})\n",
    "    # add per-class AUC if probs provided\n",
    "    if p_dep is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            auc_dep = roc_auc_score(y_true, p_dep)  # positive=depression (1)\n",
    "            auc_non = roc_auc_score(1 - y_true, 1 - p_dep)  # treat non-dep as positive\n",
    "            df_rep.loc[\"depression\", \"roc_auc\"] = float(auc_dep)\n",
    "            df_rep.loc[\"non depression\", \"roc_auc\"] = float(auc_non)\n",
    "        except Exception:\n",
    "            df_rep[\"roc_auc\"] = np.nan\n",
    "    else:\n",
    "        df_rep[\"roc_auc\"] = np.nan\n",
    "    return df_rep\n",
    "\n",
    "# ----------------------------\n",
    "# main logic (notebook style)\n",
    "# ----------------------------\n",
    "# (keep your existing config vars set above this block)\n",
    "# model_dirs = [...]\n",
    "# dataset_csv = ...\n",
    "# batch_size, max_length, seed = ...\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "df = pd.read_csv(dataset_csv)\n",
    "if not {\"text\", \"label\"}.issubset(df.columns):\n",
    "    raise ValueError(\"dataset_csv must contain columns: text,label\")\n",
    "\n",
    "df[\"label\"] = df[\"label\"].map(normalize_label)\n",
    "texts = df[\"text\"].astype(str).tolist()\n",
    "y_true = np.array([to_binary_int(x) for x in df[\"label\"]])\n",
    "print(f\"Loaded dataset: {len(df)} rows | class counts:\\n{df['label'].value_counts()}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for mdir in model_dirs:\n",
    "    print(\"\\nEvaluating:\", mdir)\n",
    "    tok = AutoTokenizer.from_pretrained(mdir, use_fast=True)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(mdir).to(device)\n",
    "    mdl.eval()\n",
    "\n",
    "    dep_idx = find_depression_index(mdl)\n",
    "    yhat_ids, probs = infer(mdl, tok, texts, batch_size=batch_size, max_length=max_length, device=device)\n",
    "    y_pred = np.array([1 if int(i) == dep_idx else 0 for i in yhat_ids])\n",
    "    p_dep = probs[:, dep_idx]\n",
    "\n",
    "    acc, prec, rec, f1, auc, cm = evaluate_binary(y_true, y_pred, p_dep)\n",
    "    print(f\"accuracy={acc:.4f}  precision(dep)={prec:.4f}  recall(dep)={rec:.4f}  f1(dep)={f1:.4f}  auc(dep)={auc}\")\n",
    "    print(\"confusion matrix rows [dep, non]:\")\n",
    "    print(cm)\n",
    "\n",
    "    # ---- per-class table ----\n",
    "    lw = labelwise_report(y_true, y_pred, p_dep)\n",
    "    print(\"\\nLabel-wise metrics (per class):\")\n",
    "    print(lw.round(4).to_string())\n",
    "\n",
    "    # ---- overall metrics (macro/weighted) ----\n",
    "    om = overall_metrics(y_true, y_pred)\n",
    "    print(\"\\nOverall metrics:\")\n",
    "    print(\n",
    "        \"macro  - precision={:.4f} recall={:.4f} f1={:.4f}\\n\"\n",
    "        \"weighted - precision={:.4f} recall={:.4f} f1={:.4f}\\n\"\n",
    "        \"balanced_accuracy={:.4f}\".format(\n",
    "            om[\"precision_macro\"], om[\"recall_macro\"], om[\"f1_macro\"],\n",
    "            om[\"precision_weighted\"], om[\"recall_weighted\"], om[\"f1_weighted\"],\n",
    "            om[\"balanced_accuracy\"]\n",
    "        )\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_gpu_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
