{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ccb0e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4696 rows from d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\combined_mental_condition_dataset_no_stress.csv\n",
      "Dropped 1 empty text or label rows\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "depression    2322\n",
      "suicide        838\n",
      "none           706\n",
      "anxiety        416\n",
      "ptsd           414\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample rows:\n",
      "                                                   text       label\n",
      "1550  Older female fighting depression\\nHi I’m a 54 ...  depression\n",
      "3801  This weekend was terrible, and I guess I wante...        ptsd\n",
      "3730  Its hard knowing that everyone around you does...        ptsd\n",
      "584   I am Mark, this is my first post on here. I am...  depression\n",
      "2240  I'm rewatching Bojack Horseman on Netflix for ...  depression\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== config =====\n",
    "DATASET_NAME = \"combined_mental_condition_dataset_no_stress.csv\"\n",
    "\n",
    "# Option A: set this if you know the path\n",
    "# data_warehouse = Path(r\"/absolute/path/to/Data_Warehouse\")\n",
    "\n",
    "# Option B: auto locate Data_Warehouse by walking up from current working folder\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate a folder named Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent  # running as a script\n",
    "    except NameError:\n",
    "        script_dir = Path.cwd()                       # running in a notebook\n",
    "    data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ===== load =====\n",
    "data_path = data_warehouse / DATASET_NAME\n",
    "df_ns = pd.read_csv(data_path)\n",
    "\n",
    "# ===== basic checks =====\n",
    "required = {\"text\", \"label\"}\n",
    "missing = required - set(df_ns.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# coerce types and clean minimal issues\n",
    "df_ns[\"text\"] = df_ns[\"text\"].astype(str)\n",
    "df_ns[\"label\"] = df_ns[\"label\"].astype(str).str.strip()\n",
    "\n",
    "# drop empty text or label rows if any\n",
    "before = len(df_ns)\n",
    "df_ns = df_ns[(df_ns[\"text\"].str.strip() != \"\") & (df_ns[\"label\"].str.strip() != \"\")]\n",
    "dropped = before - len(df_ns)\n",
    "\n",
    "# sanity check for stress leakage\n",
    "if df_ns[\"label\"].str.lower().eq(\"stress\").any():\n",
    "    raise ValueError(\"Found label 'stress' in the no stress dataset. Please verify the input file.\")\n",
    "\n",
    "print(f\"Loaded {len(df_ns)} rows from {data_path}\")\n",
    "if dropped:\n",
    "    print(f\"Dropped {dropped} empty text or label rows\")\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df_ns[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(df_ns.sample(min(5, len(df_ns)), random_state=42)[[\"text\", \"label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a978384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes and ids (forced 'none' → 4):\n",
      "anxiety -> 0\n",
      "depression -> 1\n",
      "ptsd -> 2\n",
      "suicide -> 3\n",
      "none -> 4\n",
      "\n",
      "Final split sizes\n",
      "Train: 3756  Validation: 470  Test: 470\n",
      "\n",
      "Train size: 3756\n",
      "label_enc\n",
      "0     332\n",
      "1    1858\n",
      "2     332\n",
      "3     670\n",
      "4     564\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation size: 470\n",
      "label_enc\n",
      "0     42\n",
      "1    232\n",
      "2     41\n",
      "3     84\n",
      "4     71\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test size: 470\n",
      "label_enc\n",
      "0     42\n",
      "1    232\n",
      "2     41\n",
      "3     84\n",
      "4     71\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved splits and label mapping to D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_no_stress\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===== config =====\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.10\n",
    "VAL_SIZE = 0.10   # portion of the full dataset\n",
    "\n",
    "# ===== input =====\n",
    "# df_ns is assumed to exist with columns [\"text\", \"label\"]\n",
    "\n",
    "# ===== manual encode labels =====\n",
    "unique_labels = sorted([lbl for lbl in df_ns[\"label\"].unique() if lbl.lower() != \"none\"])\n",
    "\n",
    "# Assign 0–3 to all labels except \"none\"\n",
    "class_to_id = {lbl: i for i, lbl in enumerate(unique_labels)}\n",
    "\n",
    "# Force \"none\" → 4\n",
    "class_to_id[\"none\"] = 4\n",
    "\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n",
    "\n",
    "# Map labels to ids\n",
    "df_ns = df_ns.copy()\n",
    "df_ns[\"label_enc\"] = df_ns[\"label\"].str.lower().map(class_to_id)\n",
    "\n",
    "print(\"Classes and ids (forced 'none' → 4):\")\n",
    "for k, v in class_to_id.items():\n",
    "    print(f\"{k} -> {v}\")\n",
    "\n",
    "# ===== split 80 10 10 with stratify =====\n",
    "df_trainval, df_test = train_test_split(\n",
    "    df_ns,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=df_ns[\"label_enc\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "val_size_relative = VAL_SIZE / (1.0 - TEST_SIZE)  # 0.10 / 0.90 = 0.111...\n",
    "df_train, df_val = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=val_size_relative,\n",
    "    stratify=df_trainval[\"label_enc\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# ===== quick checks =====\n",
    "def show_split_stats(name, frame):\n",
    "    print(f\"\\n{name} size: {len(frame)}\")\n",
    "    print(frame[\"label_enc\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nFinal split sizes\")\n",
    "print(f\"Train: {len(df_train)}  Validation: {len(df_val)}  Test: {len(df_test)}\")\n",
    "show_split_stats(\"Train\", df_train)\n",
    "show_split_stats(\"Validation\", df_val)\n",
    "show_split_stats(\"Test\", df_test)\n",
    "\n",
    "# ===== save artifacts =====\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    data_warehouse = Path.cwd()\n",
    "\n",
    "out_dir = data_warehouse / \"mental_health_splits_no_stress\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_train.to_csv(out_dir / \"train.csv\", index=False)\n",
    "df_val.to_csv(out_dir / \"val.csv\", index=False)\n",
    "df_test.to_csv(out_dir / \"test.csv\", index=False)\n",
    "\n",
    "pd.Series(class_to_id).to_csv(out_dir / \"label_classes.csv\")\n",
    "\n",
    "print(f\"\\nSaved splits and label mapping to {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a013d5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [07:50<00:00, 31.35s/it]\n",
      "Batches: 100%|██████████| 2/2 [01:05<00:00, 33.00s/it]\n",
      "Batches: 100%|██████████| 2/2 [01:01<00:00, 30.89s/it]\n",
      "c:\\Users\\nipua\\AppData\\Local\\anaconda3\\envs\\py312_xai\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shapes: (3756, 1024) (470, 1024) (470, 1024)\n",
      "Embeddings computed on device: cuda\n",
      "\n",
      "=== Logistic Regression validation ===\n",
      "accuracy: 0.8000\n",
      "precision_macro: 0.8005  precision_weighted: 0.7939\n",
      "recall_macro: 0.7412  recall_weighted: 0.8000\n",
      "f1_macro: 0.7634  f1_weighted: 0.7899\n",
      "roc_auc_ovr: 0.9392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8056    0.6905    0.7436        42\n",
      "           1     0.7836    0.9052    0.8400       232\n",
      "           2     0.7647    0.6341    0.6933        41\n",
      "           3     0.7018    0.4762    0.5674        84\n",
      "           4     0.9467    1.0000    0.9726        71\n",
      "\n",
      "    accuracy                         0.8000       470\n",
      "   macro avg     0.8005    0.7412    0.7634       470\n",
      "weighted avg     0.7939    0.8000    0.7899       470\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 29  10   2   1   0]\n",
      " [  2 210   4  15   1]\n",
      " [  5   8  26   1   1]\n",
      " [  0  40   2  40   2]\n",
      " [  0   0   0   0  71]]\n",
      "\n",
      "=== Logistic Regression test ===\n",
      "accuracy: 0.8021\n",
      "precision_macro: 0.8150  precision_weighted: 0.7998\n",
      "recall_macro: 0.7345  recall_weighted: 0.8021\n",
      "f1_macro: 0.7627  f1_weighted: 0.7890\n",
      "roc_auc_ovr: 0.9513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.5952    0.6494        42\n",
      "           1     0.7714    0.9310    0.8438       232\n",
      "           2     0.8824    0.7317    0.8000        41\n",
      "           3     0.7347    0.4286    0.5414        84\n",
      "           4     0.9722    0.9859    0.9790        71\n",
      "\n",
      "    accuracy                         0.8021       470\n",
      "   macro avg     0.8150    0.7345    0.7627       470\n",
      "weighted avg     0.7998    0.8021    0.7890       470\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 25  14   2   1   0]\n",
      " [  5 216   1  10   0]\n",
      " [  5   5  30   1   0]\n",
      " [  0  45   1  36   2]\n",
      " [  0   0   0   1  70]]\n",
      "\n",
      "=== Linear SVM validation ===\n",
      "accuracy: 0.8064\n",
      "precision_macro: 0.8094  precision_weighted: 0.8016\n",
      "recall_macro: 0.7518  recall_weighted: 0.8064\n",
      "f1_macro: 0.7752  f1_weighted: 0.7991\n",
      "roc_auc_ovr: 0.9346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    0.7143    0.7692        42\n",
      "           1     0.7917    0.9009    0.8427       232\n",
      "           2     0.7647    0.6341    0.6933        41\n",
      "           3     0.6984    0.5238    0.5986        84\n",
      "           4     0.9589    0.9859    0.9722        71\n",
      "\n",
      "    accuracy                         0.8064       470\n",
      "   macro avg     0.8094    0.7518    0.7752       470\n",
      "weighted avg     0.8016    0.8064    0.7991       470\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 30  10   1   1   0]\n",
      " [  2 209   5  15   1]\n",
      " [  4   8  26   2   1]\n",
      " [  0  37   2  44   1]\n",
      " [  0   0   0   1  70]]\n",
      "\n",
      "=== Linear SVM test ===\n",
      "accuracy: 0.7915\n",
      "precision_macro: 0.7789  precision_weighted: 0.7833\n",
      "recall_macro: 0.7289  recall_weighted: 0.7915\n",
      "f1_macro: 0.7484  f1_weighted: 0.7817\n",
      "roc_auc_ovr: 0.9450\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6857    0.5714    0.6234        42\n",
      "           1     0.7865    0.9052    0.8417       232\n",
      "           2     0.7895    0.7317    0.7595        41\n",
      "           3     0.6610    0.4643    0.5455        84\n",
      "           4     0.9718    0.9718    0.9718        71\n",
      "\n",
      "    accuracy                         0.7915       470\n",
      "   macro avg     0.7789    0.7289    0.7484       470\n",
      "weighted avg     0.7833    0.7915    0.7817       470\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 24  12   3   3   0]\n",
      " [  5 210   2  15   0]\n",
      " [  6   4  30   1   0]\n",
      " [  0  41   2  39   2]\n",
      " [  0   0   1   1  69]]\n",
      "\n",
      "Saved models and metrics to D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_no_stress\\baselines_all_roberta_large_v1\n",
      "  split       model  accuracy  precision_macro  precision_weighted  \\\n",
      "0   val      logreg  0.800000         0.800453            0.793911   \n",
      "1   val  linear_svm  0.806383         0.809405            0.801635   \n",
      "2  test      logreg  0.802128         0.814997            0.799766   \n",
      "3  test  linear_svm  0.791489         0.778911            0.783331   \n",
      "\n",
      "   recall_macro  recall_weighted  f1_macro  f1_weighted  roc_auc_ovr  \n",
      "0      0.741197         0.800000  0.763380     0.789897     0.939184  \n",
      "1      0.751804         0.806383  0.775234     0.799072     0.934608  \n",
      "2      0.734493         0.802128  0.762695     0.788951     0.951274  \n",
      "3      0.728885         0.791489  0.748368     0.781723     0.944971  \n"
     ]
    }
   ],
   "source": [
    "# ===== baselines with GPU for embeddings and calibrated SVM for AUC =====\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Optional GPU accelerated models via RAPIDS cuML if available\n",
    "use_cuml = False\n",
    "try:\n",
    "    import cuml\n",
    "    from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
    "    from cuml.svm import SVC as cuSVC\n",
    "    use_cuml = True\n",
    "except Exception:\n",
    "    use_cuml = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# df_train, df_val, df_test are expected to exist with [\"text\", \"label\", \"label_enc\"]\n",
    "assert {\"text\", \"label\", \"label_enc\"}.issubset(df_train.columns)\n",
    "assert {\"text\", \"label\", \"label_enc\"}.issubset(df_val.columns)\n",
    "assert {\"text\", \"label\", \"label_enc\"}.issubset(df_test.columns)\n",
    "\n",
    "# ===== embeddings on GPU if available =====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "st_model = SentenceTransformer(\"sentence-transformers/all-roberta-large-v1\", device=device)\n",
    "\n",
    "def embed_texts(model, texts, batch_size=256, show_progress=True):\n",
    "    return model.encode(\n",
    "        list(texts),\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=show_progress,\n",
    "    )\n",
    "\n",
    "X_train = embed_texts(st_model, df_train[\"text\"])\n",
    "y_train = df_train[\"label_enc\"].to_numpy()\n",
    "\n",
    "X_val   = embed_texts(st_model, df_val[\"text\"])\n",
    "y_val   = df_val[\"label_enc\"].to_numpy()\n",
    "\n",
    "X_test  = embed_texts(st_model, df_test[\"text\"])\n",
    "y_test  = df_test[\"label_enc\"].to_numpy()\n",
    "\n",
    "print(\"Embedding shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Embeddings computed on device:\", device)\n",
    "\n",
    "# ===== helper to evaluate =====\n",
    "def evaluate_model(name, model, X, y_true, proba=None):\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    prec_macro = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    prec_weighted = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    rec_macro = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    rec_weighted = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    auc = None\n",
    "    if proba is not None:\n",
    "        auc = roc_auc_score(y_true, proba, multi_class=\"ovr\")\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"accuracy: {acc:.4f}\")\n",
    "    print(f\"precision_macro: {prec_macro:.4f}  precision_weighted: {prec_weighted:.4f}\")\n",
    "    print(f\"recall_macro: {rec_macro:.4f}  recall_weighted: {rec_weighted:.4f}\")\n",
    "    print(f\"f1_macro: {f1_macro:.4f}  f1_weighted: {f1_weighted:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"roc_auc_ovr: {auc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision_macro\": float(prec_macro),\n",
    "        \"precision_weighted\": float(prec_weighted),\n",
    "        \"recall_macro\": float(rec_macro),\n",
    "        \"recall_weighted\": float(rec_weighted),\n",
    "        \"f1_macro\": float(f1_macro),\n",
    "        \"f1_weighted\": float(f1_weighted),\n",
    "        \"roc_auc_ovr\": float(auc) if auc is not None else None,\n",
    "    }\n",
    "\n",
    "# ===== Logistic Regression =====\n",
    "if use_cuml:\n",
    "    # cuML Logistic Regression uses GPU\n",
    "    logreg = cuLogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=2000,\n",
    "        tol=1e-4,\n",
    "        fit_intercept=True,\n",
    "        multi_class=\"ovr\",  # cuML supports ovr; macro metrics will still be fair\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=0,\n",
    "    )\n",
    "    logreg.fit(X_train, y_train)\n",
    "    # probability estimates\n",
    "    val_proba_lr = logreg.predict_proba(X_val)\n",
    "    test_proba_lr = logreg.predict_proba(X_test)\n",
    "else:\n",
    "    logreg = LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=2000,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    logreg.fit(X_train, y_train)\n",
    "    val_proba_lr = logreg.predict_proba(X_val)\n",
    "    test_proba_lr = logreg.predict_proba(X_test)\n",
    "\n",
    "metrics_val_lr  = evaluate_model(\"Logistic Regression validation\", logreg, X_val, y_val, proba=val_proba_lr)\n",
    "metrics_test_lr = evaluate_model(\"Logistic Regression test\", logreg, X_test, y_test, proba=test_proba_lr)\n",
    "\n",
    "# ===== Linear SVM with probability calibration for valid multiclass AUC =====\n",
    "if use_cuml:\n",
    "    # cuML SVC supports probability=True with Platt scaling on GPU\n",
    "    svm_base = cuSVC(C=1.0, kernel=\"linear\", probability=True, random_state=RANDOM_STATE)\n",
    "    svm_base.fit(X_train, y_train)\n",
    "    val_proba_svm = svm_base.predict_proba(X_val)\n",
    "    test_proba_svm = svm_base.predict_proba(X_test)\n",
    "    # Use the same object for predict\n",
    "    svm_for_pred = svm_base\n",
    "else:\n",
    "    # scikit LinearSVC has no predict_proba\n",
    "    # Calibrate with sigmoid to get probabilities\n",
    "    svm_linear = LinearSVC(C=1.0, random_state=RANDOM_STATE)\n",
    "    svm = CalibratedClassifierCV(svm_linear, method=\"sigmoid\", cv=5)\n",
    "    svm.fit(X_train, y_train)\n",
    "    val_proba_svm = svm.predict_proba(X_val)\n",
    "    test_proba_svm = svm.predict_proba(X_test)\n",
    "    svm_for_pred = svm\n",
    "\n",
    "metrics_val_svm  = evaluate_model(\"Linear SVM validation\", svm_for_pred, X_val, y_val, proba=val_proba_svm)\n",
    "metrics_test_svm = evaluate_model(\"Linear SVM test\", svm_for_pred, X_test, y_test, proba=test_proba_svm)\n",
    "\n",
    "# ===== persist artifacts and summary =====\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    data_warehouse = Path.cwd()\n",
    "\n",
    "art_dir = data_warehouse / \"mental_health_splits_no_stress\" / \"baselines_all_roberta_large_v1\"\n",
    "art_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "if use_cuml:\n",
    "    # cuML models are picklable with joblib\n",
    "    joblib.dump(logreg, art_dir / \"logreg_cuml.joblib\")\n",
    "    joblib.dump(svm_for_pred, art_dir / \"linear_svm_cuml.joblib\")\n",
    "else:\n",
    "    joblib.dump(logreg, art_dir / \"logreg.joblib\")\n",
    "    joblib.dump(svm_for_pred, art_dir / \"linear_svm_calibrated.joblib\")\n",
    "\n",
    "# Save metrics and a compact table for quick comparison\n",
    "metrics = {\n",
    "    \"val_logreg\": metrics_val_lr,\n",
    "    \"test_logreg\": metrics_test_lr,\n",
    "    \"val_linear_svm\": metrics_val_svm,\n",
    "    \"test_linear_svm\": metrics_test_svm,\n",
    "}\n",
    "with open(art_dir / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "summary_rows = []\n",
    "for split in [\"val\", \"test\"]:\n",
    "    for model_name, m in [(\"logreg\", metrics[f\"{split}_logreg\"]), (\"linear_svm\", metrics[f\"{split}_linear_svm\"])]:\n",
    "        summary_rows.append({\n",
    "            \"split\": split,\n",
    "            \"model\": model_name,\n",
    "            **m\n",
    "        })\n",
    "summary = pd.DataFrame(summary_rows)\n",
    "summary.to_csv(art_dir / \"metrics_summary.csv\", index=False)\n",
    "\n",
    "print(f\"\\nSaved models and metrics to {art_dir.resolve()}\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf1c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping used:\n",
      "0: anxiety\n",
      "1: depression\n",
      "2: ptsd\n",
      "3: suicide\n",
      "4: none\n",
      "Torch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: [2.262650489807129, 0.40430569648742676, 2.262650489807129, 1.1211940050125122, 1.3319149017333984]\n",
      "\n",
      "Starting trainer.train() ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_15548\\2998837337.py:199: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = TrainerClass(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [295/295 15:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.837200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.609400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 | epoch 0.02 | loss 1.8306 | lr 0.000020\n",
      "step 50 | epoch 0.85 | loss 1.2600 | lr 0.000017\n",
      "step 100 | epoch 1.70 | loss 0.8372 | lr 0.000013\n",
      "step 150 | epoch 2.54 | loss 0.6094 | lr 0.000010\n",
      "step 200 | epoch 3.39 | loss 0.4380 | lr 0.000007\n",
      "step 250 | epoch 4.24 | loss 0.3510 | lr 0.000003\n",
      "step 295 | epoch 5.00\n",
      "Finished trainer.train()\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics: {'accuracy': 0.723404255319149, 'precision_macro': 0.728296305957712, 'precision_weighted': 0.7620640699864728, 'recall_macro': 0.761778337532907, 'recall_weighted': 0.723404255319149, 'f1_macro': 0.7347998942531804, 'f1_weighted': 0.730514158415949, 'roc_auc_ovr': 0.9333625458178337}\n",
      "Test metrics: {'accuracy': 0.8085106382978723, 'precision_macro': 0.8052368226148715, 'precision_weighted': 0.8268573199943205, 'recall_macro': 0.8187317766988812, 'recall_weighted': 0.8085106382978723, 'f1_macro': 0.8084007414176005, 'f1_weighted': 0.8138389978863159, 'roc_auc_ovr': 0.9601324886755472}\n",
      "\n",
      "Saved full model and metrics to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_no_stress\\bert_large_multiclass\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Fine tune google-bert/bert-large-uncased for multiclass classification\n",
    "# ==========================================\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------- progress visibility & Windows safety ----------\n",
    "os.environ.pop(\"HF_DISABLE_PROGRESS_BARS\", None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "class ConsoleLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        parts = []\n",
    "        if state.global_step is not None: parts.append(f\"step {state.global_step}\")\n",
    "        if state.epoch is not None: parts.append(f\"epoch {state.epoch:.2f}\")\n",
    "        if \"loss\" in logs: parts.append(f\"loss {logs['loss']:.4f}\")\n",
    "        if \"learning_rate\" in logs: parts.append(f\"lr {logs['learning_rate']:.6f}\")\n",
    "        print(\" | \".join(parts), flush=True)\n",
    "\n",
    "# ---------- config ----------\n",
    "RANDOM_STATE = 42\n",
    "MODEL_NAME = \"google-bert/bert-large-uncased\"\n",
    "MAX_LENGTH = 512\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "WD = 0.01\n",
    "TRAIN_BS = 8           # per-device train batch (small for memory)\n",
    "EVAL_BS = 16           # per-device eval batch\n",
    "GRAD_ACCUM_STEPS = 8   # effective batch = 8 * 8 = 64\n",
    "USE_CLASS_WEIGHTS = True\n",
    "\n",
    "# ---------- locate Data_Warehouse ----------\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate a folder named Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    script_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ---------- load splits (already created earlier) ----------\n",
    "split_dir = data_warehouse / \"mental_health_splits_no_stress\"\n",
    "df_train = pd.read_csv(split_dir / \"train.csv\")\n",
    "df_val   = pd.read_csv(split_dir / \"val.csv\")\n",
    "df_test  = pd.read_csv(split_dir / \"test.csv\")\n",
    "\n",
    "# checks\n",
    "for name, df_ in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    req = {\"text\", \"label\", \"label_enc\"}\n",
    "    if not req.issubset(df_.columns):\n",
    "        raise ValueError(f\"{name} split missing required columns: {req}\")\n",
    "\n",
    "num_labels = int(df_train[\"label_enc\"].max()) + 1\n",
    "# build mapping from the split (keeps your 'none' -> 4 if that’s how you saved it)\n",
    "enc_to_label = df_train[[\"label_enc\", \"label\"]].drop_duplicates().sort_values(\"label_enc\")\n",
    "id2label = {int(r.label_enc): str(r.label) for _, r in enc_to_label.iterrows()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(\"Label mapping used:\")\n",
    "for k in sorted(id2label.keys()):\n",
    "    print(f\"{k}: {id2label[k]}\")\n",
    "\n",
    "# ---------- Dataset wrapper with dynamic padding ----------\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df[\"text\"].astype(str).tolist()\n",
    "        self.labels = df[\"label_enc\"].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=False,          # dynamic padding via collator\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# ---------- tokenizer & model ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch device:\", device)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "model.to(device)\n",
    "\n",
    "# ---------- class weights ----------\n",
    "class_weights = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    counts = df_train[\"label_enc\"].value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    # inverse frequency: total / (num_labels * count_c)\n",
    "    weights = total / (num_labels * counts)\n",
    "    class_weights = torch.tensor(weights.to_numpy(), dtype=torch.float, device=device)\n",
    "    print(\"Using class weights:\", [float(x) for x in class_weights])\n",
    "\n",
    "# ---------- custom Trainer with robust compute_loss ----------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels, weight=class_weights) if class_weights is not None else F.cross_entropy(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ---------- datasets & collator ----------\n",
    "train_ds = TextClsDataset(df_train, tokenizer)\n",
    "val_ds   = TextClsDataset(df_val, tokenizer)\n",
    "test_ds  = TextClsDataset(df_test, tokenizer)\n",
    "\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "# ---------- metrics ----------\n",
    "def compute_metrics_from_logits(logits, labels):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    out = {\n",
    "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
    "        \"precision_macro\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"precision_weighted\": float(precision_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"recall_macro\": float(recall_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall_weighted\": float(recall_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"f1_macro\": float(f1_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"f1_weighted\": float(f1_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "    try:\n",
    "        out[\"roc_auc_ovr\"] = float(roc_auc_score(labels, probs, multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        out[\"roc_auc_ovr\"] = None\n",
    "    return out, preds\n",
    "\n",
    "# ---------- output dir ----------\n",
    "out_dir = data_warehouse / \"mental_health_splits_no_stress\" / \"bert_large_multiclass\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- TrainingArguments (Windows-friendly) ----------\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WD,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    dataloader_num_workers=0,     # <-- critical on Windows to avoid hangs\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    report_to=[],\n",
    "    seed=RANDOM_STATE,\n",
    "    # optim=\"adamw_torch_fused\",  # optional (PyTorch 2.0+ on recent GPUs); enable if available\n",
    ")\n",
    "\n",
    "TrainerClass = WeightedTrainer if USE_CLASS_WEIGHTS else Trainer\n",
    "trainer = TrainerClass(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,   # (we still do manual eval)\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer.add_callback(ConsoleLogger())\n",
    "\n",
    "# ---------- train ----------\n",
    "print(\"\\nStarting trainer.train() ...\", flush=True)\n",
    "train_result = trainer.train()\n",
    "print(\"Finished trainer.train()\", flush=True)\n",
    "trainer.save_model(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "with open(out_dir / \"train_metrics.json\", \"w\") as f:\n",
    "    json.dump({k: (float(v) if isinstance(v, (int, float)) else str(v)) for k, v in train_result.metrics.items()}, f, indent=2)\n",
    "\n",
    "# ---------- manual evaluation ----------\n",
    "val_out = trainer.predict(val_ds)\n",
    "val_metrics, _ = compute_metrics_from_logits(val_out.predictions, val_out.label_ids)\n",
    "\n",
    "test_out = trainer.predict(test_ds)\n",
    "test_metrics, test_preds = compute_metrics_from_logits(test_out.predictions, test_out.label_ids)\n",
    "\n",
    "with open(out_dir / \"val_metrics.json\", \"w\") as f:\n",
    "    json.dump(val_metrics, f, indent=2)\n",
    "with open(out_dir / \"test_metrics.json\", \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "rep = classification_report(\n",
    "    test_out.label_ids, test_preds,\n",
    "    target_names=[id2label[i] for i in range(num_labels)],\n",
    "    digits=4, zero_division=0\n",
    ")\n",
    "cm = confusion_matrix(test_out.label_ids, test_preds)\n",
    "\n",
    "with open(out_dir / \"test_classification_report.txt\", \"w\") as f:\n",
    "    f.write(rep)\n",
    "\n",
    "pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"true_{id2label[i]}\" for i in range(num_labels)],\n",
    "    columns=[f\"pred_{id2label[i]}\" for i in range(num_labels)],\n",
    ").to_csv(out_dir / \"test_confusion_matrix.csv\", index=True)\n",
    "\n",
    "print(\"\\nValidation metrics:\", val_metrics)\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "print(\"\\nSaved full model and metrics to:\", out_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbd8bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping used:\n",
      "0: anxiety\n",
      "1: depression\n",
      "2: ptsd\n",
      "3: suicide\n",
      "4: none\n",
      "Torch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: [2.262650489807129, 0.40430569648742676, 2.262650489807129, 1.1211940050125122, 1.3319149017333984]\n",
      "\n",
      "Starting trainer.train() ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_11032\\3808639977.py:208: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = TrainerClass(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [295/295 08:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.804800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.739700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 | epoch 0.02 | loss 1.6068 | lr 0.000020\n",
      "step 50 | epoch 0.85 | loss 1.4168 | lr 0.000017\n",
      "step 100 | epoch 1.70 | loss 1.1258 | lr 0.000013\n",
      "step 150 | epoch 2.54 | loss 0.9415 | lr 0.000010\n",
      "step 200 | epoch 3.39 | loss 0.8048 | lr 0.000007\n",
      "step 250 | epoch 4.24 | loss 0.7397 | lr 0.000003\n",
      "step 295 | epoch 5.00\n",
      "Finished trainer.train()\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics: {'accuracy': 0.7319148936170212, 'precision_macro': 0.7146765996128218, 'precision_weighted': 0.7601665893553777, 'recall_macro': 0.7511117447606864, 'recall_weighted': 0.7319148936170212, 'f1_macro': 0.7241587920832082, 'f1_weighted': 0.737719563727177, 'roc_auc_ovr': 0.9212729605008072}\n",
      "Test metrics: {'accuracy': 0.7829787234042553, 'precision_macro': 0.7733648399802349, 'precision_weighted': 0.8035480535608602, 'recall_macro': 0.7887544216800665, 'recall_weighted': 0.7829787234042553, 'f1_macro': 0.7778025677880166, 'f1_weighted': 0.7892983734292202, 'roc_auc_ovr': 0.9464913487644114}\n",
      "\n",
      "Saved MPNet model and metrics to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_no_stress\\all_mpnet_base_v2_multiclass\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Fine tune sentence-transformers/all-mpnet-base-v2 for multiclass classification\n",
    "# ==========================================\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------- progress visibility & Windows safety ----------\n",
    "os.environ.pop(\"HF_DISABLE_PROGRESS_BARS\", None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "class ConsoleLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        parts = []\n",
    "        if state.global_step is not None: parts.append(f\"step {state.global_step}\")\n",
    "        if state.epoch is not None: parts.append(f\"epoch {state.epoch:.2f}\")\n",
    "        if \"loss\" in logs: parts.append(f\"loss {logs['loss']:.4f}\")\n",
    "        if \"learning_rate\" in logs: parts.append(f\"lr {logs['learning_rate']:.6f}\")\n",
    "        print(\" | \".join(parts), flush=True)\n",
    "\n",
    "# ---------- config ----------\n",
    "RANDOM_STATE = 42\n",
    "TOKENIZER_NAME = \"sentence-transformers/all-mpnet-base-v2\"  # tokenizer\n",
    "BACKBONE_NAME  = \"microsoft/mpnet-base\"                     # classification head\n",
    "MAX_LENGTH = 512\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "WD = 0.01\n",
    "TRAIN_BS = 8\n",
    "EVAL_BS = 16\n",
    "GRAD_ACCUM_STEPS = 8     # effective batch = 64 (reduce if OOM)\n",
    "USE_CLASS_WEIGHTS = True\n",
    "\n",
    "# ---------- locate Data_Warehouse ----------\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate a folder named Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    script_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ---------- load splits ----------\n",
    "split_dir = data_warehouse / \"mental_health_splits_no_stress\"\n",
    "df_train = pd.read_csv(split_dir / \"train.csv\")\n",
    "df_val   = pd.read_csv(split_dir / \"val.csv\")\n",
    "df_test  = pd.read_csv(split_dir / \"test.csv\")\n",
    "\n",
    "for name, df_ in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    req = {\"text\", \"label\", \"label_enc\"}\n",
    "    if not req.issubset(df_.columns):\n",
    "        raise ValueError(f\"{name} split missing required columns: {req}\")\n",
    "\n",
    "num_labels = int(df_train[\"label_enc\"].max()) + 1\n",
    "enc_to_label = df_train[[\"label_enc\", \"label\"]].drop_duplicates().sort_values(\"label_enc\")\n",
    "id2label = {int(r.label_enc): str(r.label) for _, r in enc_to_label.iterrows()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(\"Label mapping used:\")\n",
    "for k in sorted(id2label.keys()):\n",
    "    print(f\"{k}: {id2label[k]}\")\n",
    "\n",
    "# ---------- dataset (dynamic padding) ----------\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df[\"text\"].astype(str).tolist()\n",
    "        self.labels = df[\"label_enc\"].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=False,      # pad per-batch via collator\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# ---------- tokenizer & model ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BACKBONE_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# MPNet does not support gradient checkpointing → guard it\n",
    "gc_enabled = False\n",
    "try:\n",
    "    if getattr(model, \"supports_gradient_checkpointing\", False):\n",
    "        model.gradient_checkpointing_enable()\n",
    "        gc_enabled = True\n",
    "except Exception as e:\n",
    "    print(f\"Gradient checkpointing not available for MPNet: {e}\")\n",
    "\n",
    "# some configs lack use_cache; guard it\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# ---------- class weights ----------\n",
    "class_weights = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    counts = df_train[\"label_enc\"].value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    weights = total / (num_labels * counts)  # inverse frequency\n",
    "    class_weights = torch.tensor(weights.to_numpy(), dtype=torch.float, device=device)\n",
    "    print(\"Using class weights:\", [float(x) for x in class_weights])\n",
    "\n",
    "# ---------- custom Trainer ----------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels, weight=class_weights) if class_weights is not None else F.cross_entropy(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ---------- datasets & collator ----------\n",
    "train_ds = TextClsDataset(df_train, tokenizer)\n",
    "val_ds   = TextClsDataset(df_val, tokenizer)\n",
    "test_ds  = TextClsDataset(df_test, tokenizer)\n",
    "\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "# ---------- metrics ----------\n",
    "def compute_metrics_from_logits(logits, labels):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    out = {\n",
    "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
    "        \"precision_macro\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"precision_weighted\": float(precision_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"recall_macro\": float(recall_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall_weighted\": float(recall_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"f1_macro\": float(f1_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"f1_weighted\": float(f1_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "    try:\n",
    "        out[\"roc_auc_ovr\"] = float(roc_auc_score(labels, probs, multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        out[\"roc_auc_ovr\"] = None\n",
    "    return out, preds\n",
    "\n",
    "# ---------- output dir ----------\n",
    "out_dir = data_warehouse / \"mental_health_splits_no_stress\" / \"all_mpnet_base_v2_multiclass\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- TrainingArguments ----------\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WD,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    dataloader_num_workers=0,   # Windows-safe\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    report_to=[],\n",
    "    seed=RANDOM_STATE,\n",
    "    # optim=\"adamw_torch_fused\",  # optional on PyTorch 2.x + recent GPUs\n",
    ")\n",
    "\n",
    "TrainerClass = WeightedTrainer if USE_CLASS_WEIGHTS else Trainer\n",
    "trainer = TrainerClass(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer.add_callback(ConsoleLogger())\n",
    "\n",
    "# ---------- train ----------\n",
    "print(\"\\nStarting trainer.train() ...\", flush=True)\n",
    "train_result = trainer.train()\n",
    "print(\"Finished trainer.train()\", flush=True)\n",
    "trainer.save_model(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "with open(out_dir / \"train_metrics.json\", \"w\") as f:\n",
    "    json.dump({k: (float(v) if isinstance(v, (int, float)) else str(v)) for k, v in train_result.metrics.items()}, f, indent=2)\n",
    "\n",
    "# ---------- manual evaluation ----------\n",
    "val_out = trainer.predict(val_ds)\n",
    "val_metrics, _ = compute_metrics_from_logits(val_out.predictions, val_out.label_ids)\n",
    "\n",
    "test_out = trainer.predict(test_ds)\n",
    "test_metrics, test_preds = compute_metrics_from_logits(test_out.predictions, test_out.label_ids)\n",
    "\n",
    "with open(out_dir / \"val_metrics.json\", \"w\") as f:\n",
    "    json.dump(val_metrics, f, indent=2)\n",
    "with open(out_dir / \"test_metrics.json\", \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "rep = classification_report(\n",
    "    test_out.label_ids, test_preds,\n",
    "    target_names=[id2label[i] for i in range(num_labels)],\n",
    "    digits=4, zero_division=0\n",
    ")\n",
    "cm = confusion_matrix(test_out.label_ids, test_preds)\n",
    "\n",
    "with open(out_dir / \"test_classification_report.txt\", \"w\") as f:\n",
    "    f.write(rep)\n",
    "\n",
    "pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"true_{id2label[i]}\" for i in range(num_labels)],\n",
    "    columns=[f\"pred_{id2label[i]}\" for i in range(num_labels)],\n",
    ").to_csv(out_dir / \"test_confusion_matrix.csv\", index=True)\n",
    "\n",
    "print(\"\\nValidation metrics:\", val_metrics)\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "print(\"\\nSaved MPNet model and metrics to:\", out_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8058186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping used:\n",
      "0: anxiety\n",
      "1: depression\n",
      "2: ptsd\n",
      "3: suicide\n",
      "4: none\n",
      "Torch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: [2.262650489807129, 0.40430569648742676, 2.262650489807129, 1.1211940050125122, 1.3319149017333984]\n",
      "\n",
      "Starting trainer.train() ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_15548\\2454647703.py:257: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = TrainerClass(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='590' max='590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [590/590 17:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.273400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.078300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 | epoch 0.01 | loss 1.7303 | lr 0.000020\n",
      "step 50 | epoch 0.43 | loss 1.1968 | lr 0.000018\n",
      "step 100 | epoch 0.85 | loss 0.7040 | lr 0.000017\n",
      "[epoch 1.00] val f1_macro=0.6953 acc=0.7021\n",
      "step 150 | epoch 1.27 | loss 0.5011 | lr 0.000015\n",
      "step 200 | epoch 1.70 | loss 0.4195 | lr 0.000013\n",
      "[epoch 2.00] val f1_macro=0.7736 acc=0.8064\n",
      "step 250 | epoch 2.12 | loss 0.4003 | lr 0.000012\n",
      "step 300 | epoch 2.55 | loss 0.2804 | lr 0.000010\n",
      "step 350 | epoch 2.97 | loss 0.2734 | lr 0.000008\n",
      "[epoch 3.00] val f1_macro=0.7879 acc=0.8149\n",
      "step 400 | epoch 3.39 | loss 0.1728 | lr 0.000006\n",
      "step 450 | epoch 3.82 | loss 0.1641 | lr 0.000005\n",
      "[epoch 4.00] val f1_macro=0.7912 acc=0.8191\n",
      "step 500 | epoch 4.24 | loss 0.1272 | lr 0.000003\n",
      "step 550 | epoch 4.66 | loss 0.0783 | lr 0.000001\n",
      "[epoch 5.00] val f1_macro=0.7946 acc=0.8191\n",
      "step 590 | epoch 5.00\n",
      "Finished trainer.train()\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "Label-wise metrics — TRAIN\n",
      "==========================\n",
      "              precision  recall  f1-score    support  roc_auc_ovr\n",
      "anxiety          0.9764  0.9970    0.9866   332.0000       0.9998\n",
      "depression       0.9950  0.9645    0.9795  1858.0000       0.9974\n",
      "ptsd             0.9970  0.9940    0.9955   332.0000       1.0000\n",
      "suicide          0.9196  0.9896    0.9533   670.0000       0.9970\n",
      "none             1.0000  1.0000    1.0000   564.0000       1.0000\n",
      "accuracy         0.9798  0.9798    0.9798     0.9798          NaN\n",
      "macro avg        0.9776  0.9890    0.9830  3756.0000          NaN\n",
      "weighted avg     0.9808  0.9798    0.9799  3756.0000          NaN\n",
      "\n",
      "========================\n",
      "Label-wise metrics — VAL\n",
      "========================\n",
      "              precision  recall  f1-score   support  roc_auc_ovr\n",
      "anxiety          0.6731  0.8333    0.7447   42.0000       0.9772\n",
      "depression       0.8690  0.8578    0.8633  232.0000       0.9456\n",
      "ptsd             0.8056  0.7073    0.7532   41.0000       0.9810\n",
      "suicide          0.6548  0.6548    0.6548   84.0000       0.9280\n",
      "none             0.9710  0.9437    0.9571   71.0000       0.9990\n",
      "accuracy         0.8191  0.8191    0.8191    0.8191          NaN\n",
      "macro avg        0.7947  0.7994    0.7946  470.0000          NaN\n",
      "weighted avg     0.8231  0.8191    0.8200  470.0000          NaN\n",
      "\n",
      "=========================\n",
      "Label-wise metrics — TEST\n",
      "=========================\n",
      "              precision  recall  f1-score   support  roc_auc_ovr\n",
      "anxiety          0.7955  0.8333    0.8140   42.0000       0.9705\n",
      "depression       0.9200  0.8922    0.9059  232.0000       0.9697\n",
      "ptsd             0.9444  0.8293    0.8831   41.0000       0.9976\n",
      "suicide          0.7292  0.8333    0.7778   84.0000       0.9596\n",
      "none             0.9855  0.9577    0.9714   71.0000       0.9987\n",
      "accuracy         0.8809  0.8809    0.8809    0.8809          NaN\n",
      "macro avg        0.8749  0.8692    0.8704  470.0000          NaN\n",
      "weighted avg     0.8868  0.8809    0.8827  470.0000          NaN\n",
      "\n",
      "Generalization gaps (train - val): {'acc': 0.16061677203000024, 'f1_macro': 0.18833259811983594, 'f1_weighted': 0.1599140749569451}\n",
      "\n",
      "Train metrics: {'accuracy': 0.979765708200213, 'precision_macro': 0.9775877960247061, 'precision_weighted': 0.9808253365459652, 'recall_macro': 0.9889988054978456, 'recall_weighted': 0.979765708200213, 'f1_macro': 0.9829671841611696, 'f1_weighted': 0.979939322469557, 'roc_auc_ovr': 0.9988312287998632}\n",
      "Val metrics: {'accuracy': 0.8191489361702128, 'precision_macro': 0.7946809018671559, 'precision_weighted': 0.823076647727828, 'recall_macro': 0.7993665807573221, 'recall_weighted': 0.8191489361702128, 'f1_macro': 0.7946345860413336, 'f1_weighted': 0.8200252475126119, 'roc_auc_ovr': 0.9661677556738134}\n",
      "Test metrics: {'accuracy': 0.8808510638297873, 'precision_macro': 0.8749145805884936, 'precision_weighted': 0.8867921864341846, 'recall_macro': 0.8691845635066355, 'recall_weighted': 0.8808510638297873, 'f1_macro': 0.8704369633950826, 'f1_weighted': 0.8827005935343879, 'roc_auc_ovr': 0.9792260466017186}\n",
      "\n",
      "Saved model, best checkpoint (if any), and metrics to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_no_stress\\all_roberta_large_v1_multiclass\n",
      "\n",
      "Evaluating best checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_15548\\2454647703.py:382: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  best_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best checkpoint metrics:\n",
      "Val: {'accuracy': 0.8191489361702128, 'precision_macro': 0.7946809018671559, 'precision_weighted': 0.823076647727828, 'recall_macro': 0.7993665807573221, 'recall_weighted': 0.8191489361702128, 'f1_macro': 0.7946345860413336, 'f1_weighted': 0.8200252475126119, 'roc_auc_ovr': 0.9661677556738134}\n",
      "Test: {'accuracy': 0.8808510638297873, 'precision_macro': 0.8749145805884936, 'precision_weighted': 0.8867921864341846, 'recall_macro': 0.8691845635066355, 'recall_weighted': 0.8808510638297873, 'f1_macro': 0.8704369633950826, 'f1_weighted': 0.8827005935343879, 'roc_auc_ovr': 0.9792260466017186}\n",
      "\n",
      "===========================================\n",
      "Label-wise metrics — TEST (Best checkpoint)\n",
      "===========================================\n",
      "              precision  recall  f1-score   support  roc_auc_ovr\n",
      "anxiety          0.7955  0.8333    0.8140   42.0000       0.9705\n",
      "depression       0.9200  0.8922    0.9059  232.0000       0.9697\n",
      "ptsd             0.9444  0.8293    0.8831   41.0000       0.9976\n",
      "suicide          0.7292  0.8333    0.7778   84.0000       0.9596\n",
      "none             0.9855  0.9577    0.9714   71.0000       0.9987\n",
      "accuracy         0.8809  0.8809    0.8809    0.8809          NaN\n",
      "macro avg        0.8749  0.8692    0.8704  470.0000          NaN\n",
      "weighted avg     0.8868  0.8809    0.8827  470.0000          NaN\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Fine tune sentence-transformers/all-roberta-large-v1\n",
    "# with overfitting checks, early stopping, and label-wise display\n",
    "# ==========================================\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------- progress visibility & Windows safety ----------\n",
    "os.environ.pop(\"HF_DISABLE_PROGRESS_BARS\", None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "class ConsoleLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        parts = []\n",
    "        if state.global_step is not None: parts.append(f\"step {state.global_step}\")\n",
    "        if state.epoch is not None: parts.append(f\"epoch {state.epoch:.2f}\")\n",
    "        if \"loss\" in logs: parts.append(f\"loss {logs['loss']:.4f}\")\n",
    "        if \"learning_rate\" in logs: parts.append(f\"lr {logs['learning_rate']:.6f}\")\n",
    "        print(\" | \".join(parts), flush=True)\n",
    "\n",
    "# ---------- config ----------\n",
    "RANDOM_STATE = 42\n",
    "TOKENIZER_NAME = \"sentence-transformers/all-roberta-large-v1\"  # ST tokenizer\n",
    "BACKBONE_NAME  = \"roberta-large\"                               # classification backbone\n",
    "MAX_LENGTH = 512\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "WD = 0.01\n",
    "TRAIN_BS = 4             # roberta-large is heavy; keep modest\n",
    "EVAL_BS = 16\n",
    "GRAD_ACCUM_STEPS = 8     # effective batch = 4 * 8 = 32\n",
    "USE_CLASS_WEIGHTS = True\n",
    "LABEL_SMOOTHING = 0.0    # e.g., set 0.05 if you observe overfitting\n",
    "\n",
    "EARLY_STOP_PATIENCE = 2  # epochs without improvement before stopping\n",
    "EARLY_STOP_MONITOR = \"f1_macro\"\n",
    "\n",
    "# ---------- locate Data_Warehouse ----------\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate a folder named Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    script_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ---------- load splits ----------\n",
    "split_dir = data_warehouse / \"mental_health_splits_no_stress\"\n",
    "df_train = pd.read_csv(split_dir / \"train.csv\")\n",
    "df_val   = pd.read_csv(split_dir / \"val.csv\")\n",
    "df_test  = pd.read_csv(split_dir / \"test.csv\")\n",
    "\n",
    "# checks\n",
    "for name, df_ in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    req = {\"text\", \"label\", \"label_enc\"}\n",
    "    if not req.issubset(df_.columns):\n",
    "        raise ValueError(f\"{name} split missing required columns: {req}\")\n",
    "\n",
    "num_labels = int(df_train[\"label_enc\"].max()) + 1\n",
    "enc_to_label = df_train[[\"label_enc\", \"label\"]].drop_duplicates().sort_values(\"label_enc\")\n",
    "id2label = {int(r.label_enc): str(r.label) for _, r in enc_to_label.iterrows()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "label_names = [id2label[i] for i in range(num_labels)]\n",
    "\n",
    "print(\"Label mapping used:\")\n",
    "for k in sorted(id2label.keys()):\n",
    "    print(f\"{k}: {id2label[k]}\")\n",
    "\n",
    "# ---------- dataset (dynamic padding) ----------\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df[\"text\"].astype(str).tolist()\n",
    "        self.labels = df[\"label_enc\"].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=False,      # pad per-batch via collator\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# ---------- tokenizer & model ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BACKBONE_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Try to enable gradient checkpointing to save memory\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not enable gradient checkpointing: {e}\")\n",
    "\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# ---------- class weights ----------\n",
    "class_weights = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    counts = df_train[\"label_enc\"].value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    weights = total / (num_labels * counts)  # inverse frequency\n",
    "    class_weights = torch.tensor(weights.to_numpy(), dtype=torch.float, device=device)\n",
    "    print(\"Using class weights:\", [float(x) for x in class_weights])\n",
    "\n",
    "# ---------- custom Trainer (weighted CE + optional label smoothing) ----------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(\n",
    "            logits, labels,\n",
    "            weight=class_weights,\n",
    "            label_smoothing=LABEL_SMOOTHING if LABEL_SMOOTHING > 0 else 0.0\n",
    "        ) if class_weights is not None else F.cross_entropy(\n",
    "            logits, labels, label_smoothing=LABEL_SMOOTHING if LABEL_SMOOTHING > 0 else 0.0\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ---------- datasets & collator ----------\n",
    "train_ds = TextClsDataset(df_train, tokenizer)\n",
    "val_ds   = TextClsDataset(df_val, tokenizer)\n",
    "test_ds  = TextClsDataset(df_test, tokenizer)\n",
    "\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "# ---------- metrics (overall) ----------\n",
    "def compute_metrics_from_logits(logits, labels):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    out = {\n",
    "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
    "        \"precision_macro\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"precision_weighted\": float(precision_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"recall_macro\": float(recall_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall_weighted\": float(recall_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"f1_macro\": float(f1_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"f1_weighted\": float(f1_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "    try:\n",
    "        out[\"roc_auc_ovr\"] = float(roc_auc_score(labels, probs, multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        out[\"roc_auc_ovr\"] = None\n",
    "    return out, preds, probs\n",
    "\n",
    "# ---------- label-wise metrics (DISPLAY ONLY) ----------\n",
    "def per_label_report(logits, labels, label_names):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "\n",
    "    rep = classification_report(\n",
    "        labels, preds, target_names=label_names,\n",
    "        output_dict=True, zero_division=0\n",
    "    )\n",
    "    df = pd.DataFrame(rep).T\n",
    "\n",
    "    # add one-vs-rest ROC-AUC per class\n",
    "    aucs = {}\n",
    "    y_true = np.array(labels)\n",
    "    for i, name in enumerate(label_names):\n",
    "        try:\n",
    "            y_bin = (y_true == i).astype(int)\n",
    "            aucs[name] = roc_auc_score(y_bin, probs[:, i])\n",
    "        except Exception:\n",
    "            aucs[name] = np.nan\n",
    "\n",
    "    for name, auc in aucs.items():\n",
    "        if name in df.index:\n",
    "            df.loc[name, \"roc_auc_ovr\"] = float(auc) if auc == auc else None\n",
    "\n",
    "    # keep useful columns for display\n",
    "    cols = [\"precision\", \"recall\", \"f1-score\", \"support\", \"roc_auc_ovr\"]\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "def show_labelwise(df, title):\n",
    "    print(\"\\n\" + \"=\"*len(title))\n",
    "    print(title)\n",
    "    print(\"=\"*len(title))\n",
    "    print(df.round(4).to_string())\n",
    "\n",
    "# ---------- output dir ----------\n",
    "out_dir = data_warehouse / \"mental_health_splits_no_stress\" / \"all_roberta_large_v1_multiclass\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- TrainingArguments (Windows-friendly) ----------\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WD,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    dataloader_num_workers=0,   # Windows: avoid hangs\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    report_to=[],\n",
    "    seed=RANDOM_STATE,\n",
    "    # optim=\"adamw_torch_fused\",  # optional on PyTorch 2.x + recent GPUs\n",
    ")\n",
    "\n",
    "TrainerClass = WeightedTrainer if USE_CLASS_WEIGHTS else Trainer\n",
    "trainer = TrainerClass(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,   # we run our own eval per epoch too\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.add_callback(ConsoleLogger())\n",
    "\n",
    "# ---------- Overfitting monitor: eval each epoch + early stopping + best checkpoint ----------\n",
    "class EvalEveryEpochCallback(TrainerCallback):\n",
    "    def __init__(self, trainer_ref, val_ds, out_dir, patience=2, monitor=\"f1_macro\"):\n",
    "        self.trainer_ref = trainer_ref\n",
    "        self.val_ds = val_ds\n",
    "        self.out_dir = Path(out_dir)\n",
    "        self.monitor = monitor\n",
    "        self.best = -float(\"inf\")\n",
    "        self.bad_epochs = 0\n",
    "        self.patience = patience\n",
    "        self.log_path = self.out_dir / \"epoch_metrics.jsonl\"\n",
    "        self._fh = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self._fh = open(self.log_path, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        out = self.trainer_ref.predict(self.val_ds)\n",
    "        metrics, _, _ = compute_metrics_from_logits(out.predictions, out.label_ids)\n",
    "        metrics[\"epoch\"] = float(state.epoch)\n",
    "        self._fh.write(json.dumps(metrics) + \"\\n\"); self._fh.flush()\n",
    "        print(f\"[epoch {metrics['epoch']:.2f}] val f1_macro={metrics['f1_macro']:.4f} \"\n",
    "              f\"acc={metrics['accuracy']:.4f}\", flush=True)\n",
    "        score = metrics.get(self.monitor, -float(\"inf\"))\n",
    "        if score > self.best + 1e-8:\n",
    "            self.best = score\n",
    "            self.bad_epochs = 0\n",
    "            self.trainer_ref.save_model(self.out_dir / \"best\")\n",
    "        else:\n",
    "            self.bad_epochs += 1\n",
    "            if self.bad_epochs >= self.patience:\n",
    "                print(f\"Early stopping: no improvement in {self.patience} epoch(s).\", flush=True)\n",
    "                control.should_training_stop = True\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self._fh:\n",
    "            self._fh.close()\n",
    "\n",
    "trainer.add_callback(EvalEveryEpochCallback(trainer, val_ds, out_dir, patience=EARLY_STOP_PATIENCE, monitor=EARLY_STOP_MONITOR))\n",
    "\n",
    "# ---------- train ----------\n",
    "print(\"\\nStarting trainer.train() ...\", flush=True)\n",
    "train_result = trainer.train()\n",
    "print(\"Finished trainer.train()\", flush=True)\n",
    "\n",
    "# save \"last\" model + tokenizer\n",
    "trainer.save_model(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "with open(out_dir / \"train_metrics.json\", \"w\") as f:\n",
    "    json.dump({k: (float(v) if isinstance(v, (int, float)) else str(v)) for k, v in train_result.metrics.items()}, f, indent=2)\n",
    "\n",
    "# ---------- manual evaluation (last model) ----------\n",
    "val_out = trainer.predict(val_ds)\n",
    "val_metrics, val_preds, val_probs = compute_metrics_from_logits(val_out.predictions, val_out.label_ids)\n",
    "\n",
    "test_out = trainer.predict(test_ds)\n",
    "test_metrics, test_preds, test_probs = compute_metrics_from_logits(test_out.predictions, test_out.label_ids)\n",
    "\n",
    "# also evaluate TRAIN to check generalization gap\n",
    "#train_out = trainer.predict(train_dataset=train_ds)\n",
    "train_out = trainer.predict(train_ds)\n",
    "train_metrics, train_preds, train_probs = compute_metrics_from_logits(train_out.predictions, train_out.label_ids)\n",
    "\n",
    "# save overall metrics\n",
    "with open(out_dir / \"train_eval_metrics.json\", \"w\") as f:\n",
    "    json.dump(train_metrics, f, indent=2)\n",
    "with open(out_dir / \"val_metrics.json\", \"w\") as f:\n",
    "    json.dump(val_metrics, f, indent=2)\n",
    "with open(out_dir / \"test_metrics.json\", \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "# ---------- DISPLAY label-wise metrics for each split ----------\n",
    "train_labelwise = per_label_report(train_out.predictions, train_out.label_ids, label_names)\n",
    "val_labelwise   = per_label_report(val_out.predictions,   val_out.label_ids,   label_names)\n",
    "test_labelwise  = per_label_report(test_out.predictions,  test_out.label_ids,  label_names)\n",
    "\n",
    "show_labelwise(train_labelwise, \"Label-wise metrics — TRAIN\")\n",
    "show_labelwise(val_labelwise,   \"Label-wise metrics — VAL\")\n",
    "show_labelwise(test_labelwise,  \"Label-wise metrics — TEST\")\n",
    "\n",
    "# ---------- classification report & confusion matrix on test ----------\n",
    "rep = classification_report(\n",
    "    test_out.label_ids, test_preds,\n",
    "    target_names=label_names,\n",
    "    digits=4, zero_division=0\n",
    ")\n",
    "cm = confusion_matrix(test_out.label_ids, test_preds)\n",
    "\n",
    "with open(out_dir / \"test_classification_report.txt\", \"w\") as f:\n",
    "    f.write(rep)\n",
    "pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"true_{n}\" for n in label_names],\n",
    "    columns=[f\"pred_{n}\" for n in label_names],\n",
    ").to_csv(out_dir / \"test_confusion_matrix.csv\", index=True)\n",
    "\n",
    "# ---------- simple gap printout ----------\n",
    "print(\"\\nGeneralization gaps (train - val):\",\n",
    "      {\"acc\": train_metrics[\"accuracy\"] - val_metrics[\"accuracy\"],\n",
    "       \"f1_macro\": train_metrics[\"f1_macro\"] - val_metrics[\"f1_macro\"],\n",
    "       \"f1_weighted\": train_metrics[\"f1_weighted\"] - val_metrics[\"f1_weighted\"]})\n",
    "\n",
    "print(\"\\nTrain metrics:\", train_metrics)\n",
    "print(\"Val metrics:\", val_metrics)\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "\n",
    "print(\"\\nSaved model, best checkpoint (if any), and metrics to:\", out_dir.resolve())\n",
    "\n",
    "# ---------- (Optional) Evaluate the saved 'best' checkpoint and DISPLAY label-wise ----------\n",
    "best_dir = out_dir / \"best\"\n",
    "if best_dir.exists():\n",
    "    print(\"\\nEvaluating best checkpoint ...\")\n",
    "    best_model = AutoModelForSequenceClassification.from_pretrained(best_dir).to(device)\n",
    "    best_trainer = Trainer(\n",
    "        model=best_model,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    b_val = best_trainer.predict(val_ds)\n",
    "    b_val_metrics, _, _ = compute_metrics_from_logits(b_val.predictions, b_val.label_ids)\n",
    "    b_test = best_trainer.predict(test_ds)\n",
    "    b_test_metrics, _, _ = compute_metrics_from_logits(b_test.predictions, b_test.label_ids)\n",
    "\n",
    "    print(\"\\nBest checkpoint metrics:\")\n",
    "    print(\"Val:\", b_val_metrics)\n",
    "    print(\"Test:\", b_test_metrics)\n",
    "\n",
    "    # label-wise display for best test\n",
    "    b_test_labelwise = per_label_report(b_test.predictions, b_test.label_ids, label_names)\n",
    "    show_labelwise(b_test_labelwise, \"Label-wise metrics — TEST (Best checkpoint)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
