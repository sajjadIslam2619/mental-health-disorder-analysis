{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e3b51a",
   "metadata": {},
   "source": [
    "### Without Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990b6263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 470\n",
      "                                                text       label  label_enc\n",
      "0  I had a promising academic future. I had a won...     suicide          3\n",
      "1  It seems that goalkeepers tend to be older com...        none          4\n",
      "2  \"Life has no meaning the moment you lose the i...  depression          1\n",
      "3  I got way too attached and when she ghosted me...     suicide          3\n",
      "4  I have severe severe depression, high magnitud...  depression          1\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load no-stress test split\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== locate Data_Warehouse =====\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent  # if running as script\n",
    "    except NameError:\n",
    "        script_dir = Path.cwd()                       # if running in notebook\n",
    "    data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ===== load no-stress test.csv =====\n",
    "test_path = data_warehouse / \"mental_health_splits_no_stress\" / \"test.csv\"\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Rows:\", len(df_test))\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b20afb",
   "metadata": {},
   "source": [
    "### With Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a54515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 697\n",
      "                                                text  label_norm  label_enc\n",
      "0  I use a lot of different ingredients when prep...        none          5\n",
      "1  When we are at work we joke around but we all ...      stress          4\n",
      "2  1 year ago, I left for good. I was a reddit lu...  depression          1\n",
      "3  someone once told me smoking weed and then tou...        none          5\n",
      "4  The main source of this stress is a scholarshi...      stress          4\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load no-stress test split\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== locate Data_Warehouse =====\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent  # if running as script\n",
    "    except NameError:\n",
    "        script_dir = Path.cwd()                       # if running in notebook\n",
    "    data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ===== load no-stress test.csv =====\n",
    "test_path = data_warehouse / \"mental_health_splits_with_stress\" / \"test.csv\"\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Rows:\", len(df_test))\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77549e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prompt builder for mental health classification\n",
    "# zero shot first, few shot supported if you later set is_few_shot=True\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def build_full_prompt(\n",
    "    labels: List[str],\n",
    "    test_data_for_prompt: str,\n",
    "    is_few_shot: bool = False,\n",
    "    train_data_for_prompt: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a single prompt that can contain multiple test items.\n",
    "    test_data_for_prompt should be lines like: id | text\n",
    "    if is_few_shot=True, train_data_for_prompt should be lines like: id | label | text\n",
    "    \"\"\"\n",
    "\n",
    "    instruction = \"\"\"\n",
    "Instruction\n",
    "Your task is to assign exactly one mental health label to each user text. This is a single label classification task only.\n",
    "\"\"\"\n",
    "\n",
    "    context = f\"\"\"\n",
    "Context\n",
    "This will be used in a research demo that compares large language models on mental health text classification.\n",
    "Do not provide medical advice. Do not provide crisis instructions. Only return labels.\n",
    "Each example contains\n",
    "    label the ground truth class used for evaluation\n",
    "    text a short natural language post\n",
    "\n",
    "Predefined Labels\n",
    "{\", \".join(labels)}\n",
    "\"\"\"\n",
    "\n",
    "    handling_uncertainty = \"\"\"\n",
    "Handling Uncertainty\n",
    "If the text is unclear or could reasonably match multiple categories, select the closest label by meaning.\n",
    "If there is no signal of a mental health condition, use the label none.\n",
    "\"\"\"\n",
    "\n",
    "    training_data = \"\"\"\n",
    "Training Data\n",
    "You will see a few labeled examples inside triple quotes.\n",
    "Format\n",
    "id | label | text\n",
    "\"\"\"\n",
    "\n",
    "    test_data_intro = \"\"\"\n",
    "Test Data\n",
    "You will see unlabeled items inside triple quotes.\n",
    "For each item, return one predicted label from the predefined list.\n",
    "Format\n",
    "id | text\n",
    "\"\"\"\n",
    "\n",
    "    output_format = \"\"\"\n",
    "Output Format\n",
    "Return one line per item using this exact format\n",
    "id | predicted_label | text\n",
    "Do not add explanations. Do not add extra fields. Keep the original text unchanged.\n",
    "\"\"\"\n",
    "\n",
    "    if is_few_shot:\n",
    "        full_prompt = \"\\n\\n\".join([\n",
    "            instruction.strip(),\n",
    "            context.strip(),\n",
    "            handling_uncertainty.strip(),\n",
    "            training_data.strip(),\n",
    "            f'\"\"\"\\n{train_data_for_prompt.strip()}\\n\"\"\"',\n",
    "            test_data_intro.strip(),\n",
    "            f'\"\"\"\\n{test_data_for_prompt.strip()}\\n\"\"\"',\n",
    "            output_format.strip()\n",
    "        ])\n",
    "    else:\n",
    "        full_prompt = \"\\n\\n\".join([\n",
    "            instruction.strip(),\n",
    "            context.strip(),\n",
    "            handling_uncertainty.strip(),\n",
    "            test_data_intro.strip(),\n",
    "            f'\"\"\"\\n{test_data_for_prompt.strip()}\\n\"\"\"',\n",
    "            output_format.strip()\n",
    "        ])\n",
    "\n",
    "    return full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4edca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction\n",
      "Your task is to assign exactly one mental health label to each user text. This is a single label classification task only.\n",
      "\n",
      "Context\n",
      "This will be used in a research demo that compares large language models on mental health text classification.\n",
      "Do not provide medical advice. Do not provide crisis instructions. Only return labels.\n",
      "Each example contains\n",
      "    label the ground truth class used for evaluation\n",
      "    text a short natural language post\n",
      "\n",
      "Predefined Labels\n",
      "anxiety, depression, none, ptsd, suicide\n",
      "\n",
      "Handling Uncertainty\n",
      "If the text is unclear or could reasonably match multiple categories, select the closest label by meaning.\n",
      "If there is no signal of a mental health condition, use the label none.\n",
      "\n",
      "Test Data\n",
      "You will see unlabeled items inside triple quotes.\n",
      "For each item, return one predicted label from the predefined list.\n",
      "Format\n",
      "id | text\n",
      "\n",
      "\"\"\"\n",
      "0 | I had a promising academic future. I had a wonderful, sweet partner. I had so much drive. I had so much support. People were proud of me. I have been a medical experiment for 2 years. None of the meds have helped much. I had to drop out of grad school after getting straight failing grades. The love of my life has fallen out of love with me. My grandmother, who raised me, died. I have thousands of dollars in medical debt associated with my mental illness and my only means of work is to take my clothes off and grind on strange men. I hate what my life has become. I hate what it is. I feel like I have nothing left to live for.\n",
      "1 | It seems that goalkeepers tend to be older compared to outfield players, and can even still play first team football into their early 40's. I've also heard people say that goalkeepers peak later than other players. Why is that? Do teams really value experience over potentially much faster reflexes that you would get from a younger player?\n",
      "2 | \"Life has no meaning the moment you lose the illusion of being eternal.\" This quote fucked me up. I have been constantly struggling to find meanin\n"
     ]
    }
   ],
   "source": [
    "# Build label list from your split and a compact test block\n",
    "labels = sorted(df_test[\"label\"].astype(str).str.strip().unique().tolist())\n",
    "\n",
    "# Create lines: id | text\n",
    "test_lines = []\n",
    "for i, r in df_test.reset_index(drop=True).iterrows():\n",
    "    txt = str(r[\"text\"]).replace(\"\\n\", \" \").strip()\n",
    "    test_lines.append(f\"{i} | {txt}\")\n",
    "test_block = \"\\n\".join(test_lines[:50])  # you can choose any slice size\n",
    "\n",
    "# Build the final zero shot prompt\n",
    "full_prompt = build_full_prompt(labels=labels, test_data_for_prompt=test_block, is_few_shot=False)\n",
    "print(full_prompt[:2000])  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef73ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Zero shot run and evaluation on mental_health_splits_no_stress/test.csv\n",
    "\n",
    "import os, re, json, time, math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ===== inputs already prepared earlier =====\n",
    "# df_test -> loaded from Data_Warehouse/mental_health_splits_no_stress/test.csv\n",
    "# build_full_prompt(...) -> from the previous message\n",
    "\n",
    "# ===== user knobs =====\n",
    "llm = \"GPT\"            # \"GPT\" or DeepSeek\n",
    "BATCH_SIZE = 5        # items per request\n",
    "TEMPERATURE = 1.0\n",
    "TOP_P = 1.0\n",
    "RUN_TAG = \"zero_shot_nostress_test\"\n",
    "\n",
    "OUT_DIR = Path(\"llm_runs_zero_shot\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===== model routing exactly as you shared =====\n",
    "if llm == \"GPT\":\n",
    "    api_key = os.getenv(\"GPT_API_KEY\")\n",
    "    api_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    model_name = \"gpt-5\"\n",
    "else:\n",
    "    api_key = os.getenv(\"DS_API_KEY\")\n",
    "    api_url = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "    model_name = \"deepseek-chat\" #deepseek-reasoner\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a51b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== helpers =====\n",
    "def normalize_label(s: str) -> str:\n",
    "    return \" \".join(str(s).strip().lower().split())\n",
    "\n",
    "def canonicalize(pred_label: str, allowed: list[str]) -> str | None:\n",
    "    \"\"\"map a predicted label string to one of the allowed labels\"\"\"\n",
    "    if pred_label is None:\n",
    "        return None\n",
    "    norm = normalize_label(pred_label)\n",
    "    allowed_norm = {normalize_label(a): a for a in allowed}\n",
    "    if norm in allowed_norm:\n",
    "        return allowed_norm[norm]\n",
    "    # simple token overlap fallback\n",
    "    toks = set(norm.split())\n",
    "    if not toks:\n",
    "        return None\n",
    "    best, best_score = None, -1\n",
    "    for a in allowed:\n",
    "        atoks = set(normalize_label(a).split())\n",
    "        score = len(toks & atoks)\n",
    "        if score > best_score:\n",
    "            best, best_score = a, score\n",
    "    return best\n",
    "\n",
    "def build_test_block(df: pd.DataFrame, start: int, end: int) -> str:\n",
    "    lines = []\n",
    "    for i, r in df.iloc[start:end].reset_index(drop=True).iterrows():\n",
    "        txt = str(r[\"text\"]).replace(\"\\n\", \" \").strip()\n",
    "        lines.append(f\"{start+i} | {txt}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "line_pat = re.compile(r\"^\\s*(\\d+)\\s*\\|\\s*([^\\|]+?)\\s*\\|\\s*(.*)$\")\n",
    "\n",
    "def parse_output_to_df(raw_text: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    expects lines: id | predicted_label | text\n",
    "    returns a dataframe with columns: id, pred, text\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for line in raw_text.splitlines():\n",
    "        m = line_pat.match(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        idx = int(m.group(1))\n",
    "        pred = m.group(2).strip()\n",
    "        txt  = m.group(3).strip()\n",
    "        rows.append({\"id\": idx, \"pred_raw\": pred, \"text_out\": txt})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a careful labeler. Classify mental health text. Do not give advice.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P\n",
    "    }\n",
    "    for attempt in range(5):\n",
    "        r = requests.post(api_url, headers=headers, json=data, timeout=120)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        time.sleep(1.5 * (attempt + 1))\n",
    "    raise RuntimeError(f\"LLM API error: {r.status_code} {r.text}\")\n",
    "\n",
    "def evaluate(y_true: list[str], y_pred: list[str], labels: list[str]) -> dict:\n",
    "    return {\n",
    "        \"n\": len(y_true),\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"report\": classification_report(y_true, y_pred, labels=labels, output_dict=True),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred, labels=labels).tolist(),\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8492d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0-5 got reply in 18.8s\n",
      "Batch 5-10 got reply in 13.8s\n",
      "Batch 10-15 got reply in 20.8s\n",
      "Batch 15-20 got reply in 24.3s\n",
      "Batch 20-25 got reply in 7.8s\n",
      "Batch 25-30 got reply in 31.9s\n",
      "Batch 30-35 got reply in 21.8s\n",
      "Batch 35-40 got reply in 27.4s\n",
      "Batch 40-45 got reply in 14.0s\n",
      "Batch 45-50 got reply in 34.4s\n",
      "Batch 50-55 got reply in 14.7s\n",
      "Batch 55-60 got reply in 21.6s\n",
      "Batch 60-65 got reply in 24.2s\n",
      "Batch 65-70 got reply in 18.1s\n",
      "Batch 70-75 got reply in 11.6s\n",
      "Batch 75-80 got reply in 13.1s\n",
      "Batch 80-85 got reply in 17.3s\n",
      "Batch 85-90 got reply in 14.0s\n",
      "Batch 90-95 got reply in 7.8s\n",
      "Batch 95-100 got reply in 29.7s\n",
      "Batch 100-105 got reply in 13.0s\n",
      "Batch 105-110 got reply in 10.9s\n",
      "Batch 110-115 got reply in 18.1s\n",
      "Batch 115-120 got reply in 23.1s\n",
      "Batch 120-125 got reply in 16.8s\n",
      "Batch 125-130 got reply in 22.3s\n",
      "Batch 130-135 got reply in 18.9s\n",
      "Batch 135-140 got reply in 13.4s\n",
      "Batch 140-145 got reply in 13.5s\n",
      "Batch 145-150 got reply in 13.0s\n",
      "Batch 150-155 got reply in 9.1s\n",
      "Batch 155-160 got reply in 40.5s\n",
      "Batch 160-165 got reply in 20.7s\n",
      "Batch 165-170 got reply in 25.4s\n",
      "Batch 170-175 got reply in 42.4s\n",
      "Batch 175-180 got reply in 19.9s\n",
      "Batch 180-185 got reply in 11.0s\n",
      "Batch 185-190 got reply in 24.4s\n",
      "Batch 190-195 got reply in 13.8s\n",
      "Batch 195-200 got reply in 15.0s\n",
      "Batch 200-205 got reply in 19.5s\n",
      "Batch 205-210 got reply in 16.4s\n",
      "Batch 210-215 got reply in 12.1s\n",
      "Batch 215-220 got reply in 9.7s\n",
      "Batch 220-225 got reply in 15.7s\n",
      "Batch 225-230 got reply in 17.3s\n",
      "Batch 230-235 got reply in 17.4s\n",
      "Batch 235-240 got reply in 22.1s\n",
      "Batch 240-245 got reply in 21.2s\n",
      "Batch 245-250 got reply in 17.3s\n",
      "Batch 250-255 got reply in 25.0s\n",
      "Batch 255-260 got reply in 18.2s\n",
      "Batch 260-265 got reply in 20.9s\n",
      "Batch 265-270 got reply in 19.3s\n",
      "Batch 270-275 got reply in 23.6s\n",
      "Batch 275-280 got reply in 20.7s\n",
      "Batch 280-285 got reply in 12.8s\n",
      "Batch 285-290 got reply in 38.0s\n",
      "Batch 290-295 got reply in 15.7s\n",
      "Batch 295-300 got reply in 15.5s\n",
      "Batch 300-305 got reply in 17.7s\n",
      "Batch 305-310 got reply in 12.5s\n",
      "Batch 310-315 got reply in 14.7s\n",
      "Batch 315-320 got reply in 47.1s\n",
      "Batch 320-325 got reply in 9.0s\n",
      "Batch 325-330 got reply in 35.9s\n",
      "Batch 330-335 got reply in 44.5s\n",
      "Batch 335-340 got reply in 48.3s\n",
      "Batch 340-345 got reply in 52.2s\n",
      "Batch 345-350 got reply in 14.8s\n",
      "Batch 350-355 got reply in 18.8s\n",
      "Batch 355-360 got reply in 7.1s\n",
      "Batch 360-365 got reply in 16.4s\n",
      "Batch 365-370 got reply in 18.3s\n",
      "Batch 370-375 got reply in 11.6s\n",
      "Batch 375-380 got reply in 19.2s\n",
      "Batch 380-385 got reply in 13.1s\n",
      "Batch 385-390 got reply in 37.6s\n",
      "Batch 390-395 got reply in 18.3s\n",
      "Batch 395-400 got reply in 11.9s\n",
      "Batch 400-405 got reply in 26.1s\n",
      "Batch 405-410 got reply in 39.3s\n",
      "Batch 410-415 got reply in 13.9s\n",
      "Batch 415-420 got reply in 32.7s\n",
      "Batch 420-425 got reply in 16.3s\n",
      "Batch 425-430 got reply in 21.5s\n",
      "Batch 430-435 got reply in 23.1s\n",
      "Batch 435-440 got reply in 18.9s\n",
      "Batch 440-445 got reply in 11.1s\n",
      "Batch 445-450 got reply in 29.2s\n",
      "Batch 450-455 got reply in 16.2s\n",
      "Batch 455-460 got reply in 27.5s\n",
      "Batch 460-465 got reply in 51.9s\n",
      "Batch 465-470 got reply in 23.0s\n",
      "Total wall time 1970.6s for 470 items\n",
      "\n",
      "Overall metrics\n",
      "{'accuracy': 0.6638, 'f1_macro': 0.6621, 'f1_weighted': 0.6623}\n",
      "\n",
      "Label wise metrics on TEST\n",
      "     anxiety  p=0.544  r=0.738  f1=0.626  support=42\n",
      "  depression  p=0.909  r=0.517  f1=0.659  support=232\n",
      "        none  p=0.574  r=0.986  f1=0.725  support=71\n",
      "        ptsd  p=0.880  r=0.537  f1=0.667  support=41\n",
      "     suicide  p=0.515  r=0.821  f1=0.633  support=84\n"
     ]
    }
   ],
   "source": [
    "# ===== label list from split =====\n",
    "label_list = sorted(df_test[\"label\"].astype(str).str.strip().unique().tolist())\n",
    "\n",
    "# ===== batch over test set =====\n",
    "n = len(df_test)\n",
    "all_preds = {}     # id -> predicted label\n",
    "all_conf = {}      # optional future use\n",
    "start_time = time.time()\n",
    "\n",
    "for b in range(0, n, BATCH_SIZE):\n",
    "    e = min(b + BATCH_SIZE, n)\n",
    "    test_block = build_test_block(df_test, b, e)\n",
    "    prompt = build_full_prompt(labels=label_list, test_data_for_prompt=test_block, is_few_shot=False)\n",
    "\n",
    "    t0 = time.time()\n",
    "    raw = call_llm(prompt)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"Batch {b}-{e} got reply in {dt:.1f}s\")\n",
    "\n",
    "    df_out = parse_output_to_df(raw)\n",
    "\n",
    "    # map back to predictions\n",
    "    for _, row in df_out.iterrows():\n",
    "        rid = int(row[\"id\"])\n",
    "        pred_norm = canonicalize(row[\"pred_raw\"], label_list)\n",
    "        if pred_norm is None:\n",
    "            pred_norm = label_list[0]  # fallback\n",
    "        all_preds[rid] = pred_norm\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Total wall time {elapsed:.1f}s for {n} items\")\n",
    "\n",
    "# ===== build final predictions frame aligned with ground truth =====\n",
    "df_eval = df_test.copy().reset_index(drop=True)\n",
    "df_eval[\"pred\"] = [all_preds.get(i, label_list[0]) for i in range(len(df_eval))]\n",
    "\n",
    "# ===== metrics =====\n",
    "metrics = evaluate(\n",
    "    y_true=df_eval[\"label\"].tolist(),\n",
    "    y_pred=df_eval[\"pred\"].tolist(),\n",
    "    labels=label_list\n",
    ")\n",
    "\n",
    "# ===== save artifacts =====\n",
    "pred_out = OUT_DIR / f\"pred_{RUN_TAG}_{'gpt_5' if llm=='GPT' else 'deepseek'}.csv\"\n",
    "df_eval.to_csv(pred_out, index=False)\n",
    "\n",
    "metrics_out = OUT_DIR / f\"metrics_{RUN_TAG}_{'gpt_5' if llm=='GPT' else 'deepseek'}.json\"\n",
    "with open(metrics_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# ===== pretty print summary =====\n",
    "print(\"\\nOverall metrics\")\n",
    "print({k: round(v, 4) for k, v in metrics.items() if isinstance(v, float)})\n",
    "\n",
    "print(\"\\nLabel wise metrics on TEST\")\n",
    "rep = metrics[\"report\"]\n",
    "for lab in label_list:\n",
    "    if lab in rep:\n",
    "        lr = rep[lab]\n",
    "        print(f\"{lab:>12}  p={lr['precision']:.3f}  r={lr['recall']:.3f}  f1={lr['f1-score']:.3f}  support={int(lr['support'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f890ff",
   "metadata": {},
   "source": [
    "#### DeepSeek No Stress: \n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.7149, 'f1_macro': 0.694, 'f1_weighted': 0.7151}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.554  r=0.738  f1=0.633  support=42\n",
    "  depression  p=0.875  r=0.634  f1=0.735  support=232\n",
    "        none  p=0.693  r=0.986  f1=0.814  support=71\n",
    "        ptsd  p=0.759  r=0.537  f1=0.629  support=41\n",
    "     suicide  p=0.569  r=0.786  f1=0.660  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5dd78",
   "metadata": {},
   "source": [
    "### GPT-4.1 with Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.5753, 'f1_macro': 0.5616, 'f1_weighted': 0.5661}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.361  r=0.714  f1=0.480  support=42\n",
    "  depression  p=0.685  r=0.591  f1=0.634  support=232\n",
    "        none  p=0.449  r=0.986  f1=0.617  support=71\n",
    "        ptsd  p=0.568  r=0.512  f1=0.538  support=41\n",
    "      stress  p=0.864  r=0.335  f1=0.483  support=227\n",
    "     suicide  p=0.504  r=0.798  f1=0.618  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9ab7f",
   "metadata": {},
   "source": [
    "### DeepSeek with Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.5911, 'f1_macro': 0.579, 'f1_weighted': 0.5903}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.378  r=0.738  f1=0.500  support=42\n",
    "  depression  p=0.717  r=0.591  f1=0.648  support=232\n",
    "        none  p=0.453  r=0.944  f1=0.612  support=71\n",
    "        ptsd  p=0.571  r=0.585  f1=0.578  support=41\n",
    "      stress  p=0.802  r=0.410  f1=0.542  support=227\n",
    "     suicide  p=0.508  r=0.714  f1=0.594  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d2733",
   "metadata": {},
   "source": [
    "### GPT-5 with Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.571, 'f1_macro': 0.5611, 'f1_weighted': 0.5738}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.337  r=0.714  f1=0.458  support=42\n",
    "  depression  p=0.797  r=0.491  f1=0.608  support=232\n",
    "        none  p=0.395  r=0.986  f1=0.565  support=71\n",
    "        ptsd  p=0.611  r=0.537  f1=0.571  support=41\n",
    "      stress  p=0.844  r=0.405  f1=0.548  support=227\n",
    "     suicide  p=0.490  r=0.833  f1=0.617  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bc2d3",
   "metadata": {},
   "source": [
    "### GPT-5 No Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.6638, 'f1_macro': 0.6621, 'f1_weighted': 0.6623}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.544  r=0.738  f1=0.626  support=42\n",
    "  depression  p=0.909  r=0.517  f1=0.659  support=232\n",
    "        none  p=0.574  r=0.986  f1=0.725  support=71\n",
    "        ptsd  p=0.880  r=0.537  f1=0.667  support=41\n",
    "     suicide  p=0.515  r=0.821  f1=0.633  support=84"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
