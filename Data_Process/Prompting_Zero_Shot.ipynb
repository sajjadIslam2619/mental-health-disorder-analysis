{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e3b51a",
   "metadata": {},
   "source": [
    "### Without Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "990b6263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 470\n",
      "                                                text       label  label_enc\n",
      "0  I had a promising academic future. I had a won...     suicide          3\n",
      "1  It seems that goalkeepers tend to be older com...        none          4\n",
      "2  \"Life has no meaning the moment you lose the i...  depression          1\n",
      "3  I got way too attached and when she ghosted me...     suicide          3\n",
      "4  I have severe severe depression, high magnitud...  depression          1\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load no-stress test split\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== locate Data_Warehouse =====\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent  # if running as script\n",
    "    except NameError:\n",
    "        script_dir = Path.cwd()                       # if running in notebook\n",
    "    data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ===== load no-stress test.csv =====\n",
    "test_path = data_warehouse / \"mental_health_splits_no_stress\" / \"test.csv\"\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Rows:\", len(df_test))\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b20afb",
   "metadata": {},
   "source": [
    "### With Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a54515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 697\n",
      "                                                text  label_norm  label_enc\n",
      "0  I use a lot of different ingredients when prep...        none          5\n",
      "1  When we are at work we joke around but we all ...      stress          4\n",
      "2  1 year ago, I left for good. I was a reddit lu...  depression          1\n",
      "3  someone once told me smoking weed and then tou...        none          5\n",
      "4  The main source of this stress is a scholarshi...      stress          4\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load no-stress test split\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== locate Data_Warehouse =====\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent  # if running as script\n",
    "    except NameError:\n",
    "        script_dir = Path.cwd()                       # if running in notebook\n",
    "    data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ===== load no-stress test.csv =====\n",
    "test_path = data_warehouse / \"mental_health_splits_with_stress\" / \"test.csv\"\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Rows:\", len(df_test))\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77549e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prompt builder for mental health classification\n",
    "# zero shot first, few shot supported if you later set is_few_shot=True\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def build_full_prompt(\n",
    "    labels: List[str],\n",
    "    test_data_for_prompt: str,\n",
    "    is_few_shot: bool = False,\n",
    "    train_data_for_prompt: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a single prompt that can contain multiple test items.\n",
    "    test_data_for_prompt should be lines like: id | text\n",
    "    if is_few_shot=True, train_data_for_prompt should be lines like: id | label | text\n",
    "    \"\"\"\n",
    "\n",
    "    instruction = \"\"\"\n",
    "Instruction\n",
    "Your task is to assign exactly one mental health label to each user text. This is a single label classification task only.\n",
    "\"\"\"\n",
    "\n",
    "    context = f\"\"\"\n",
    "Context\n",
    "This will be used in a research demo that compares large language models on mental health text classification.\n",
    "Do not provide medical advice. Do not provide crisis instructions. Only return labels.\n",
    "Each example contains\n",
    "    label the ground truth class used for evaluation\n",
    "    text a short natural language post\n",
    "\n",
    "Predefined Labels\n",
    "{\", \".join(labels)}\n",
    "\"\"\"\n",
    "\n",
    "    handling_uncertainty = \"\"\"\n",
    "Handling Uncertainty\n",
    "If the text is unclear or could reasonably match multiple categories, select the closest label by meaning.\n",
    "If there is no signal of a mental health condition, use the label none.\n",
    "\"\"\"\n",
    "\n",
    "    training_data = \"\"\"\n",
    "Training Data\n",
    "You will see a few labeled examples inside triple quotes.\n",
    "Format\n",
    "id | label | text\n",
    "\"\"\"\n",
    "\n",
    "    test_data_intro = \"\"\"\n",
    "Test Data\n",
    "You will see unlabeled items inside triple quotes.\n",
    "For each item, return one predicted label from the predefined list.\n",
    "Format\n",
    "id | text\n",
    "\"\"\"\n",
    "\n",
    "    output_format = \"\"\"\n",
    "Output Format\n",
    "Return one line per item using this exact format\n",
    "id | predicted_label | text\n",
    "Do not add explanations. Do not add extra fields. Keep the original text unchanged.\n",
    "\"\"\"\n",
    "\n",
    "    if is_few_shot:\n",
    "        full_prompt = \"\\n\\n\".join([\n",
    "            instruction.strip(),\n",
    "            context.strip(),\n",
    "            handling_uncertainty.strip(),\n",
    "            training_data.strip(),\n",
    "            f'\"\"\"\\n{train_data_for_prompt.strip()}\\n\"\"\"',\n",
    "            test_data_intro.strip(),\n",
    "            f'\"\"\"\\n{test_data_for_prompt.strip()}\\n\"\"\"',\n",
    "            output_format.strip()\n",
    "        ])\n",
    "    else:\n",
    "        full_prompt = \"\\n\\n\".join([\n",
    "            instruction.strip(),\n",
    "            context.strip(),\n",
    "            handling_uncertainty.strip(),\n",
    "            test_data_intro.strip(),\n",
    "            f'\"\"\"\\n{test_data_for_prompt.strip()}\\n\"\"\"',\n",
    "            output_format.strip()\n",
    "        ])\n",
    "\n",
    "    return full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4edca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction\n",
      "Your task is to assign exactly one mental health label to each user text. This is a single label classification task only.\n",
      "\n",
      "Context\n",
      "This will be used in a research demo that compares large language models on mental health text classification.\n",
      "Do not provide medical advice. Do not provide crisis instructions. Only return labels.\n",
      "Each example contains\n",
      "    label the ground truth class used for evaluation\n",
      "    text a short natural language post\n",
      "\n",
      "Predefined Labels\n",
      "anxiety, depression, none, ptsd, suicide\n",
      "\n",
      "Handling Uncertainty\n",
      "If the text is unclear or could reasonably match multiple categories, select the closest label by meaning.\n",
      "If there is no signal of a mental health condition, use the label none.\n",
      "\n",
      "Test Data\n",
      "You will see unlabeled items inside triple quotes.\n",
      "For each item, return one predicted label from the predefined list.\n",
      "Format\n",
      "id | text\n",
      "\n",
      "\"\"\"\n",
      "0 | I had a promising academic future. I had a wonderful, sweet partner. I had so much drive. I had so much support. People were proud of me. I have been a medical experiment for 2 years. None of the meds have helped much. I had to drop out of grad school after getting straight failing grades. The love of my life has fallen out of love with me. My grandmother, who raised me, died. I have thousands of dollars in medical debt associated with my mental illness and my only means of work is to take my clothes off and grind on strange men. I hate what my life has become. I hate what it is. I feel like I have nothing left to live for.\n",
      "1 | It seems that goalkeepers tend to be older compared to outfield players, and can even still play first team football into their early 40's. I've also heard people say that goalkeepers peak later than other players. Why is that? Do teams really value experience over potentially much faster reflexes that you would get from a younger player?\n",
      "2 | \"Life has no meaning the moment you lose the illusion of being eternal.\" This quote fucked me up. I have been constantly struggling to find meanin\n"
     ]
    }
   ],
   "source": [
    "# Build label list from your split and a compact test block\n",
    "labels = sorted(df_test[\"label\"].astype(str).str.strip().unique().tolist())\n",
    "\n",
    "# Create lines: id | text\n",
    "test_lines = []\n",
    "for i, r in df_test.reset_index(drop=True).iterrows():\n",
    "    txt = str(r[\"text\"]).replace(\"\\n\", \" \").strip()\n",
    "    test_lines.append(f\"{i} | {txt}\")\n",
    "test_block = \"\\n\".join(test_lines[:50])  # you can choose any slice size\n",
    "\n",
    "# Build the final zero shot prompt\n",
    "full_prompt = build_full_prompt(labels=labels, test_data_for_prompt=test_block, is_few_shot=False)\n",
    "print(full_prompt[:2000])  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef73ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Zero shot run and evaluation on mental_health_splits_no_stress/test.csv\n",
    "\n",
    "import os, re, json, time, math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ===== inputs already prepared earlier =====\n",
    "# df_test -> loaded from Data_Warehouse/mental_health_splits_no_stress/test.csv\n",
    "# build_full_prompt(...) -> from the previous message\n",
    "\n",
    "# ===== user knobs =====\n",
    "llm = \"GPT\"            # \"GPT\" or DeepSeek\n",
    "BATCH_SIZE = 5        # items per request\n",
    "TEMPERATURE = 0.0\n",
    "TOP_P = 1.0\n",
    "RUN_TAG = \"zero_shot_nostress_test\"\n",
    "\n",
    "OUT_DIR = Path(\"llm_runs_zero_shot\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===== model routing exactly as you shared =====\n",
    "if llm == \"GPT\":\n",
    "    api_key = os.getenv(\"GPT_API_KEY\")\n",
    "    api_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    model_name = \"gpt-5\"\n",
    "else:\n",
    "    api_key = os.getenv(\"DS_API_KEY\")\n",
    "    api_url = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "    model_name = \"deepseek-chat\" #deepseek-reasoner\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a51b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== helpers =====\n",
    "def normalize_label(s: str) -> str:\n",
    "    return \" \".join(str(s).strip().lower().split())\n",
    "\n",
    "def canonicalize(pred_label: str, allowed: list[str]) -> str | None:\n",
    "    \"\"\"map a predicted label string to one of the allowed labels\"\"\"\n",
    "    if pred_label is None:\n",
    "        return None\n",
    "    norm = normalize_label(pred_label)\n",
    "    allowed_norm = {normalize_label(a): a for a in allowed}\n",
    "    if norm in allowed_norm:\n",
    "        return allowed_norm[norm]\n",
    "    # simple token overlap fallback\n",
    "    toks = set(norm.split())\n",
    "    if not toks:\n",
    "        return None\n",
    "    best, best_score = None, -1\n",
    "    for a in allowed:\n",
    "        atoks = set(normalize_label(a).split())\n",
    "        score = len(toks & atoks)\n",
    "        if score > best_score:\n",
    "            best, best_score = a, score\n",
    "    return best\n",
    "\n",
    "def build_test_block(df: pd.DataFrame, start: int, end: int) -> str:\n",
    "    lines = []\n",
    "    for i, r in df.iloc[start:end].reset_index(drop=True).iterrows():\n",
    "        txt = str(r[\"text\"]).replace(\"\\n\", \" \").strip()\n",
    "        lines.append(f\"{start+i} | {txt}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "line_pat = re.compile(r\"^\\s*(\\d+)\\s*\\|\\s*([^\\|]+?)\\s*\\|\\s*(.*)$\")\n",
    "\n",
    "def parse_output_to_df(raw_text: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    expects lines: id | predicted_label | text\n",
    "    returns a dataframe with columns: id, pred, text\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for line in raw_text.splitlines():\n",
    "        m = line_pat.match(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        idx = int(m.group(1))\n",
    "        pred = m.group(2).strip()\n",
    "        txt  = m.group(3).strip()\n",
    "        rows.append({\"id\": idx, \"pred_raw\": pred, \"text_out\": txt})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a careful labeler. Classify mental health text. Do not give advice.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P\n",
    "    }\n",
    "    for attempt in range(5):\n",
    "        r = requests.post(api_url, headers=headers, json=data, timeout=120)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        time.sleep(1.5 * (attempt + 1))\n",
    "    #raise RuntimeError(f\"LLM API error: {r.status_code} {r.text}\")\n",
    "    raise RuntimeError(\n",
    "        f\"LLM API error with model '{model_name}': {r.status_code} {r.text}\"\n",
    "    )\n",
    "\n",
    "def evaluate(y_true: list[str], y_pred: list[str], labels: list[str]) -> dict:\n",
    "    return {\n",
    "        \"n\": len(y_true),\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"report\": classification_report(y_true, y_pred, labels=labels, output_dict=True),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred, labels=labels).tolist(),\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8492d2d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "LLM API error with model 'gpt-5': 400 {\n  \"error\": {\n    \"message\": \"Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.\",\n    \"type\": \"invalid_request_error\",\n    \"param\": \"temperature\",\n    \"code\": \"unsupported_value\"\n  }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m prompt = build_full_prompt(labels=label_list, test_data_for_prompt=test_block, is_few_shot=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     15\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m raw = \u001b[43mcall_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m dt = time.time() - t0\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m got reply in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mcall_llm\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m     64\u001b[39m     time.sleep(\u001b[32m1.5\u001b[39m * (attempt + \u001b[32m1\u001b[39m))\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m#raise RuntimeError(f\"LLM API error: {r.status_code} {r.text}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     67\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM API error with model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: LLM API error with model 'gpt-5': 400 {\n  \"error\": {\n    \"message\": \"Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.\",\n    \"type\": \"invalid_request_error\",\n    \"param\": \"temperature\",\n    \"code\": \"unsupported_value\"\n  }\n}"
     ]
    }
   ],
   "source": [
    "# ===== label list from split =====\n",
    "label_list = sorted(df_test[\"label\"].astype(str).str.strip().unique().tolist())\n",
    "\n",
    "# ===== batch over test set =====\n",
    "n = len(df_test)\n",
    "all_preds = {}     # id -> predicted label\n",
    "all_conf = {}      # optional future use\n",
    "start_time = time.time()\n",
    "\n",
    "for b in range(0, n, BATCH_SIZE):\n",
    "    e = min(b + BATCH_SIZE, n)\n",
    "    test_block = build_test_block(df_test, b, e)\n",
    "    prompt = build_full_prompt(labels=label_list, test_data_for_prompt=test_block, is_few_shot=False)\n",
    "\n",
    "    t0 = time.time()\n",
    "    raw = call_llm(prompt)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"Batch {b}-{e} got reply in {dt:.1f}s\")\n",
    "\n",
    "    df_out = parse_output_to_df(raw)\n",
    "\n",
    "    # map back to predictions\n",
    "    for _, row in df_out.iterrows():\n",
    "        rid = int(row[\"id\"])\n",
    "        pred_norm = canonicalize(row[\"pred_raw\"], label_list)\n",
    "        if pred_norm is None:\n",
    "            pred_norm = label_list[0]  # fallback\n",
    "        all_preds[rid] = pred_norm\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Total wall time {elapsed:.1f}s for {n} items\")\n",
    "\n",
    "# ===== build final predictions frame aligned with ground truth =====\n",
    "df_eval = df_test.copy().reset_index(drop=True)\n",
    "df_eval[\"pred\"] = [all_preds.get(i, label_list[0]) for i in range(len(df_eval))]\n",
    "\n",
    "# ===== metrics =====\n",
    "metrics = evaluate(\n",
    "    y_true=df_eval[\"label\"].tolist(),\n",
    "    y_pred=df_eval[\"pred\"].tolist(),\n",
    "    labels=label_list\n",
    ")\n",
    "\n",
    "# ===== save artifacts =====\n",
    "pred_out = OUT_DIR / f\"pred_{RUN_TAG}_{'gpt_5' if llm=='GPT' else 'deepseek'}.csv\"\n",
    "df_eval.to_csv(pred_out, index=False)\n",
    "\n",
    "metrics_out = OUT_DIR / f\"metrics_{RUN_TAG}_{'gpt_5' if llm=='GPT' else 'deepseek'}.json\"\n",
    "with open(metrics_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# ===== pretty print summary =====\n",
    "print(\"\\nOverall metrics\")\n",
    "print({k: round(v, 4) for k, v in metrics.items() if isinstance(v, float)})\n",
    "\n",
    "print(\"\\nLabel wise metrics on TEST\")\n",
    "rep = metrics[\"report\"]\n",
    "for lab in label_list:\n",
    "    if lab in rep:\n",
    "        lr = rep[lab]\n",
    "        print(f\"{lab:>12}  p={lr['precision']:.3f}  r={lr['recall']:.3f}  f1={lr['f1-score']:.3f}  support={int(lr['support'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f890ff",
   "metadata": {},
   "source": [
    "#### DeepSeek No Stress: \n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.7149, 'f1_macro': 0.694, 'f1_weighted': 0.7151}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.554  r=0.738  f1=0.633  support=42\n",
    "  depression  p=0.875  r=0.634  f1=0.735  support=232\n",
    "        none  p=0.693  r=0.986  f1=0.814  support=71\n",
    "        ptsd  p=0.759  r=0.537  f1=0.629  support=41\n",
    "     suicide  p=0.569  r=0.786  f1=0.660  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5dd78",
   "metadata": {},
   "source": [
    "### GPT-4.1 with Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.5753, 'f1_macro': 0.5616, 'f1_weighted': 0.5661}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.361  r=0.714  f1=0.480  support=42\n",
    "  depression  p=0.685  r=0.591  f1=0.634  support=232\n",
    "        none  p=0.449  r=0.986  f1=0.617  support=71\n",
    "        ptsd  p=0.568  r=0.512  f1=0.538  support=41\n",
    "      stress  p=0.864  r=0.335  f1=0.483  support=227\n",
    "     suicide  p=0.504  r=0.798  f1=0.618  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9ab7f",
   "metadata": {},
   "source": [
    "### DeepSeek with Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.5911, 'f1_macro': 0.579, 'f1_weighted': 0.5903}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.378  r=0.738  f1=0.500  support=42\n",
    "  depression  p=0.717  r=0.591  f1=0.648  support=232\n",
    "        none  p=0.453  r=0.944  f1=0.612  support=71\n",
    "        ptsd  p=0.571  r=0.585  f1=0.578  support=41\n",
    "      stress  p=0.802  r=0.410  f1=0.542  support=227\n",
    "     suicide  p=0.508  r=0.714  f1=0.594  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d2733",
   "metadata": {},
   "source": [
    "### GPT-5 with Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.571, 'f1_macro': 0.5611, 'f1_weighted': 0.5738}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.337  r=0.714  f1=0.458  support=42\n",
    "  depression  p=0.797  r=0.491  f1=0.608  support=232\n",
    "        none  p=0.395  r=0.986  f1=0.565  support=71\n",
    "        ptsd  p=0.611  r=0.537  f1=0.571  support=41\n",
    "      stress  p=0.844  r=0.405  f1=0.548  support=227\n",
    "     suicide  p=0.490  r=0.833  f1=0.617  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bc2d3",
   "metadata": {},
   "source": [
    "### GPT-5 No Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.6638, 'f1_macro': 0.6621, 'f1_weighted': 0.6623}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.544  r=0.738  f1=0.626  support=42\n",
    "  depression  p=0.909  r=0.517  f1=0.659  support=232\n",
    "        none  p=0.574  r=0.986  f1=0.725  support=71\n",
    "        ptsd  p=0.880  r=0.537  f1=0.667  support=41\n",
    "     suicide  p=0.515  r=0.821  f1=0.633  support=84"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_gpu_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
