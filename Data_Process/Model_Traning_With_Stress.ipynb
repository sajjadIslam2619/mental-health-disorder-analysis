{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b8ccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6970 rows from d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\combined_mental_condition_dataset_balanced.csv\n",
      "Dropped 1 empty text or label rows\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "anxiety        416\n",
      "depression    2322\n",
      "none           706\n",
      "ptsd           414\n",
      "stress        2274\n",
      "suicide        838\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample rows:\n",
      "                                                   text       label\n",
      "132   I always sound like Iâ€™m about to cry. I get re...     anxiety\n",
      "3236  How do migraines work? Yesterday I had an AWFU...        none\n",
      "2168  I have fallen down further, when I thought I c...  depression\n",
      "4086  I've been planning on cleaning but then stupid...      stress\n",
      "4410  This week has been really rough, for some reas...      stress\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ===== config =====\n",
    "DATASET_NAME = \"combined_mental_condition_dataset_balanced.csv\"\n",
    "\n",
    "# Option A: set this if you know the path\n",
    "# data_warehouse = Path(r\"/absolute/path/to/Data_Warehouse\")\n",
    "\n",
    "# Option B: auto locate Data_Warehouse by walking up from current working folder\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate a folder named Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent  # running as a script\n",
    "    except NameError:\n",
    "        script_dir = Path.cwd()                       # running in a notebook\n",
    "    data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ===== load =====\n",
    "data_path = data_warehouse / DATASET_NAME\n",
    "df_all = pd.read_csv(data_path)\n",
    "\n",
    "# ===== basic checks =====\n",
    "required = {\"text\", \"label\"}\n",
    "missing = required - set(df_all.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# coerce types and minimal cleanup\n",
    "df_all[\"text\"] = df_all[\"text\"].astype(str)\n",
    "df_all[\"label\"] = df_all[\"label\"].astype(str).str.strip()\n",
    "\n",
    "# drop any empty text or label\n",
    "before = len(df_all)\n",
    "df_all = df_all[(df_all[\"text\"].str.strip() != \"\") & (df_all[\"label\"].str.strip() != \"\")]\n",
    "dropped = before - len(df_all)\n",
    "\n",
    "# expected label set and sanity checks\n",
    "expected_labels = {\"anxiety\", \"depression\", \"ptsd\", \"suicide\", \"stress\", \"none\"}\n",
    "observed = set(df_all[\"label\"].str.lower().unique())\n",
    "missing_expected = expected_labels - observed\n",
    "\n",
    "if missing_expected:\n",
    "    print(\"Warning: the following expected labels were not found:\", sorted(missing_expected))\n",
    "\n",
    "# final echo\n",
    "print(f\"Loaded {len(df_all)} rows from {data_path}\")\n",
    "if dropped:\n",
    "    print(f\"Dropped {dropped} empty text or label rows\")\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df_all[\"label\"].str.lower().value_counts().reindex(sorted(observed), fill_value=0))\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(df_all.sample(min(5, len(df_all)), random_state=42)[[\"text\", \"label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5c2dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes and ids:\n",
      "anxiety -> 0\n",
      "depression -> 1\n",
      "ptsd -> 2\n",
      "suicide -> 3\n",
      "stress -> 4\n",
      "none -> 5\n",
      "\n",
      "Final split sizes\n",
      "Train: 5576  Validation: 697  Test: 697\n",
      "\n",
      "Train size: 5576\n",
      "label_enc\n",
      "0     332\n",
      "1    1858\n",
      "2     332\n",
      "3     670\n",
      "4    1820\n",
      "5     564\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train label counts by name:\n",
      "label_norm\n",
      "anxiety        332\n",
      "depression    1858\n",
      "ptsd           332\n",
      "suicide        670\n",
      "stress        1820\n",
      "none           564\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation size: 697\n",
      "label_enc\n",
      "0     42\n",
      "1    232\n",
      "2     41\n",
      "3     84\n",
      "4    227\n",
      "5     71\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation label counts by name:\n",
      "label_norm\n",
      "anxiety        42\n",
      "depression    232\n",
      "ptsd           41\n",
      "suicide        84\n",
      "stress        227\n",
      "none           71\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test size: 697\n",
      "label_enc\n",
      "0     42\n",
      "1    232\n",
      "2     41\n",
      "3     84\n",
      "4    227\n",
      "5     71\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label counts by name:\n",
      "label_norm\n",
      "anxiety        42\n",
      "depression    232\n",
      "ptsd           41\n",
      "suicide        84\n",
      "stress        227\n",
      "none           71\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved splits and label mapping to D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_with_stress\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===== config =====\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.10\n",
    "VAL_SIZE = 0.10   # portion of the full dataset\n",
    "\n",
    "# ===== input =====\n",
    "# df_all is assumed to exist with columns [\"text\", \"label\"]\n",
    "\n",
    "# ===== fixed label to id mapping with 'none' = 5 =====\n",
    "canonical_order = [\"anxiety\", \"depression\", \"ptsd\", \"suicide\", \"stress\", \"none\"]\n",
    "class_to_id = {\n",
    "    \"anxiety\": 0,\n",
    "    \"depression\": 1,\n",
    "    \"ptsd\": 2,\n",
    "    \"suicide\": 3,\n",
    "    \"stress\": 4,\n",
    "    \"none\": 5,\n",
    "}\n",
    "id_to_class = {v: k for k, v in class_to_id.items()}\n",
    "\n",
    "# normalize and map\n",
    "df_all = df_all.copy()\n",
    "df_all[\"label_norm\"] = df_all[\"label\"].str.lower().str.strip()\n",
    "unknown = sorted(set(df_all[\"label_norm\"].unique()) - set(class_to_id.keys()))\n",
    "if unknown:\n",
    "    raise ValueError(f\"Found unknown labels in dataset: {unknown}\")\n",
    "\n",
    "df_all[\"label_enc\"] = df_all[\"label_norm\"].map(class_to_id)\n",
    "\n",
    "print(\"Classes and ids:\")\n",
    "for name in canonical_order:\n",
    "    print(f\"{name} -> {class_to_id[name]}\")\n",
    "\n",
    "# ===== split 80 10 10 with stratify =====\n",
    "df_trainval, df_test = train_test_split(\n",
    "    df_all,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=df_all[\"label_enc\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "val_size_relative = VAL_SIZE / (1.0 - TEST_SIZE)  # 0.10 / 0.90\n",
    "df_train, df_val = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=val_size_relative,\n",
    "    stratify=df_trainval[\"label_enc\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# ===== quick checks =====\n",
    "def show_split_stats(name, frame):\n",
    "    print(f\"\\n{name} size: {len(frame)}\")\n",
    "    print(frame[\"label_enc\"].value_counts().sort_index())\n",
    "    counts = frame[\"label_norm\"].value_counts().reindex(canonical_order, fill_value=0)\n",
    "    print(\"\\n\" + name + \" label counts by name:\")\n",
    "    print(counts)\n",
    "\n",
    "print(\"\\nFinal split sizes\")\n",
    "print(f\"Train: {len(df_train)}  Validation: {len(df_val)}  Test: {len(df_test)}\")\n",
    "show_split_stats(\"Train\", df_train)\n",
    "show_split_stats(\"Validation\", df_val)\n",
    "show_split_stats(\"Test\", df_test)\n",
    "\n",
    "# ===== save artifacts =====\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    data_warehouse = Path.cwd()\n",
    "\n",
    "out_dir = data_warehouse / \"mental_health_splits_with_stress\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cols_to_save = [\"text\", \"label_norm\", \"label_enc\"]\n",
    "df_train[cols_to_save].to_csv(out_dir / \"train.csv\", index=False)\n",
    "df_val[cols_to_save].to_csv(out_dir / \"val.csv\", index=False)\n",
    "df_test[cols_to_save].to_csv(out_dir / \"test.csv\", index=False)\n",
    "\n",
    "pd.Series(class_to_id).rename(\"id\").to_csv(out_dir / \"label_classes.csv\")\n",
    "\n",
    "print(f\"\\nSaved splits and label mapping to {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42730832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb6676fd5a64824b6fe13cd7a16ff1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2719b2fbe61444fa8a4cd4088911567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5745f6f106cf4f78818fe570023b29c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shapes: (5576, 1024) (697, 1024) (697, 1024)\n",
      "Embeddings computed on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nipua\\AppData\\Local\\anaconda3\\envs\\py312_xai\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression validation ===\n",
      "accuracy: 0.7690\n",
      "precision_macro: 0.7209  precision_weighted: 0.7559\n",
      "recall_macro: 0.6732  recall_weighted: 0.7690\n",
      "f1_macro: 0.6905  f1_weighted: 0.7583\n",
      "roc_auc_ovr: 0.9473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.5714    0.6154        42\n",
      "           1     0.7395    0.8319    0.7830       232\n",
      "           2     0.6071    0.4146    0.4928        41\n",
      "           3     0.5254    0.3690    0.4336        84\n",
      "           4     0.8423    0.8943    0.8675       227\n",
      "           5     0.9444    0.9577    0.9510        71\n",
      "\n",
      "    accuracy                         0.7690       697\n",
      "   macro avg     0.7209    0.6732    0.6905       697\n",
      "weighted avg     0.7559    0.7690    0.7583       697\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 24  10   3   1   4   0]\n",
      " [  3 193   2  23  11   0]\n",
      " [  5   3  17   3  13   0]\n",
      " [  1  42   0  31   8   2]\n",
      " [  3  12   6   1 203   2]\n",
      " [  0   1   0   0   2  68]]\n",
      "\n",
      "=== Logistic Regression test ===\n",
      "accuracy: 0.7920\n",
      "precision_macro: 0.7873  precision_weighted: 0.7889\n",
      "recall_macro: 0.6996  recall_weighted: 0.7920\n",
      "f1_macro: 0.7290  f1_weighted: 0.7811\n",
      "roc_auc_ovr: 0.9525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.5000    0.5833        42\n",
      "           1     0.7390    0.8664    0.7976       232\n",
      "           2     0.7857    0.5366    0.6377        41\n",
      "           3     0.7347    0.4286    0.5414        84\n",
      "           4     0.8320    0.8943    0.8620       227\n",
      "           5     0.9324    0.9718    0.9517        71\n",
      "\n",
      "    accuracy                         0.7920       697\n",
      "   macro avg     0.7873    0.6996    0.7290       697\n",
      "weighted avg     0.7889    0.7920    0.7811       697\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 21  12   1   0   8   0]\n",
      " [  4 201   0  11  16   0]\n",
      " [  4   4  22   1  10   0]\n",
      " [  0  39   1  36   6   2]\n",
      " [  1  15   4   1 203   3]\n",
      " [  0   1   0   0   1  69]]\n",
      "\n",
      "=== Linear SVM validation ===\n",
      "accuracy: 0.7891\n",
      "precision_macro: 0.7534  precision_weighted: 0.7791\n",
      "recall_macro: 0.6922  recall_weighted: 0.7891\n",
      "f1_macro: 0.7142  f1_weighted: 0.7794\n",
      "roc_auc_ovr: 0.9462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.5714    0.6154        42\n",
      "           1     0.7628    0.8319    0.7959       232\n",
      "           2     0.6800    0.4146    0.5152        41\n",
      "           3     0.6129    0.4524    0.5205        84\n",
      "           4     0.8400    0.9251    0.8805       227\n",
      "           5     0.9577    0.9577    0.9577        71\n",
      "\n",
      "    accuracy                         0.7891       697\n",
      "   macro avg     0.7534    0.6922    0.7142       697\n",
      "weighted avg     0.7791    0.7891    0.7794       697\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 24  11   1   1   5   0]\n",
      " [  3 193   3  19  14   0]\n",
      " [  6   3  17   3  12   0]\n",
      " [  0  37   0  38   7   2]\n",
      " [  3   8   4   1 210   1]\n",
      " [  0   1   0   0   2  68]]\n",
      "\n",
      "=== Linear SVM test ===\n",
      "accuracy: 0.7920\n",
      "precision_macro: 0.7947  precision_weighted: 0.7891\n",
      "recall_macro: 0.7018  recall_weighted: 0.7920\n",
      "f1_macro: 0.7350  f1_weighted: 0.7833\n",
      "roc_auc_ovr: 0.9459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7097    0.5238    0.6027        42\n",
      "           1     0.7452    0.8448    0.7919       232\n",
      "           2     0.8800    0.5366    0.6667        41\n",
      "           3     0.6724    0.4643    0.5493        84\n",
      "           4     0.8313    0.9119    0.8697       227\n",
      "           5     0.9296    0.9296    0.9296        71\n",
      "\n",
      "    accuracy                         0.7920       697\n",
      "   macro avg     0.7947    0.7018    0.7350       697\n",
      "weighted avg     0.7891    0.7920    0.7833       697\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 22  10   1   0   9   0]\n",
      " [  3 196   0  17  16   0]\n",
      " [  5   4  22   0  10   0]\n",
      " [  0  38   1  39   4   2]\n",
      " [  1  14   1   1 207   3]\n",
      " [  0   1   0   1   3  66]]\n",
      "\n",
      "Saved models and metrics to D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_with_stress\\baselines_all_roberta_large_v1_none5\n",
      "  split       model  accuracy  precision_macro  precision_weighted  \\\n",
      "0   val      logreg  0.769010         0.720911            0.755878   \n",
      "1   val  linear_svm  0.789096         0.753360            0.779088   \n",
      "2  test      logreg  0.791966         0.787296            0.788851   \n",
      "3  test  linear_svm  0.791966         0.794707            0.789064   \n",
      "\n",
      "   recall_macro  recall_weighted  f1_macro  f1_weighted  roc_auc_ovr  \n",
      "0      0.673171         0.769010  0.690539     0.758347     0.947313  \n",
      "1      0.692199         0.789096  0.714202     0.779356     0.946153  \n",
      "2      0.699607         0.791966  0.728951     0.781078     0.952476  \n",
      "3      0.701830         0.791966  0.734991     0.783282     0.945919  \n"
     ]
    }
   ],
   "source": [
    "# ===== baselines with GPU for embeddings and calibrated SVM for AUC =====\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Optional GPU accelerated models via RAPIDS cuML if available\n",
    "use_cuml = False\n",
    "try:\n",
    "    import cuml\n",
    "    from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
    "    from cuml.svm import SVC as cuSVC\n",
    "    use_cuml = True\n",
    "except Exception:\n",
    "    use_cuml = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ===== locate Data_Warehouse and load splits if needed =====\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate a folder named Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        script_dir = Path.cwd()\n",
    "    data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "splits_dir = data_warehouse / \"mental_health_splits_with_stress\"\n",
    "\n",
    "# If df_train df_val df_test are not already present, load them\n",
    "globals_present = all(name in globals() for name in [\"df_train\", \"df_val\", \"df_test\"])\n",
    "if not globals_present:\n",
    "    df_train = pd.read_csv(splits_dir / \"train.csv\")\n",
    "    df_val   = pd.read_csv(splits_dir / \"val.csv\")\n",
    "    df_test  = pd.read_csv(splits_dir / \"test.csv\")\n",
    "\n",
    "# These columns are saved by your Step 2 script\n",
    "required_cols = {\"text\", \"label_norm\", \"label_enc\"}\n",
    "assert required_cols.issubset(df_train.columns)\n",
    "assert required_cols.issubset(df_val.columns)\n",
    "assert required_cols.issubset(df_test.columns)\n",
    "\n",
    "# ===== embeddings on GPU if available =====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "st_model = SentenceTransformer(\"sentence-transformers/all-roberta-large-v1\", device=device)\n",
    "\n",
    "def embed_texts(model, texts, batch_size=256, show_progress=True):\n",
    "    return model.encode(\n",
    "        list(texts),\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=show_progress,\n",
    "    )\n",
    "\n",
    "X_train = embed_texts(st_model, df_train[\"text\"])\n",
    "y_train = df_train[\"label_enc\"].to_numpy()\n",
    "\n",
    "X_val   = embed_texts(st_model, df_val[\"text\"])\n",
    "y_val   = df_val[\"label_enc\"].to_numpy()\n",
    "\n",
    "X_test  = embed_texts(st_model, df_test[\"text\"])\n",
    "y_test  = df_test[\"label_enc\"].to_numpy()\n",
    "\n",
    "print(\"Embedding shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Embeddings computed on device:\", device)\n",
    "\n",
    "# ===== helper to evaluate =====\n",
    "def evaluate_model(name, model, X, y_true, proba=None):\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    prec_macro = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    prec_weighted = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    rec_macro = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    rec_weighted = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    auc = None\n",
    "    if proba is not None:\n",
    "        auc = roc_auc_score(y_true, proba, multi_class=\"ovr\")\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"accuracy: {acc:.4f}\")\n",
    "    print(f\"precision_macro: {prec_macro:.4f}  precision_weighted: {prec_weighted:.4f}\")\n",
    "    print(f\"recall_macro: {rec_macro:.4f}  recall_weighted: {rec_weighted:.4f}\")\n",
    "    print(f\"f1_macro: {f1_macro:.4f}  f1_weighted: {f1_weighted:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"roc_auc_ovr: {auc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision_macro\": float(prec_macro),\n",
    "        \"precision_weighted\": float(prec_weighted),\n",
    "        \"recall_macro\": float(rec_macro),\n",
    "        \"recall_weighted\": float(rec_weighted),\n",
    "        \"f1_macro\": float(f1_macro),\n",
    "        \"f1_weighted\": float(f1_weighted),\n",
    "        \"roc_auc_ovr\": float(auc) if auc is not None else None,\n",
    "    }\n",
    "\n",
    "# ===== Logistic Regression =====\n",
    "if use_cuml:\n",
    "    logreg = cuLogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=2000,\n",
    "        tol=1e-4,\n",
    "        fit_intercept=True,\n",
    "        multi_class=\"ovr\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=0,\n",
    "    )\n",
    "    logreg.fit(X_train, y_train)\n",
    "    val_proba_lr = logreg.predict_proba(X_val)\n",
    "    test_proba_lr = logreg.predict_proba(X_test)\n",
    "else:\n",
    "    logreg = LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"saga\",\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=2000,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    logreg.fit(X_train, y_train)\n",
    "    val_proba_lr = logreg.predict_proba(X_val)\n",
    "    test_proba_lr = logreg.predict_proba(X_test)\n",
    "\n",
    "metrics_val_lr  = evaluate_model(\"Logistic Regression validation\", logreg, X_val, y_val, proba=val_proba_lr)\n",
    "metrics_test_lr = evaluate_model(\"Logistic Regression test\", logreg, X_test, y_test, proba=test_proba_lr)\n",
    "\n",
    "# ===== Linear SVM with probability calibration for valid multiclass AUC =====\n",
    "if use_cuml:\n",
    "    svm_base = cuSVC(C=1.0, kernel=\"linear\", probability=True, random_state=RANDOM_STATE)\n",
    "    svm_base.fit(X_train, y_train)\n",
    "    val_proba_svm = svm_base.predict_proba(X_val)\n",
    "    test_proba_svm = svm_base.predict_proba(X_test)\n",
    "    svm_for_pred = svm_base\n",
    "else:\n",
    "    svm_linear = LinearSVC(C=1.0, random_state=RANDOM_STATE)\n",
    "    svm = CalibratedClassifierCV(svm_linear, method=\"sigmoid\", cv=5)\n",
    "    svm.fit(X_train, y_train)\n",
    "    val_proba_svm = svm.predict_proba(X_val)\n",
    "    test_proba_svm = svm.predict_proba(X_test)\n",
    "    svm_for_pred = svm\n",
    "\n",
    "metrics_val_svm  = evaluate_model(\"Linear SVM validation\", svm_for_pred, X_val, y_val, proba=val_proba_svm)\n",
    "metrics_test_svm = evaluate_model(\"Linear SVM test\", svm_for_pred, X_test, y_test, proba=test_proba_svm)\n",
    "\n",
    "# ===== persist artifacts and summary =====\n",
    "art_dir = splits_dir / \"baselines_all_roberta_large_v1_none5\"\n",
    "art_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "if use_cuml:\n",
    "    joblib.dump(logreg, art_dir / \"logreg_cuml.joblib\")\n",
    "    joblib.dump(svm_for_pred, art_dir / \"linear_svm_cuml.joblib\")\n",
    "else:\n",
    "    joblib.dump(logreg, art_dir / \"logreg.joblib\")\n",
    "    joblib.dump(svm_for_pred, art_dir / \"linear_svm_calibrated.joblib\")\n",
    "\n",
    "# Save metrics and a compact table for quick comparison\n",
    "metrics = {\n",
    "    \"val_logreg\": metrics_val_lr,\n",
    "    \"test_logreg\": metrics_test_lr,\n",
    "    \"val_linear_svm\": metrics_val_svm,\n",
    "    \"test_linear_svm\": metrics_test_svm,\n",
    "}\n",
    "with open(art_dir / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "summary_rows = []\n",
    "for split in [\"val\", \"test\"]:\n",
    "    for model_name, m in [(\"logreg\", metrics[f\"{split}_logreg\"]), (\"linear_svm\", metrics[f\"{split}_linear_svm\"])]:\n",
    "        summary_rows.append({\n",
    "            \"split\": split,\n",
    "            \"model\": model_name,\n",
    "            **m\n",
    "        })\n",
    "summary = pd.DataFrame(summary_rows)\n",
    "summary.to_csv(art_dir / \"metrics_summary.csv\", index=False)\n",
    "\n",
    "print(f\"\\nSaved models and metrics to {art_dir.resolve()}\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ace05ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping used:\n",
      "0: anxiety\n",
      "1: depression\n",
      "2: ptsd\n",
      "3: suicide\n",
      "4: stress\n",
      "5: none\n",
      "Torch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: [2.799196720123291, 0.5001794099807739, 2.799196720123291, 1.3870646953582764, 0.5106227397918701, 1.647754192352295]\n",
      "\n",
      "Starting trainer.train() ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_4668\\4242656520.py:201: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = TrainerClass(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 16:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.506700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.733500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.370200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.243600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 | epoch 0.01 | loss 1.9689 | lr 0.000020\n",
      "step 50 | epoch 0.57 | loss 1.5067 | lr 0.000018\n",
      "step 100 | epoch 1.14 | loss 1.0325 | lr 0.000016\n",
      "step 150 | epoch 1.71 | loss 0.7335 | lr 0.000013\n",
      "step 200 | epoch 2.28 | loss 0.5788 | lr 0.000011\n",
      "step 250 | epoch 2.85 | loss 0.4717 | lr 0.000009\n",
      "step 300 | epoch 3.41 | loss 0.3702 | lr 0.000006\n",
      "step 350 | epoch 3.99 | loss 0.3128 | lr 0.000004\n",
      "step 400 | epoch 4.55 | loss 0.2436 | lr 0.000002\n",
      "step 440 | epoch 5.00\n",
      "Finished trainer.train()\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics: {'accuracy': 0.7661406025824964, 'precision_macro': 0.70922170074391, 'precision_weighted': 0.7898243363525839, 'recall_macro': 0.7561041838909265, 'recall_weighted': 0.7661406025824964, 'f1_macro': 0.7252895924914541, 'f1_weighted': 0.7729052581721234, 'roc_auc_ovr': 0.9522017216091903}\n",
      "Test metrics: {'accuracy': 0.7919655667144907, 'precision_macro': 0.7397003765274226, 'precision_weighted': 0.8045580865993706, 'recall_macro': 0.7680656297525918, 'recall_weighted': 0.7919655667144907, 'f1_macro': 0.7506335637887044, 'f1_weighted': 0.7959770912560766, 'roc_auc_ovr': 0.9570098201171756}\n",
      "\n",
      "Saved full model and metrics to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_with_stress\\bert_large_with_stress\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Fine tune google-bert/bert-large-uncased for multiclass classification\n",
    "# using the with_stress splits and label ids:\n",
    "# anxiety 0, depression 1, ptsd 2, suicide 3, stress 4, none 5\n",
    "# ==========================================\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------- progress visibility and windows safety ----------\n",
    "os.environ.pop(\"HF_DISABLE_PROGRESS_BARS\", None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "class ConsoleLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        parts = []\n",
    "        if state.global_step is not None: parts.append(f\"step {state.global_step}\")\n",
    "        if state.epoch is not None: parts.append(f\"epoch {state.epoch:.2f}\")\n",
    "        if \"loss\" in logs: parts.append(f\"loss {logs['loss']:.4f}\")\n",
    "        if \"learning_rate\" in logs: parts.append(f\"lr {logs['learning_rate']:.6f}\")\n",
    "        print(\" | \".join(parts), flush=True)\n",
    "\n",
    "# ---------- config ----------\n",
    "RANDOM_STATE = 42\n",
    "MODEL_NAME = \"google-bert/bert-large-uncased\"\n",
    "MAX_LENGTH = 256          # 256 is usually enough and faster than 512\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "WD = 0.01\n",
    "TRAIN_BS = 8             # per device train batch\n",
    "EVAL_BS = 16             # per device eval batch\n",
    "GRAD_ACCUM_STEPS = 8     # effective batch = 8 * 8 = 64\n",
    "USE_CLASS_WEIGHTS = True\n",
    "\n",
    "# ---------- locate Data_Warehouse ----------\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate a folder named Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    script_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ---------- load with_stress splits ----------\n",
    "split_dir = data_warehouse / \"mental_health_splits_with_stress\"\n",
    "df_train = pd.read_csv(split_dir / \"train.csv\")\n",
    "df_val   = pd.read_csv(split_dir / \"val.csv\")\n",
    "df_test  = pd.read_csv(split_dir / \"test.csv\")\n",
    "\n",
    "# accept either label_norm or label for readability\n",
    "label_col = \"label_norm\" if \"label_norm\" in df_train.columns else \"label\"\n",
    "for name, df_ in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    req = {\"text\", label_col, \"label_enc\"}\n",
    "    if not req.issubset(df_.columns):\n",
    "        raise ValueError(f\"{name} split missing required columns: {req}\")\n",
    "\n",
    "num_labels = int(df_train[\"label_enc\"].max()) + 1\n",
    "\n",
    "# build id2label and label2id from the split to stay aligned with ids\n",
    "enc_to_label = df_train[[\"label_enc\", label_col]].drop_duplicates().sort_values(\"label_enc\")\n",
    "id2label = {int(r.label_enc): str(r[label_col]) for _, r in enc_to_label.iterrows()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(\"Label mapping used:\")\n",
    "for k in range(num_labels):\n",
    "    print(f\"{k}: {id2label[k]}\")\n",
    "\n",
    "# ---------- Dataset wrapper with dynamic padding ----------\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df[\"text\"].astype(str).tolist()\n",
    "        self.labels = df[\"label_enc\"].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=False,          # dynamic padding via collator\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# ---------- tokenizer and model ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch device:\", device)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "model.to(device)\n",
    "\n",
    "# ---------- class weights ----------\n",
    "class_weights = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    counts = df_train[\"label_enc\"].value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    weights = total / (num_labels * counts)   # inverse frequency\n",
    "    class_weights = torch.tensor(weights.to_numpy(), dtype=torch.float, device=device)\n",
    "    print(\"Using class weights:\", [float(x) for x in class_weights])\n",
    "\n",
    "# ---------- custom Trainer to apply class weights ----------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels, weight=class_weights) if class_weights is not None else F.cross_entropy(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ---------- datasets and collator ----------\n",
    "train_ds = TextClsDataset(df_train, tokenizer)\n",
    "val_ds   = TextClsDataset(df_val, tokenizer)\n",
    "test_ds  = TextClsDataset(df_test, tokenizer)\n",
    "\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "# ---------- metrics ----------\n",
    "def compute_metrics_from_logits(logits, labels):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    out = {\n",
    "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
    "        \"precision_macro\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"precision_weighted\": float(precision_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"recall_macro\": float(recall_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall_weighted\": float(recall_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"f1_macro\": float(f1_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"f1_weighted\": float(f1_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "    try:\n",
    "        out[\"roc_auc_ovr\"] = float(roc_auc_score(labels, probs, multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        out[\"roc_auc_ovr\"] = None\n",
    "    return out, preds\n",
    "\n",
    "# ---------- output dir ----------\n",
    "out_dir = split_dir / \"bert_large_with_stress\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- TrainingArguments ----------\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WD,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    dataloader_num_workers=0,     # safer on windows\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    report_to=[],\n",
    "    seed=RANDOM_STATE,\n",
    "    # optim=\"adamw_torch_fused\",  # enable if available for your gpu and pytorch\n",
    ")\n",
    "\n",
    "TrainerClass = WeightedTrainer if USE_CLASS_WEIGHTS else Trainer\n",
    "trainer = TrainerClass(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer.add_callback(ConsoleLogger())\n",
    "\n",
    "# ---------- train ----------\n",
    "print(\"\\nStarting trainer.train() ...\", flush=True)\n",
    "train_result = trainer.train()\n",
    "print(\"Finished trainer.train()\", flush=True)\n",
    "trainer.save_model(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "with open(out_dir / \"train_metrics.json\", \"w\") as f:\n",
    "    json.dump({k: (float(v) if isinstance(v, (int, float)) else str(v)) for k, v in train_result.metrics.items()}, f, indent=2)\n",
    "\n",
    "# ---------- manual evaluation ----------\n",
    "val_out = trainer.predict(val_ds)\n",
    "val_metrics, _ = compute_metrics_from_logits(val_out.predictions, val_out.label_ids)\n",
    "\n",
    "test_out = trainer.predict(test_ds)\n",
    "test_metrics, test_preds = compute_metrics_from_logits(test_out.predictions, test_out.label_ids)\n",
    "\n",
    "with open(out_dir / \"val_metrics.json\", \"w\") as f:\n",
    "    json.dump(val_metrics, f, indent=2)\n",
    "with open(out_dir / \"test_metrics.json\", \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "rep = classification_report(\n",
    "    test_out.label_ids, test_preds,\n",
    "    target_names=[id2label[i] for i in range(num_labels)],\n",
    "    digits=4, zero_division=0\n",
    ")\n",
    "cm = confusion_matrix(test_out.label_ids, test_preds)\n",
    "\n",
    "with open(out_dir / \"test_classification_report.txt\", \"w\") as f:\n",
    "    f.write(rep)\n",
    "\n",
    "pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"true_{id2label[i]}\" for i in range(num_labels)],\n",
    "    columns=[f\"pred_{id2label[i]}\" for i in range(num_labels)],\n",
    ").to_csv(out_dir / \"test_confusion_matrix.csv\", index=True)\n",
    "\n",
    "print(\"\\nValidation metrics:\", val_metrics)\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "print(\"\\nSaved full model and metrics to:\", out_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc739543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping used:\n",
      "0: anxiety\n",
      "1: depression\n",
      "2: ptsd\n",
      "3: suicide\n",
      "4: stress\n",
      "5: none\n",
      "Torch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: [2.799196720123291, 0.5001794099807739, 2.799196720123291, 1.3870646953582764, 0.5106227397918701, 1.647754192352295]\n",
      "\n",
      "Starting trainer.train() ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_4668\\1503863057.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = TrainerClass(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 07:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.794900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.807300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.723100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 | epoch 0.01 | loss 1.7949 | lr 0.000020\n",
      "step 50 | epoch 0.57 | loss 1.6446 | lr 0.000018\n",
      "step 100 | epoch 1.14 | loss 1.2998 | lr 0.000016\n",
      "step 150 | epoch 1.71 | loss 1.1120 | lr 0.000013\n",
      "step 200 | epoch 2.28 | loss 0.9804 | lr 0.000011\n",
      "step 250 | epoch 2.85 | loss 0.8895 | lr 0.000009\n",
      "step 300 | epoch 3.41 | loss 0.8073 | lr 0.000006\n",
      "step 350 | epoch 3.99 | loss 0.7784 | lr 0.000004\n",
      "step 400 | epoch 4.55 | loss 0.7231 | lr 0.000002\n",
      "step 440 | epoch 5.00\n",
      "Finished trainer.train()\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics: {'accuracy': 0.7589670014347202, 'precision_macro': 0.7044509493515791, 'precision_weighted': 0.8053597583209949, 'recall_macro': 0.7698897287742175, 'recall_weighted': 0.7589670014347202, 'f1_macro': 0.7222192495132426, 'f1_weighted': 0.769913016663927, 'roc_auc_ovr': 0.9506540530295969}\n",
      "Test metrics: {'accuracy': 0.7919655667144907, 'precision_macro': 0.7339690340084853, 'precision_weighted': 0.8150682474836102, 'recall_macro': 0.7727898153367679, 'recall_weighted': 0.7919655667144907, 'f1_macro': 0.7477005138049471, 'f1_weighted': 0.7982862365338192, 'roc_auc_ovr': 0.9497015352897065}\n",
      "\n",
      "Saved MPNet model and metrics to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_with_stress\\all_mpnet_base_v2_with_stress_none5\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Fine tune sentence-transformers/all-mpnet-base-v2 for multiclass classification\n",
    "# Dataset: Data_Warehouse/mental_health_splits_with_stress\n",
    "# Label ids preserved from splits: anxiety 0, depression 1, ptsd 2, suicide 3, stress 4, none 5\n",
    "# ==========================================\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------- progress visibility & Windows safety ----------\n",
    "os.environ.pop(\"HF_DISABLE_PROGRESS_BARS\", None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "class ConsoleLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        parts = []\n",
    "        if state.global_step is not None: parts.append(f\"step {state.global_step}\")\n",
    "        if state.epoch is not None: parts.append(f\"epoch {state.epoch:.2f}\")\n",
    "        if \"loss\" in logs: parts.append(f\"loss {logs['loss']:.4f}\")\n",
    "        if \"learning_rate\" in logs: parts.append(f\"lr {logs['learning_rate']:.6f}\")\n",
    "        print(\" | \".join(parts), flush=True)\n",
    "\n",
    "# ---------- config ----------\n",
    "RANDOM_STATE = 42\n",
    "TOKENIZER_NAME = \"sentence-transformers/all-mpnet-base-v2\"  # tokenizer\n",
    "BACKBONE_NAME  = \"microsoft/mpnet-base\"                     # classification head\n",
    "MAX_LENGTH = 256        # faster than 512 for short social text\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "WD = 0.01\n",
    "TRAIN_BS = 8\n",
    "EVAL_BS = 16\n",
    "GRAD_ACCUM_STEPS = 8     # effective batch = 64\n",
    "USE_CLASS_WEIGHTS = True\n",
    "\n",
    "# ---------- locate Data_Warehouse ----------\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate a folder named Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    script_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ---------- load splits (with stress) ----------\n",
    "split_dir = data_warehouse / \"mental_health_splits_with_stress\"\n",
    "df_train = pd.read_csv(split_dir / \"train.csv\")\n",
    "df_val   = pd.read_csv(split_dir / \"val.csv\")\n",
    "df_test  = pd.read_csv(split_dir / \"test.csv\")\n",
    "\n",
    "# accept either label_norm or label (Step 2 saved label_norm)\n",
    "label_col = \"label_norm\" if \"label_norm\" in df_train.columns else \"label\"\n",
    "for name, df_ in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    req = {\"text\", label_col, \"label_enc\"}\n",
    "    if not req.issubset(df_.columns):\n",
    "        raise ValueError(f\"{name} split missing required columns: {req}\")\n",
    "\n",
    "num_labels = int(df_train[\"label_enc\"].max()) + 1\n",
    "enc_to_label = df_train[[\"label_enc\", label_col]].drop_duplicates().sort_values(\"label_enc\")\n",
    "id2label = {int(r.label_enc): str(r[label_col]) for _, r in enc_to_label.iterrows()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(\"Label mapping used:\")\n",
    "for k in range(num_labels):\n",
    "    print(f\"{k}: {id2label[k]}\")\n",
    "\n",
    "# ---------- dataset (dynamic padding) ----------\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df[\"text\"].astype(str).tolist()\n",
    "        self.labels = df[\"label_enc\"].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=False,      # pad per-batch via collator\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# ---------- tokenizer & model ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BACKBONE_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# MPNet gradient checkpointing is not always supported; enable if available\n",
    "try:\n",
    "    if getattr(model, \"supports_gradient_checkpointing\", False):\n",
    "        model.gradient_checkpointing_enable()\n",
    "except Exception as e:\n",
    "    print(f\"Gradient checkpointing not available for MPNet: {e}\")\n",
    "\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# ---------- class weights ----------\n",
    "class_weights = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    counts = df_train[\"label_enc\"].value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    weights = total / (num_labels * counts)  # inverse frequency\n",
    "    class_weights = torch.tensor(weights.to_numpy(), dtype=torch.float, device=device)\n",
    "    print(\"Using class weights:\", [float(x) for x in class_weights])\n",
    "\n",
    "# ---------- custom Trainer ----------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels, weight=class_weights) if class_weights is not None else F.cross_entropy(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ---------- datasets & collator ----------\n",
    "train_ds = TextClsDataset(df_train, tokenizer)\n",
    "val_ds   = TextClsDataset(df_val, tokenizer)\n",
    "test_ds  = TextClsDataset(df_test, tokenizer)\n",
    "\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "# ---------- metrics ----------\n",
    "def compute_metrics_from_logits(logits, labels):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    out = {\n",
    "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
    "        \"precision_macro\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"precision_weighted\": float(precision_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"recall_macro\": float(recall_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall_weighted\": float(recall_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"f1_macro\": float(f1_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"f1_weighted\": float(f1_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "    try:\n",
    "        out[\"roc_auc_ovr\"] = float(roc_auc_score(labels, probs, multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        out[\"roc_auc_ovr\"] = None\n",
    "    return out, preds\n",
    "\n",
    "# ---------- output dir ----------\n",
    "out_dir = split_dir / \"all_mpnet_base_v2_with_stress_none5\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- TrainingArguments ----------\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WD,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    dataloader_num_workers=0,   # Windows-safe\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    report_to=[],\n",
    "    seed=RANDOM_STATE,\n",
    "    # optim=\"adamw_torch_fused\",  # optional on PyTorch 2.x + recent GPUs\n",
    ")\n",
    "\n",
    "TrainerClass = WeightedTrainer if USE_CLASS_WEIGHTS else Trainer\n",
    "trainer = TrainerClass(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer.add_callback(ConsoleLogger())\n",
    "\n",
    "# ---------- train ----------\n",
    "print(\"\\nStarting trainer.train() ...\", flush=True)\n",
    "train_result = trainer.train()\n",
    "print(\"Finished trainer.train()\", flush=True)\n",
    "trainer.save_model(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "with open(out_dir / \"train_metrics.json\", \"w\") as f:\n",
    "    json.dump({k: (float(v) if isinstance(v, (int, float)) else str(v)) for k, v in train_result.metrics.items()}, f, indent=2)\n",
    "\n",
    "# ---------- manual evaluation ----------\n",
    "val_out = trainer.predict(val_ds)\n",
    "val_metrics, _ = compute_metrics_from_logits(val_out.predictions, val_out.label_ids)\n",
    "\n",
    "test_out = trainer.predict(test_ds)\n",
    "test_metrics, test_preds = compute_metrics_from_logits(test_out.predictions, test_out.label_ids)\n",
    "\n",
    "with open(out_dir / \"val_metrics.json\", \"w\") as f:\n",
    "    json.dump(val_metrics, f, indent=2)\n",
    "with open(out_dir / \"test_metrics.json\", \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "rep = classification_report(\n",
    "    test_out.label_ids, test_preds,\n",
    "    target_names=[id2label[i] for i in range(num_labels)],\n",
    "    digits=4, zero_division=0\n",
    ")\n",
    "cm = confusion_matrix(test_out.label_ids, test_preds)\n",
    "\n",
    "with open(out_dir / \"test_classification_report.txt\", \"w\") as f:\n",
    "    f.write(rep)\n",
    "\n",
    "pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"true_{id2label[i]}\" for i in range(num_labels)],\n",
    "    columns=[f\"pred_{id2label[i]}\" for i in range(num_labels)],\n",
    ").to_csv(out_dir / \"test_confusion_matrix.csv\", index=True)\n",
    "\n",
    "print(\"\\nValidation metrics:\", val_metrics)\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "print(\"\\nSaved MPNet model and metrics to:\", out_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6162602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping used:\n",
      "0: anxiety\n",
      "1: depression\n",
      "2: ptsd\n",
      "3: suicide\n",
      "4: stress\n",
      "5: none\n",
      "Torch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: [2.799196720123291, 0.5001794099807739, 2.799196720123291, 1.3870646953582764, 0.5106227397918701, 1.647754192352295]\n",
      "\n",
      "Starting trainer.train() ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_4668\\1117688418.py:259: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = TrainerClass(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='875' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [875/875 24:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.668900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.509400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.287900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.290700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.176500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.099200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 | epoch 0.01 | loss 1.9331 | lr 0.000020\n",
      "step 50 | epoch 0.29 | loss 1.4169 | lr 0.000019\n",
      "step 100 | epoch 0.57 | loss 0.7863 | lr 0.000018\n",
      "step 150 | epoch 0.86 | loss 0.6689 | lr 0.000017\n",
      "[epoch 1.00] val f1_macro=0.7529 acc=0.8063\n",
      "step 200 | epoch 1.14 | loss 0.5094 | lr 0.000015\n",
      "step 250 | epoch 1.43 | loss 0.4856 | lr 0.000014\n",
      "step 300 | epoch 1.72 | loss 0.4853 | lr 0.000013\n",
      "step 350 | epoch 2.00 | loss 0.4474 | lr 0.000012\n",
      "[epoch 2.00] val f1_macro=0.7607 acc=0.8164\n",
      "step 400 | epoch 2.29 | loss 0.3072 | lr 0.000011\n",
      "step 450 | epoch 2.57 | loss 0.2879 | lr 0.000010\n",
      "step 500 | epoch 2.86 | loss 0.2907 | lr 0.000009\n",
      "[epoch 3.00] val f1_macro=0.7829 acc=0.8307\n",
      "step 550 | epoch 3.14 | loss 0.2301 | lr 0.000007\n",
      "step 600 | epoch 3.43 | loss 0.1862 | lr 0.000006\n",
      "step 650 | epoch 3.72 | loss 0.1765 | lr 0.000005\n",
      "step 700 | epoch 4.00 | loss 0.1817 | lr 0.000004\n",
      "[epoch 4.00] val f1_macro=0.7827 acc=0.8350\n",
      "step 750 | epoch 4.29 | loss 0.0952 | lr 0.000003\n",
      "step 800 | epoch 4.57 | loss 0.0918 | lr 0.000002\n",
      "step 850 | epoch 4.86 | loss 0.0992 | lr 0.000001\n",
      "[epoch 5.00] val f1_macro=0.7832 acc=0.8350\n",
      "step 875 | epoch 5.00\n",
      "Finished trainer.train()\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "Label-wise metrics â€” TRAIN\n",
      "==========================\n",
      "              precision  recall  f1-score    support  roc_auc_ovr\n",
      "anxiety          0.9678  0.9970    0.9822   332.0000       0.9998\n",
      "depression       0.9944  0.9634    0.9787  1858.0000       0.9985\n",
      "ptsd             0.9509  0.9910    0.9705   332.0000       0.9996\n",
      "suicide          0.9156  0.9881    0.9505   670.0000       0.9980\n",
      "stress           0.9983  0.9879    0.9931  1820.0000       0.9995\n",
      "none             1.0000  1.0000    1.0000   564.0000       1.0000\n",
      "accuracy         0.9817  0.9817    0.9817     0.9817          NaN\n",
      "macro avg        0.9712  0.9879    0.9792  5576.0000          NaN\n",
      "weighted avg     0.9826  0.9817    0.9819  5576.0000          NaN\n",
      "\n",
      "========================\n",
      "Label-wise metrics â€” VAL\n",
      "========================\n",
      "              precision  recall  f1-score  support  roc_auc_ovr\n",
      "anxiety          0.6545  0.8571    0.7423   42.000       0.9801\n",
      "depression       0.8528  0.8491    0.8510  232.000       0.9708\n",
      "ptsd             0.5897  0.5610    0.5750   41.000       0.9615\n",
      "suicide          0.6310  0.6310    0.6310   84.000       0.9502\n",
      "stress           0.9395  0.8899    0.9140  227.000       0.9856\n",
      "none             0.9726  1.0000    0.9861   71.000       0.9998\n",
      "accuracy         0.8350  0.8350    0.8350    0.835          NaN\n",
      "macro avg        0.7734  0.7980    0.7832  697.000          NaN\n",
      "weighted avg     0.8391  0.8350    0.8360  697.000          NaN\n",
      "\n",
      "=========================\n",
      "Label-wise metrics â€” TEST\n",
      "=========================\n",
      "              precision  recall  f1-score   support  roc_auc_ovr\n",
      "anxiety          0.6809  0.7619    0.7191   42.0000       0.9529\n",
      "depression       0.9039  0.8922    0.8980  232.0000       0.9783\n",
      "ptsd             0.7436  0.7073    0.7250   41.0000       0.9781\n",
      "suicide          0.7753  0.8214    0.7977   84.0000       0.9652\n",
      "stress           0.9286  0.9163    0.9224  227.0000       0.9880\n",
      "none             0.9855  0.9577    0.9714   71.0000       0.9998\n",
      "accuracy         0.8795  0.8795    0.8795    0.8795          NaN\n",
      "macro avg        0.8363  0.8428    0.8389  697.0000          NaN\n",
      "weighted avg     0.8819  0.8795    0.8804  697.0000          NaN\n",
      "\n",
      "Generalization gaps (train - val): {'acc': 0.14670014347202298, 'f1_macro': 0.19593433950884132, 'f1_weighted': 0.1458991787867514}\n",
      "\n",
      "Train metrics: {'accuracy': 0.9817073170731707, 'precision_macro': 0.9711852224651626, 'precision_weighted': 0.9826268424161211, 'recall_macro': 0.9878875172717185, 'recall_weighted': 0.9817073170731707, 'f1_macro': 0.9791561069868568, 'f1_weighted': 0.9818730603906228, 'roc_auc_ovr': 0.9992133353738244}\n",
      "Val metrics: {'accuracy': 0.8350071736011477, 'precision_macro': 0.773365483583706, 'precision_weighted': 0.8390996029388941, 'recall_macro': 0.7980127700492518, 'recall_weighted': 0.8350071736011477, 'f1_macro': 0.7832217674780155, 'f1_weighted': 0.8359738816038714, 'roc_auc_ovr': 0.974675144298052}\n",
      "Test metrics: {'accuracy': 0.8794835007173601, 'precision_macro': 0.8362884187080905, 'precision_weighted': 0.8818867128337583, 'recall_macro': 0.842822970693169, 'recall_weighted': 0.8794835007173601, 'f1_macro': 0.8389433261884545, 'f1_weighted': 0.8803947198142223, 'roc_auc_ovr': 0.9770603878619849}\n",
      "\n",
      "Saved model, best checkpoint (if any), and metrics to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_with_stress\\all_roberta_large_v1_with_stress\n",
      "\n",
      "Evaluating best checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_4668\\1117688418.py:383: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  best_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best checkpoint metrics:\n",
      "Val: {'accuracy': 0.8350071736011477, 'precision_macro': 0.773365483583706, 'precision_weighted': 0.8390996029388941, 'recall_macro': 0.7980127700492518, 'recall_weighted': 0.8350071736011477, 'f1_macro': 0.7832217674780155, 'f1_weighted': 0.8359738816038714, 'roc_auc_ovr': 0.974675144298052}\n",
      "Test: {'accuracy': 0.8794835007173601, 'precision_macro': 0.8362884187080905, 'precision_weighted': 0.8818867128337583, 'recall_macro': 0.842822970693169, 'recall_weighted': 0.8794835007173601, 'f1_macro': 0.8389433261884545, 'f1_weighted': 0.8803947198142223, 'roc_auc_ovr': 0.9770603878619849}\n",
      "\n",
      "===========================================\n",
      "Label-wise metrics â€” TEST (Best checkpoint)\n",
      "===========================================\n",
      "              precision  recall  f1-score   support  roc_auc_ovr\n",
      "anxiety          0.6809  0.7619    0.7191   42.0000       0.9529\n",
      "depression       0.9039  0.8922    0.8980  232.0000       0.9783\n",
      "ptsd             0.7436  0.7073    0.7250   41.0000       0.9781\n",
      "suicide          0.7753  0.8214    0.7977   84.0000       0.9652\n",
      "stress           0.9286  0.9163    0.9224  227.0000       0.9880\n",
      "none             0.9855  0.9577    0.9714   71.0000       0.9998\n",
      "accuracy         0.8795  0.8795    0.8795    0.8795          NaN\n",
      "macro avg        0.8363  0.8428    0.8389  697.0000          NaN\n",
      "weighted avg     0.8819  0.8795    0.8804  697.0000          NaN\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Fine tune sentence-transformers/all-roberta-large-v1\n",
    "# with overfitting checks, early stopping, and label-wise display\n",
    "# Dataset: Data_Warehouse/mental_health_splits_with_stress\n",
    "# Label ids preserved from splits: anxiety 0, depression 1, ptsd 2, suicide 3, stress 4, none 5\n",
    "# ==========================================\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------- progress visibility & Windows safety ----------\n",
    "os.environ.pop(\"HF_DISABLE_PROGRESS_BARS\", None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "class ConsoleLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs:\n",
    "            return\n",
    "        parts = []\n",
    "        if state.global_step is not None: parts.append(f\"step {state.global_step}\")\n",
    "        if state.epoch is not None: parts.append(f\"epoch {state.epoch:.2f}\")\n",
    "        if \"loss\" in logs: parts.append(f\"loss {logs['loss']:.4f}\")\n",
    "        if \"learning_rate\" in logs: parts.append(f\"lr {logs['learning_rate']:.6f}\")\n",
    "        print(\" | \".join(parts), flush=True)\n",
    "\n",
    "# ---------- config ----------\n",
    "RANDOM_STATE = 42\n",
    "TOKENIZER_NAME = \"sentence-transformers/all-roberta-large-v1\"  # ST tokenizer\n",
    "BACKBONE_NAME  = \"roberta-large\"                               # classification backbone\n",
    "MAX_LENGTH = 512\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "WD = 0.01\n",
    "TRAIN_BS = 4             # roberta-large is heavy; keep modest\n",
    "EVAL_BS = 16\n",
    "GRAD_ACCUM_STEPS = 8     # effective batch = 4 * 8 = 32\n",
    "USE_CLASS_WEIGHTS = True\n",
    "LABEL_SMOOTHING = 0.0    # set ~0.05 if you observe overfitting\n",
    "\n",
    "EARLY_STOP_PATIENCE = 2  # epochs without improvement before stopping\n",
    "EARLY_STOP_MONITOR = \"f1_macro\"\n",
    "\n",
    "# ---------- locate Data_Warehouse ----------\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate a folder named Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    script_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    script_dir = Path.cwd()\n",
    "data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "# ---------- load splits (WITH STRESS) ----------\n",
    "split_dir = data_warehouse / \"mental_health_splits_with_stress\"\n",
    "df_train = pd.read_csv(split_dir / \"train.csv\")\n",
    "df_val   = pd.read_csv(split_dir / \"val.csv\")\n",
    "df_test  = pd.read_csv(split_dir / \"test.csv\")\n",
    "\n",
    "# prefer label_norm if present\n",
    "label_col = \"label_norm\" if \"label_norm\" in df_train.columns else \"label\"\n",
    "for name, df_ in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    req = {\"text\", label_col, \"label_enc\"}\n",
    "    if not req.issubset(df_.columns):\n",
    "        raise ValueError(f\"{name} split missing required columns: {req}\")\n",
    "\n",
    "num_labels = int(df_train[\"label_enc\"].max()) + 1\n",
    "enc_to_label = df_train[[\"label_enc\", label_col]].drop_duplicates().sort_values(\"label_enc\")\n",
    "id2label = {int(r.label_enc): str(r[label_col]) for _, r in enc_to_label.iterrows()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "label_names = [id2label[i] for i in range(num_labels)]\n",
    "\n",
    "print(\"Label mapping used:\")\n",
    "for k in range(num_labels):\n",
    "    print(f\"{k}: {id2label[k]}\")\n",
    "\n",
    "# ---------- dataset (dynamic padding) ----------\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df[\"text\"].astype(str).tolist()\n",
    "        self.labels = df[\"label_enc\"].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=False,      # pad per-batch via collator\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# ---------- tokenizer & model ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BACKBONE_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Try to enable gradient checkpointing to save memory\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not enable gradient checkpointing: {e}\")\n",
    "\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# ---------- class weights ----------\n",
    "class_weights = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    counts = df_train[\"label_enc\"].value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    weights = total / (num_labels * counts)  # inverse frequency\n",
    "    class_weights = torch.tensor(weights.to_numpy(), dtype=torch.float, device=device)\n",
    "    print(\"Using class weights:\", [float(x) for x in class_weights])\n",
    "\n",
    "# ---------- custom Trainer (weighted CE + optional label smoothing) ----------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(\n",
    "            logits, labels,\n",
    "            weight=class_weights,\n",
    "            label_smoothing=LABEL_SMOOTHING if LABEL_SMOOTHING > 0 else 0.0\n",
    "        ) if class_weights is not None else F.cross_entropy(\n",
    "            logits, labels, label_smoothing=LABEL_SMOOTHING if LABEL_SMOOTHING > 0 else 0.0\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ---------- datasets & collator ----------\n",
    "train_ds = TextClsDataset(df_train, tokenizer)\n",
    "val_ds   = TextClsDataset(df_val, tokenizer)\n",
    "test_ds  = TextClsDataset(df_test, tokenizer)\n",
    "\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "# ---------- metrics (overall) ----------\n",
    "def compute_metrics_from_logits(logits, labels):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    out = {\n",
    "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
    "        \"precision_macro\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"precision_weighted\": float(precision_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"recall_macro\": float(recall_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall_weighted\": float(recall_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"f1_macro\": float(f1_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"f1_weighted\": float(f1_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "    try:\n",
    "        out[\"roc_auc_ovr\"] = float(roc_auc_score(labels, probs, multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        out[\"roc_auc_ovr\"] = None\n",
    "    return out, preds, probs\n",
    "\n",
    "# ---------- label-wise metrics (DISPLAY ONLY) ----------\n",
    "def per_label_report(logits, labels, label_names):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "\n",
    "    rep = classification_report(\n",
    "        labels, preds, target_names=label_names,\n",
    "        output_dict=True, zero_division=0\n",
    "    )\n",
    "    df = pd.DataFrame(rep).T\n",
    "\n",
    "    # add one-vs-rest ROC-AUC per class\n",
    "    aucs = {}\n",
    "    y_true = np.array(labels)\n",
    "    for i, name in enumerate(label_names):\n",
    "        try:\n",
    "            y_bin = (y_true == i).astype(int)\n",
    "            aucs[name] = roc_auc_score(y_bin, probs[:, i])\n",
    "        except Exception:\n",
    "            aucs[name] = np.nan\n",
    "\n",
    "    for name, auc in aucs.items():\n",
    "        if name in df.index:\n",
    "            df.loc[name, \"roc_auc_ovr\"] = float(auc) if auc == auc else None\n",
    "\n",
    "    cols = [\"precision\", \"recall\", \"f1-score\", \"support\", \"roc_auc_ovr\"]\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "def show_labelwise(df, title):\n",
    "    print(\"\\n\" + \"=\"*len(title))\n",
    "    print(title)\n",
    "    print(\"=\"*len(title))\n",
    "    print(df.round(4).to_string())\n",
    "\n",
    "# ---------- output dir ----------\n",
    "out_dir = split_dir / \"all_roberta_large_v1_with_stress\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- TrainingArguments (Windows-friendly) ----------\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WD,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    dataloader_num_workers=0,   # Windows: avoid hangs\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    report_to=[],\n",
    "    seed=RANDOM_STATE,\n",
    "    # optim=\"adamw_torch_fused\",  # optional on PyTorch 2.x + recent GPUs\n",
    ")\n",
    "\n",
    "TrainerClass = WeightedTrainer if USE_CLASS_WEIGHTS else Trainer\n",
    "trainer = TrainerClass(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,   # we run our own eval per epoch too\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.add_callback(ConsoleLogger())\n",
    "\n",
    "# ---------- Overfitting monitor: eval each epoch + early stopping + best checkpoint ----------\n",
    "class EvalEveryEpochCallback(TrainerCallback):\n",
    "    def __init__(self, trainer_ref, val_ds, out_dir, patience=2, monitor=\"f1_macro\"):\n",
    "        self.trainer_ref = trainer_ref\n",
    "        self.val_ds = val_ds\n",
    "        self.out_dir = Path(out_dir)\n",
    "        self.monitor = monitor\n",
    "        self.best = -float(\"inf\")\n",
    "        self.bad_epochs = 0\n",
    "        self.patience = patience\n",
    "        self.log_path = self.out_dir / \"epoch_metrics.jsonl\"\n",
    "        self._fh = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self._fh = open(self.log_path, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        out = self.trainer_ref.predict(self.val_ds)\n",
    "        metrics, _, _ = compute_metrics_from_logits(out.predictions, out.label_ids)\n",
    "        metrics[\"epoch\"] = float(state.epoch)\n",
    "        self._fh.write(json.dumps(metrics) + \"\\n\"); self._fh.flush()\n",
    "        print(f\"[epoch {metrics['epoch']:.2f}] val f1_macro={metrics['f1_macro']:.4f} \"\n",
    "              f\"acc={metrics['accuracy']:.4f}\", flush=True)\n",
    "        score = metrics.get(self.monitor, -float(\"inf\"))\n",
    "        if score > self.best + 1e-8:\n",
    "            self.best = score\n",
    "            self.bad_epochs = 0\n",
    "            self.trainer_ref.save_model(self.out_dir / \"best\")\n",
    "        else:\n",
    "            self.bad_epochs += 1\n",
    "            if self.bad_epochs >= self.patience:\n",
    "                print(f\"Early stopping: no improvement in {self.patience} epoch(s).\", flush=True)\n",
    "                control.should_training_stop = True\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self._fh:\n",
    "            self._fh.close()\n",
    "\n",
    "trainer.add_callback(EvalEveryEpochCallback(trainer, val_ds, out_dir, patience=EARLY_STOP_PATIENCE, monitor=EARLY_STOP_MONITOR))\n",
    "\n",
    "# ---------- train ----------\n",
    "print(\"\\nStarting trainer.train() ...\", flush=True)\n",
    "train_result = trainer.train()\n",
    "print(\"Finished trainer.train()\", flush=True)\n",
    "\n",
    "# save \"last\" model + tokenizer\n",
    "trainer.save_model(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "with open(out_dir / \"train_metrics.json\", \"w\") as f:\n",
    "    json.dump({k: (float(v) if isinstance(v, (int, float)) else str(v)) for k, v in train_result.metrics.items()}, f, indent=2)\n",
    "\n",
    "# ---------- manual evaluation (last model) ----------\n",
    "val_out = trainer.predict(val_ds)\n",
    "val_metrics, val_preds, val_probs = compute_metrics_from_logits(val_out.predictions, val_out.label_ids)\n",
    "\n",
    "test_out = trainer.predict(test_ds)\n",
    "test_metrics, test_preds, test_probs = compute_metrics_from_logits(test_out.predictions, test_out.label_ids)\n",
    "\n",
    "# also evaluate TRAIN to check generalization gap\n",
    "train_out = trainer.predict(train_ds)\n",
    "train_metrics, train_preds, train_probs = compute_metrics_from_logits(train_out.predictions, train_out.label_ids)\n",
    "\n",
    "# save overall metrics\n",
    "with open(out_dir / \"train_eval_metrics.json\", \"w\") as f:\n",
    "    json.dump(train_metrics, f, indent=2)\n",
    "with open(out_dir / \"val_metrics.json\", \"w\") as f:\n",
    "    json.dump(val_metrics, f, indent=2)\n",
    "with open(out_dir / \"test_metrics.json\", \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "# ---------- DISPLAY label-wise metrics for each split ----------\n",
    "train_labelwise = per_label_report(train_out.predictions, train_out.label_ids, label_names)\n",
    "val_labelwise   = per_label_report(val_out.predictions,   val_out.label_ids,   label_names)\n",
    "test_labelwise  = per_label_report(test_out.predictions,  test_out.label_ids,  label_names)\n",
    "\n",
    "show_labelwise(train_labelwise, \"Label-wise metrics â€” TRAIN\")\n",
    "show_labelwise(val_labelwise,   \"Label-wise metrics â€” VAL\")\n",
    "show_labelwise(test_labelwise,  \"Label-wise metrics â€” TEST\")\n",
    "\n",
    "# ---------- classification report & confusion matrix on test ----------\n",
    "rep = classification_report(\n",
    "    test_out.label_ids, test_preds,\n",
    "    target_names=label_names,\n",
    "    digits=4, zero_division=0\n",
    ")\n",
    "cm = confusion_matrix(test_out.label_ids, test_preds)\n",
    "\n",
    "with open(out_dir / \"test_classification_report.txt\", \"w\") as f:\n",
    "    f.write(rep)\n",
    "pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"true_{n}\" for n in label_names],\n",
    "    columns=[f\"pred_{n}\" for n in label_names],\n",
    ").to_csv(out_dir / \"test_confusion_matrix.csv\", index=True)\n",
    "\n",
    "# ---------- simple gap printout ----------\n",
    "print(\"\\nGeneralization gaps (train - val):\",\n",
    "      {\"acc\": train_metrics[\"accuracy\"] - val_metrics[\"accuracy\"],\n",
    "       \"f1_macro\": train_metrics[\"f1_macro\"] - val_metrics[\"f1_macro\"],\n",
    "       \"f1_weighted\": train_metrics[\"f1_weighted\"] - val_metrics[\"f1_weighted\"]})\n",
    "\n",
    "print(\"\\nTrain metrics:\", train_metrics)\n",
    "print(\"Val metrics:\", val_metrics)\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "\n",
    "print(\"\\nSaved model, best checkpoint (if any), and metrics to:\", out_dir.resolve())\n",
    "\n",
    "# ---------- (Optional) Evaluate the saved 'best' checkpoint and DISPLAY label-wise ----------\n",
    "best_dir = out_dir / \"best\"\n",
    "if best_dir.exists():\n",
    "    print(\"\\nEvaluating best checkpoint ...\")\n",
    "    best_model = AutoModelForSequenceClassification.from_pretrained(best_dir).to(device)\n",
    "    best_trainer = Trainer(\n",
    "        model=best_model,\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    b_val = best_trainer.predict(val_ds)\n",
    "    b_val_metrics, _, _ = compute_metrics_from_logits(b_val.predictions, b_val.label_ids)\n",
    "    b_test = best_trainer.predict(test_ds)\n",
    "    b_test_metrics, _, _ = compute_metrics_from_logits(b_test.predictions, b_test.label_ids)\n",
    "\n",
    "    print(\"\\nBest checkpoint metrics:\")\n",
    "    print(\"Val:\", b_val_metrics)\n",
    "    print(\"Test:\", b_test_metrics)\n",
    "\n",
    "    b_test_labelwise = per_label_report(b_test.predictions, b_test.label_ids, label_names)\n",
    "    show_labelwise(b_test_labelwise, \"Label-wise metrics â€” TEST (Best checkpoint)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_with_stress\\mental_roberta_base_with_stress\n",
      "Label map: {0: 'anxiety', 1: 'depression', 2: 'ptsd', 3: 'suicide', 4: 'stress', 5: 'none'}\n",
      "Torch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at mental/mental-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [2.799196720123291, 0.5001794099807739, 2.799196720123291, 1.3870646953582764, 0.5106227397918701, 1.647754192352295]\n",
      "\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_18264\\4079697078.py:124: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_18264\\4079697078.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train_loss 0.9748 val_loss 0.5404 val_f1_macro 0.7546 time 105.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_18264\\4079697078.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train_loss 0.4693 val_loss 0.5203 val_f1_macro 0.7660 time 106.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_18264\\4079697078.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train_loss 0.2743 val_loss 0.5080 val_f1_macro 0.7903 time 107.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_18264\\4079697078.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train_loss 0.1609 val_loss 0.6293 val_f1_macro 0.7912 time 107.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipua\\AppData\\Local\\Temp\\ipykernel_18264\\4079697078.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train_loss 0.0989 val_loss 0.6699 val_f1_macro 0.7986 time 107.7s\n",
      "\n",
      "Saved LAST model to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_with_stress\\mental_roberta_base_with_stress\n",
      "Saved BEST checkpoint to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_with_stress\\mental_roberta_base_with_stress\\best\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Fine tune mental/mental-roberta-base  (SAVES MODEL + BEST CHECKPOINT)\n",
    "# Trainer-free, AMP-enabled\n",
    "# Assumes: data_warehouse is already defined (from Step 1/2)\n",
    "# Expects: df_train, df_val, df_test with columns: text, label_enc\n",
    "# Saves to: Data_Warehouse/mental_health_splits_with_stress/mental_roberta_base_with_stress(/best)\n",
    "# ==========================================\n",
    "\n",
    "import os, time, json, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# ---------------------- config ----------------------\n",
    "RANDOM_STATE = 42\n",
    "MODEL_NAME = \"mental/mental-roberta-base\"\n",
    "HF_TOKEN = os.getenv('HF_TOKEN') # set if gated\n",
    "MAX_LENGTH = 512\n",
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "WD = 0.01\n",
    "TRAIN_BS = 8\n",
    "EVAL_BS = 32\n",
    "EARLY_STOP_PATIENCE = 2\n",
    "LABEL_SMOOTHING = 0.0\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "\n",
    "# ---------------- save dirs (uses data_warehouse from Step 1/2) ----------------\n",
    "splits_dir = data_warehouse / \"mental_health_splits_with_stress\"\n",
    "out_dir = splits_dir / \"mental_roberta_base_with_stress\"\n",
    "best_dir = out_dir / \"best\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Saving to:\", out_dir.resolve())\n",
    "\n",
    "# ---------------- label maps from your splits ----------------\n",
    "num_labels = int(df_train[\"label_enc\"].max()) + 1\n",
    "if \"label\" in df_train.columns:\n",
    "    enc_to_label = df_train[[\"label_enc\", \"label\"]].drop_duplicates().sort_values(\"label_enc\")\n",
    "    id2label = {int(r.label_enc): str(r.label) for _, r in enc_to_label.iterrows()}\n",
    "else:\n",
    "    id2label = {i: f\"label_{i}\" for i in range(num_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "label_names = [id2label[i] for i in range(num_labels)]\n",
    "print(\"Label map:\", id2label)\n",
    "\n",
    "# ---------------- dataset ----------------\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df[\"text\"].astype(str).tolist()\n",
    "        self.labels = df[\"label_enc\"].astype(int).tolist()\n",
    "        self.tok = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(\n",
    "            self.texts[i],\n",
    "            truncation=True,\n",
    "            padding=False,     # per-batch padding via collate\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[i], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=HF_TOKEN)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    token=HF_TOKEN\n",
    ").to(device)\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False\n",
    "\n",
    "# ---------------- DataLoader with dynamic padding ----------------\n",
    "def collate(batch):\n",
    "    keys = batch[0].keys()\n",
    "    out = {}\n",
    "    for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if k in keys and all(k in x for x in batch):\n",
    "            pad_id = tokenizer.pad_token_id if k == \"input_ids\" else 0\n",
    "            out[k] = torch.nn.utils.rnn.pad_sequence(\n",
    "                [x[k] for x in batch], batch_first=True, padding_value=pad_id\n",
    "            )\n",
    "    out[\"labels\"] = torch.tensor([x[\"labels\"].item() for x in batch], dtype=torch.long)\n",
    "    return out\n",
    "\n",
    "train_ds = TextClsDataset(df_train, tokenizer)\n",
    "val_ds   = TextClsDataset(df_val, tokenizer)\n",
    "test_ds  = TextClsDataset(df_test, tokenizer)\n",
    "\n",
    "g = torch.Generator(); g.manual_seed(RANDOM_STATE)\n",
    "train_loader = DataLoader(train_ds, batch_size=TRAIN_BS, shuffle=True,  collate_fn=collate, generator=g)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=EVAL_BS, shuffle=False, collate_fn=collate)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=EVAL_BS, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# ---------------- class weights ----------------\n",
    "counts = df_train[\"label_enc\"].value_counts().sort_index()\n",
    "weights = (counts.sum() / (num_labels * counts)).astype(\"float32\").to_numpy()\n",
    "class_weights = torch.tensor(weights, device=device)\n",
    "print(\"Class weights:\", [float(x) for x in class_weights])\n",
    "\n",
    "# ---------------- optim + sched + amp ----------------\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "num_steps = max(1, len(train_loader)) * EPOCHS // max(1, GRAD_ACCUM_STEPS)\n",
    "sched = get_linear_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_warmup_steps=int(0.06 * num_steps),\n",
    "    num_training_steps=num_steps\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "@torch.no_grad()\n",
    "def forward_eval(loader):\n",
    "    model.eval()\n",
    "    all_logits, all_labels, loss_sum, n_steps = [], [], 0.0, 0\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "        logits = out.logits\n",
    "        loss = F.cross_entropy(logits, batch[\"labels\"], weight=class_weights, label_smoothing=LABEL_SMOOTHING)\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "        all_labels.append(batch[\"labels\"].detach().cpu())\n",
    "        loss_sum += loss.item(); n_steps += 1\n",
    "    logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    return loss_sum / max(1, n_steps), logits, labels\n",
    "\n",
    "def compute_metrics_from_logits(logits, labels, names):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    out = {\n",
    "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
    "        \"precision_macro\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"precision_weighted\": float(precision_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"recall_macro\": float(recall_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall_weighted\": float(recall_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "        \"f1_macro\": float(f1_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"f1_weighted\": float(f1_score(labels, preds, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "    try:\n",
    "        out[\"roc_auc_ovr\"] = float(roc_auc_score(labels, probs, multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        out[\"roc_auc_ovr\"] = None\n",
    "\n",
    "    rep = classification_report(labels, preds, target_names=names, digits=4, zero_division=0, output_dict=True)\n",
    "    rep_df = pd.DataFrame(rep).T.rename(columns={\"f1-score\": \"f1_score\"})\n",
    "\n",
    "    # per class auc\n",
    "    try:\n",
    "        for i, n in enumerate(names):\n",
    "            y_bin = (labels == i).astype(int)\n",
    "            rep_df.loc[n, \"roc_auc_ovr\"] = float(roc_auc_score(y_bin, probs[:, i]))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return out, rep_df\n",
    "\n",
    "def save_metrics_block(prefix: str, metrics: dict, rep_df: pd.DataFrame):\n",
    "    (out_dir / f\"{prefix}_metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
    "    rep_df.to_csv(out_dir / f\"{prefix}_classification_report.csv\", index=True)\n",
    "\n",
    "def save_confusion_matrix(prefix: str, labels, logits):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    cm_df = pd.DataFrame(cm, index=[f\"true_{n}\" for n in label_names], columns=[f\"pred_{n}\" for n in label_names])\n",
    "    cm_df.to_csv(out_dir / f\"{prefix}_confusion_matrix.csv\", index=True)\n",
    "\n",
    "# ---------------- training loop with best checkpoint ----------------\n",
    "best_f1 = -1.0\n",
    "bad_epochs = 0\n",
    "\n",
    "print(\"\\nStarting training\")\n",
    "model.train()\n",
    "optim.zero_grad(set_to_none=True)\n",
    "step_in_accum = 0\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    running = 0.0; nsteps = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
    "            out = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            loss = F.cross_entropy(out.logits, batch[\"labels\"], weight=class_weights, label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "        step_in_accum += 1\n",
    "        scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
    "        if step_in_accum % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.step(optim); scaler.update()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            sched.step()\n",
    "\n",
    "        running += loss.item(); nsteps += 1\n",
    "\n",
    "    # end-epoch eval\n",
    "    val_loss, val_logits, val_labels = forward_eval(val_loader)\n",
    "    val_metrics, _ = compute_metrics_from_logits(val_logits, val_labels, label_names)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"epoch {ep} train_loss {running/max(1,nsteps):.4f} \"\n",
    "          f\"val_loss {val_loss:.4f} val_f1_macro {val_metrics['f1_macro']:.4f} time {dt:.1f}s\")\n",
    "\n",
    "    # save best immediately\n",
    "    if val_metrics[\"f1_macro\"] > best_f1 + 1e-8:\n",
    "        best_f1 = val_metrics[\"f1_macro\"]\n",
    "        model.save_pretrained(best_dir)\n",
    "        tokenizer.save_pretrained(best_dir)\n",
    "        cfg = AutoModelForSequenceClassification.from_pretrained(best_dir).config\n",
    "        cfg.id2label = id2label; cfg.label2id = label2id\n",
    "        cfg.save_pretrained(best_dir)\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        if bad_epochs >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"Early stopping after {ep} epoch(s) without improvement\")\n",
    "            break\n",
    "\n",
    "# ---------------- final eval + SAVE last ----------------\n",
    "train_loss, train_logits, train_labels = forward_eval(train_loader)\n",
    "val_loss,   val_logits,   val_labels   = forward_eval(val_loader)\n",
    "test_loss,  test_logits,  test_labels  = forward_eval(test_loader)\n",
    "\n",
    "train_metrics, train_rep = compute_metrics_from_logits(train_logits, train_labels, label_names)\n",
    "val_metrics,   val_rep   = compute_metrics_from_logits(val_logits,   val_labels,   label_names)\n",
    "test_metrics,  test_rep  = compute_metrics_from_logits(test_logits,  test_labels,  label_names)\n",
    "\n",
    "save_metrics_block(\"train\", train_metrics, train_rep)\n",
    "save_metrics_block(\"val\",   val_metrics,   val_rep)\n",
    "save_metrics_block(\"test\",  test_metrics,  test_rep)\n",
    "save_confusion_matrix(\"test\", test_labels, test_logits)\n",
    "\n",
    "model.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "cfg = AutoModelForSequenceClassification.from_pretrained(out_dir).config\n",
    "cfg.id2label = id2label; cfg.label2id = label2id\n",
    "cfg.save_pretrained(out_dir)\n",
    "\n",
    "summary = {\n",
    "    \"epochs_trained\": ep,\n",
    "    \"best_val_f1_macro\": best_f1,\n",
    "    \"out_dir\": str(out_dir.resolve()),\n",
    "    \"best_dir\": str(best_dir.resolve()),\n",
    "}\n",
    "(out_dir / \"summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"\\nSaved LAST model to:\", out_dir.resolve())\n",
    "print(\"Saved BEST checkpoint to:\", best_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd187111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "Label-wise metrics â€” TRAIN\n",
      "==========================\n",
      "            precision  recall  f1_score  support  roc_auc_ovr\n",
      "anxiety        0.9566  0.9970    0.9764    332.0       0.9995\n",
      "depression     0.9955  0.9510    0.9727   1858.0       0.9977\n",
      "ptsd           0.9375  0.9940    0.9649    332.0       0.9992\n",
      "suicide        0.8850  0.9881    0.9337    670.0       0.9971\n",
      "stress         0.9983  0.9824    0.9903   1820.0       0.9995\n",
      "none           0.9982  0.9982    0.9982    564.0       1.0000\n",
      "\n",
      "========================\n",
      "Label-wise metrics â€” VAL\n",
      "========================\n",
      "            precision  recall  f1_score  support  roc_auc_ovr\n",
      "anxiety        0.6364  0.8333    0.7216     42.0       0.9737\n",
      "depression     0.8491  0.8491    0.8491    232.0       0.9693\n",
      "ptsd           0.6923  0.6585    0.6750     41.0       0.9636\n",
      "suicide        0.6667  0.6429    0.6545     84.0       0.9530\n",
      "stress         0.9312  0.8943    0.9124    227.0       0.9871\n",
      "none           0.9722  0.9859    0.9790     71.0       0.9997\n",
      "\n",
      "=========================\n",
      "Label-wise metrics â€” TEST\n",
      "=========================\n",
      "            precision  recall  f1_score  support  roc_auc_ovr\n",
      "anxiety        0.6400  0.7619    0.6957     42.0       0.9692\n",
      "depression     0.8904  0.8405    0.8647    232.0       0.9750\n",
      "ptsd           0.8571  0.7317    0.7895     41.0       0.9828\n",
      "suicide        0.6957  0.7619    0.7273     84.0       0.9629\n",
      "stress         0.9009  0.9207    0.9107    227.0       0.9852\n",
      "none           0.9855  0.9577    0.9714     71.0       0.9988\n"
     ]
    }
   ],
   "source": [
    "# ---- DISPLAY label-wise metrics (precision/recall/F1/support + per-class AUC) ----\n",
    "cols = [\"precision\", \"recall\", \"f1_score\", \"support\", \"roc_auc_ovr\"]\n",
    "\n",
    "def show_labelwise(df, title):\n",
    "    # keep only label rows (drop 'accuracy', 'macro avg', 'weighted avg' if present)\n",
    "    df_show = df.loc[[n for n in df.index if n in label_names], cols].copy()\n",
    "    print(\"\\n\" + \"=\"*len(title))\n",
    "    print(title)\n",
    "    print(\"=\"*len(title))\n",
    "    print(df_show.round(4).to_string())\n",
    "\n",
    "show_labelwise(train_rep, \"Label-wise metrics â€” TRAIN\")\n",
    "show_labelwise(val_rep,   \"Label-wise metrics â€” VAL\")\n",
    "show_labelwise(test_rep,  \"Label-wise metrics â€” TEST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c028e410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix â€” TEST (rows=true, cols=pred):\n",
      "                 pred_anxiety  pred_depression  pred_ptsd  pred_suicide  pred_stress  pred_none\n",
      "true_anxiety               32                4          1             0            5          0\n",
      "true_depression             6              195          0            27            4          0\n",
      "true_ptsd                   1                1         30             0            9          0\n",
      "true_suicide                2               14          0            64            4          0\n",
      "true_stress                 9                4          4             0          209          1\n",
      "true_none                   0                1          0             1            1         68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"\\nConfusion matrix â€” TEST (rows=true, cols=pred):\")\n",
    "print(pd.DataFrame(\n",
    "    confusion_matrix(test_labels, np.argmax(test_logits, axis=1)),\n",
    "    index=[f\"true_{n}\" for n in label_names],\n",
    "    columns=[f\"pred_{n}\" for n in label_names],\n",
    ").to_string())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_gpu_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
