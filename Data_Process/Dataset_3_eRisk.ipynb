{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "724ca06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbe3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== directories =====\n",
    "project_dir = Path.cwd()\n",
    "source_dir = project_dir / \"Data_Lake\" / \"Dataset_3\"\n",
    "all_combined_dir = source_dir / \"all_combined\"\n",
    "labels_path = source_dir / \"shuffled_ground_truth_labels.txt\"\n",
    "warehouse_dir = project_dir / \"Data_Warehouse\"\n",
    "warehouse_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_csv = warehouse_dir / \"erisk_task2_posts_and_comments.csv\"  # new name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211b0ea",
   "metadata": {},
   "source": [
    "This script processes the eRisk Task 2 dataset to build a user-level depression vs. non-depression dataset. It:\n",
    "\n",
    "Reads ground-truth labels from shuffled_ground_truth_labels.txt.\n",
    "\n",
    "Iterates through all .json files in all_combined/, collecting posts and comments only from the target user (excluding deleted_user).\n",
    "\n",
    "Cleans each text (removes extra whitespace, excludes texts with URLs).\n",
    "\n",
    "Concatenates all valid writings per user into a single document.\n",
    "\n",
    "Applies a length filter: keeps only texts with 50–400 words (trimming longer ones to 400).\n",
    "\n",
    "Assigns labels (depression or non depression) using the ground truth.\n",
    "\n",
    "Saves the result as a JSONL file (erisk_task2_userlevel.json), where each line contains user_id, text, label_id, and label.\n",
    "\n",
    "The output can then be loaded as a DataFrame or used directly for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb27d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users written: 902\n",
      "Saved user-level JSONL to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\erisk_task2_userlevel.json\n",
      "\n",
      "Label counts:\n",
      "label\n",
      "non depression    800\n",
      "depression        102\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, re\n",
    "import pandas as pd\n",
    "\n",
    "# ===== directories (your layout) =====\n",
    "project_dir = Path.cwd()\n",
    "source_dir = project_dir / \"Data_Lake\" / \"Dataset_3\"\n",
    "all_combined_dir = source_dir / \"all_combined\"\n",
    "labels_path = source_dir / \"shuffled_ground_truth_labels.txt\"\n",
    "warehouse_dir = project_dir / \"Data_Warehouse\"\n",
    "warehouse_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# output: one line per user (JSONL)\n",
    "out_jsonl = warehouse_dir / \"erisk_task2_userlevel.json\"\n",
    "\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "WS_RE  = re.compile(r\"\\s+\")\n",
    "\n",
    "def contains_url(text: str) -> bool:\n",
    "    return bool(URL_RE.search(text or \"\"))\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    return WS_RE.sub(\" \", s.strip())\n",
    "\n",
    "def word_count(text: str) -> int:\n",
    "    # simple token-ish word counter\n",
    "    return len(re.findall(r\"\\b\\w+\\b\", text or \"\"))\n",
    "\n",
    "def load_labels(path: Path) -> dict:\n",
    "    labels = {}\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                labels[parts[0]] = int(parts[1])\n",
    "    return labels\n",
    "\n",
    "labels = load_labels(labels_path)\n",
    "\n",
    "records = []\n",
    "files = sorted(all_combined_dir.glob(\"*.json\"))\n",
    "for jf in files:\n",
    "    try:\n",
    "        blocks = json.loads(jf.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[skip read] {jf.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # collect all target==True writings for this file’s subject\n",
    "    target_user_ids = set()\n",
    "    pieces = []\n",
    "\n",
    "    for blk in blocks:\n",
    "        sub = blk.get(\"submission\") or {}\n",
    "        # target user's submission?\n",
    "        if sub.get(\"target\", False):\n",
    "            uid = sub.get(\"user_id\")\n",
    "            if uid and uid != \"deleted_user\" and uid in labels:\n",
    "                target_user_ids.add(uid)\n",
    "                # combine title+body, exclude if URL present\n",
    "                title = clean_text(sub.get(\"title\") or \"\")\n",
    "                body  = clean_text(sub.get(\"body\") or \"\")\n",
    "                text  = \" \".join([t for t in (title, body) if t])\n",
    "                if text and not contains_url(text):\n",
    "                    pieces.append(text)\n",
    "\n",
    "        # target user's comments in this thread\n",
    "        for c in blk.get(\"comments\") or []:\n",
    "            if not c.get(\"target\", False):\n",
    "                continue\n",
    "            uid = c.get(\"user_id\")\n",
    "            if not uid or uid == \"deleted_user\" or uid not in labels:\n",
    "                continue\n",
    "            target_user_ids.add(uid)\n",
    "            text = clean_text(c.get(\"body\") or \"\")\n",
    "            if text and not contains_url(text):\n",
    "                pieces.append(text)\n",
    "\n",
    "    # sanity: expect exactly one target user per file\n",
    "    if len(target_user_ids) == 0:\n",
    "        # no target content found -> skip file\n",
    "        continue\n",
    "    if len(target_user_ids) > 1:\n",
    "        print(f\"[warn] {jf.name}: multiple target users {sorted(target_user_ids)}; using first\")\n",
    "    uid = sorted(target_user_ids)[0]\n",
    "\n",
    "    # concatenate all cleaned pieces (excluding any that contained URLs)\n",
    "    if not pieces:\n",
    "        continue\n",
    "    concat_text = clean_text(\"\\n\\n\".join(pieces))\n",
    "\n",
    "    # length gate: 50–400 words (trim to 400 if longer)\n",
    "    wc = word_count(concat_text)\n",
    "    if wc < 50:\n",
    "        # too little content after URL filtering\n",
    "        continue\n",
    "    if wc > 400:\n",
    "        # trim to first 400 words\n",
    "        words = re.findall(r\"\\b\\w+\\b\", concat_text)\n",
    "        # rebuild using a simple slice; keep original spacing roughly\n",
    "        trimmed = \" \".join(words[:400])\n",
    "        concat_text = trimmed\n",
    "\n",
    "    y = labels[uid]\n",
    "    records.append({\n",
    "        \"user_id\": uid,\n",
    "        \"text\": concat_text,\n",
    "        \"label_id\": int(y),\n",
    "        \"label\": \"depression\" if y == 1 else \"non depression\",\n",
    "    })\n",
    "\n",
    "# save JSONL\n",
    "with out_jsonl.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in records:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Users written: {len(records)}\")\n",
    "print(f\"Saved user-level JSONL to: {out_jsonl.resolve()}\")\n",
    "\n",
    "# (Optional) quick peek as a dataframe\n",
    "if records:\n",
    "    df = pd.DataFrame(records)\n",
    "    print(\"\\nLabel counts:\")\n",
    "    print(df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89642cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset counts:\n",
      "label\n",
      "non depression    800\n",
      "depression        102\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampled dataset counts:\n",
      "label\n",
      "depression        50\n",
      "non depression    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved 50-50 dataset to D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\erisk_task2_userlevel_50_50.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# ===== input/output =====\n",
    "warehouse_dir = Path.cwd() / \"Data_Warehouse\"\n",
    "src_jsonl = warehouse_dir / \"erisk_task2_userlevel.json\"\n",
    "out_csv = warehouse_dir / \"erisk_task2_userlevel_50_50.csv\"\n",
    "\n",
    "# ===== load JSONL into dataframe =====\n",
    "records = []\n",
    "with src_jsonl.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            records.append(json.loads(line))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(\"Full dataset counts:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "# ===== stratified sampling: 50 per class =====\n",
    "dfs = []\n",
    "for lbl in [\"depression\", \"non depression\"]:\n",
    "    subset = df[df[\"label\"] == lbl]\n",
    "    if len(subset) < 50:\n",
    "        raise ValueError(f\"Not enough samples for {lbl}: found {len(subset)}\")\n",
    "    dfs.append(subset.sample(n=50, random_state=42))\n",
    "\n",
    "df_small = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nSampled dataset counts:\")\n",
    "print(df_small[\"label\"].value_counts())\n",
    "\n",
    "# ===== save =====\n",
    "df_small.to_csv(out_csv, index=False)\n",
    "print(f\"\\nSaved 50-50 dataset to {out_csv.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_gpu_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
