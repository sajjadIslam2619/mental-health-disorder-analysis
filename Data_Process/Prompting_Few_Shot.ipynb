{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a8b24e",
   "metadata": {},
   "source": [
    "### With No Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ebec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rows: 470\n",
      "Train rows: 3756\n",
      "                                                text       label  label_enc\n",
      "0  I had a promising academic future. I had a won...     suicide          3\n",
      "1  It seems that goalkeepers tend to be older com...        none          4\n",
      "2  \"Life has no meaning the moment you lose the i...  depression          1\n",
      "3  I got way too attached and when she ghosted me...     suicide          3\n",
      "4  I have severe severe depression, high magnitud...  depression          1\n",
      "Saved few shot examples to: d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_no_stress\\few_shot_examples_no_stress_k3.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1 and 2: Load no stress splits and build few shot examples\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "FEW_SHOT_K = 3\n",
    "\n",
    "# ===== locate Data_Warehouse =====\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent  # if running as script\n",
    "    except NameError:\n",
    "        script_dir = Path.cwd()                       # if running in notebook\n",
    "    data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "split_dir = data_warehouse / \"mental_health_splits_no_stress\"\n",
    "\n",
    "# ===== load no stress test.csv and train.csv =====\n",
    "test_path = split_dir / \"test.csv\"\n",
    "train_path = split_dir / \"train.csv\"\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_train = pd.read_csv(train_path)\n",
    "\n",
    "print(\"Test rows:\", len(df_test))\n",
    "print(\"Train rows:\", len(df_train))\n",
    "print(df_test.head())\n",
    "\n",
    "# ===== light cleaning =====\n",
    "for df in (df_train, df_test):\n",
    "    if \"text\" not in df.columns or \"label\" not in df.columns:\n",
    "        raise ValueError(\"Expected columns text and label in the split files\")\n",
    "    df[\"text\"] = df[\"text\"].astype(str).str.replace(\"\\n\", \" \").str.strip()\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "\n",
    "# ===== ensure label overlap between train and test =====\n",
    "labels_test = sorted(df_test[\"label\"].unique().tolist())\n",
    "labels_train = sorted(df_train[\"label\"].unique().tolist())\n",
    "missing_in_train = sorted(set(labels_test) - set(labels_train))\n",
    "if missing_in_train:\n",
    "    print(\"Warning: the following test labels do not appear in train and cannot provide few shot examples:\", missing_in_train)\n",
    "\n",
    "# ===== build few shot dataframe: up to 3 examples per label =====\n",
    "fewshot_parts = []\n",
    "for lab in labels_test:\n",
    "    block = df_train[df_train[\"label\"] == lab]\n",
    "    take = min(FEW_SHOT_K, len(block))\n",
    "    if take == 0:\n",
    "        continue\n",
    "    fewshot_parts.append(block.sample(n=take, random_state=RANDOM_STATE))\n",
    "\n",
    "if not fewshot_parts:\n",
    "    raise RuntimeError(\"Could not collect any few shot examples. Check your train split.\")\n",
    "\n",
    "df_fewshot = pd.concat(fewshot_parts, ignore_index=True)\n",
    "\n",
    "# optional truncation to keep prompts compact\n",
    "MAX_TEXT_CHARS = 400\n",
    "df_fewshot[\"text_short\"] = df_fewshot[\"text\"].str.slice(0, MAX_TEXT_CHARS)\n",
    "\n",
    "# save few shot examples for reference\n",
    "fewshot_out = split_dir / \"few_shot_examples_no_stress_k3.csv\"\n",
    "df_fewshot[[\"label\", \"text\"]].to_csv(fewshot_out, index=False)\n",
    "print(\"Saved few shot examples to:\", fewshot_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bcdc7b",
   "metadata": {},
   "source": [
    "### With Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3b297e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rows: 697\n",
      "Train rows: 5576\n",
      "                                                text  label_norm  label_enc\n",
      "0  I use a lot of different ingredients when prep...        none          5\n",
      "1  When we are at work we joke around but we all ...      stress          4\n",
      "2  1 year ago, I left for good. I was a reddit lu...  depression          1\n",
      "3  someone once told me smoking weed and then tou...        none          5\n",
      "4  The main source of this stress is a scholarshi...      stress          4\n",
      "Saved few shot examples to: d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_with_stress\\few_shot_examples_no_stress_k3.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1 and 2: Load no stress splits and build few shot examples\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "FEW_SHOT_K = 3\n",
    "\n",
    "# ===== locate Data_Warehouse =====\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    data_warehouse\n",
    "except NameError:\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent  # if running as script\n",
    "    except NameError:\n",
    "        script_dir = Path.cwd()                       # if running in notebook\n",
    "    data_warehouse = find_data_warehouse(script_dir)\n",
    "\n",
    "split_dir = data_warehouse / \"mental_health_splits_with_stress\"\n",
    "\n",
    "# ===== load no stress test.csv and train.csv =====\n",
    "test_path = split_dir / \"test.csv\"\n",
    "train_path = split_dir / \"train.csv\"\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_train = pd.read_csv(train_path)\n",
    "\n",
    "print(\"Test rows:\", len(df_test))\n",
    "print(\"Train rows:\", len(df_train))\n",
    "print(df_test.head())\n",
    "\n",
    "# ===== light cleaning =====\n",
    "for df in (df_train, df_test):\n",
    "    if \"text\" not in df.columns or \"label_norm\" not in df.columns:\n",
    "        raise ValueError(\"Expected columns text and label in the split files\")\n",
    "    df[\"text\"] = df[\"text\"].astype(str).str.replace(\"\\n\", \" \").str.strip()\n",
    "    df[\"label_norm\"] = df[\"label_norm\"].astype(str).str.strip()\n",
    "\n",
    "# ===== ensure label overlap between train and test =====\n",
    "labels_test = sorted(df_test[\"label_norm\"].unique().tolist())\n",
    "labels_train = sorted(df_train[\"label_norm\"].unique().tolist())\n",
    "missing_in_train = sorted(set(labels_test) - set(labels_train))\n",
    "if missing_in_train:\n",
    "    print(\"Warning: the following test labels do not appear in train and cannot provide few shot examples:\", missing_in_train)\n",
    "\n",
    "# ===== build few shot dataframe: up to 3 examples per label =====\n",
    "fewshot_parts = []\n",
    "for lab in labels_test:\n",
    "    block = df_train[df_train[\"label_norm\"] == lab]\n",
    "    take = min(FEW_SHOT_K, len(block))\n",
    "    if take == 0:\n",
    "        continue\n",
    "    fewshot_parts.append(block.sample(n=take, random_state=RANDOM_STATE))\n",
    "\n",
    "if not fewshot_parts:\n",
    "    raise RuntimeError(\"Could not collect any few shot examples. Check your train split.\")\n",
    "\n",
    "df_fewshot = pd.concat(fewshot_parts, ignore_index=True)\n",
    "\n",
    "# optional truncation to keep prompts compact\n",
    "MAX_TEXT_CHARS = 400\n",
    "df_fewshot[\"text_short\"] = df_fewshot[\"text\"].str.slice(0, MAX_TEXT_CHARS)\n",
    "\n",
    "# save few shot examples for reference\n",
    "fewshot_out = split_dir / \"few_shot_examples_no_stress_k3.csv\"\n",
    "df_fewshot[[\"label_norm\", \"text\"]].to_csv(fewshot_out, index=False)\n",
    "print(\"Saved few shot examples to:\", fewshot_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "614d9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prompt builder for mental health classification\n",
    "# zero shot first, few shot supported if you later set is_few_shot=True\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def build_full_prompt(\n",
    "    labels: List[str],\n",
    "    test_data_for_prompt: str,\n",
    "    is_few_shot: bool = False,\n",
    "    train_data_for_prompt: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build a single prompt that can contain multiple test items.\n",
    "    test_data_for_prompt should be lines like: id | text\n",
    "    if is_few_shot=True, train_data_for_prompt should be lines like: id | label | text\n",
    "    \"\"\"\n",
    "\n",
    "    instruction = \"\"\"\n",
    "Instruction\n",
    "Your task is to assign exactly one mental health label to each user text. This is a single label classification task only.\n",
    "\"\"\"\n",
    "\n",
    "    context = f\"\"\"\n",
    "Context\n",
    "This will be used in a research demo that compares large language models on mental health text classification.\n",
    "Do not provide medical advice. Do not provide crisis instructions. Only return labels.\n",
    "Each example contains\n",
    "    label the ground truth class used for evaluation\n",
    "    text a short natural language post\n",
    "\n",
    "Predefined Labels\n",
    "{\", \".join(labels)}\n",
    "\"\"\"\n",
    "\n",
    "    handling_uncertainty = \"\"\"\n",
    "Handling Uncertainty\n",
    "If the text is unclear or could reasonably match multiple categories, select the closest label by meaning.\n",
    "If there is no signal of a mental health condition, use the label none.\n",
    "\"\"\"\n",
    "\n",
    "    training_data = \"\"\"\n",
    "Training Data\n",
    "You will see a few labeled examples inside triple quotes.\n",
    "Format\n",
    "id | label | text\n",
    "\"\"\"\n",
    "\n",
    "    test_data_intro = \"\"\"\n",
    "Test Data\n",
    "You will see unlabeled items inside triple quotes.\n",
    "For each item, return one predicted label from the predefined list.\n",
    "Format\n",
    "id | text\n",
    "\"\"\"\n",
    "\n",
    "    output_format = \"\"\"\n",
    "Output Format\n",
    "Return one line per item using this exact format\n",
    "id | predicted_label | text\n",
    "Do not add explanations. Do not add extra fields. Keep the original text unchanged.\n",
    "\"\"\"\n",
    "\n",
    "    if is_few_shot:\n",
    "        full_prompt = \"\\n\\n\".join([\n",
    "            instruction.strip(),\n",
    "            context.strip(),\n",
    "            handling_uncertainty.strip(),\n",
    "            training_data.strip(),\n",
    "            f'\"\"\"\\n{train_data_for_prompt.strip()}\\n\"\"\"',\n",
    "            test_data_intro.strip(),\n",
    "            f'\"\"\"\\n{test_data_for_prompt.strip()}\\n\"\"\"',\n",
    "            output_format.strip()\n",
    "        ])\n",
    "    else:\n",
    "        full_prompt = \"\\n\\n\".join([\n",
    "            instruction.strip(),\n",
    "            context.strip(),\n",
    "            handling_uncertainty.strip(),\n",
    "            test_data_intro.strip(),\n",
    "            f'\"\"\"\\n{test_data_for_prompt.strip()}\\n\"\"\"',\n",
    "            output_format.strip()\n",
    "        ])\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b10a0a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# collect up to 3 examples per label\u001b[39;00m\n\u001b[32m      9\u001b[39m train_lines = []\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lab \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlabels\u001b[49m:\n\u001b[32m     11\u001b[39m     subset = df_train[df_train[\u001b[33m\"\u001b[39m\u001b[33mlabel_norm\u001b[39m\u001b[33m\"\u001b[39m] == lab]\n\u001b[32m     12\u001b[39m     take = \u001b[38;5;28mmin\u001b[39m(FEW_SHOT_K, \u001b[38;5;28mlen\u001b[39m(subset))\n",
      "\u001b[31mNameError\u001b[39m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== build few-shot training block from train split =====\n",
    "FEW_SHOT_K = 3\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# get labels from train split\n",
    "labels_train = sorted(df_train[\"label_norm\"].astype(str).str.strip().unique().tolist())\n",
    "\n",
    "# collect up to 3 examples per label\n",
    "train_lines = []\n",
    "for lab in labels:\n",
    "    subset = df_train[df_train[\"label_norm\"] == lab]\n",
    "    take = min(FEW_SHOT_K, len(subset))\n",
    "    if take == 0:\n",
    "        continue\n",
    "    sampled = subset.sample(n=take, random_state=RANDOM_STATE)\n",
    "    for j, row in sampled.iterrows():\n",
    "        txt = str(row[\"text\"]).replace(\"\\n\", \" \").strip()\n",
    "        train_lines.append(f\"{j} | {lab} | {txt}\")\n",
    "\n",
    "train_block = \"\\n\".join(train_lines)\n",
    "\n",
    "# ===== build test block as before =====\n",
    "test_lines = []\n",
    "for i, r in df_test.reset_index(drop=True).iterrows():\n",
    "    txt = str(r[\"text\"]).replace(\"\\n\", \" \").strip()\n",
    "    test_lines.append(f\"{i} | {txt}\")\n",
    "test_block = \"\\n\".join(test_lines[:20])  # pick a slice for one batch\n",
    "\n",
    "# ===== build final few-shot prompt =====\n",
    "full_prompt = build_full_prompt(\n",
    "    labels=labels,\n",
    "    test_data_for_prompt=test_block,\n",
    "    is_few_shot=True,\n",
    "    train_data_for_prompt=train_block\n",
    ")\n",
    "\n",
    "print(full_prompt[:15000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44e52cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Zero shot run and evaluation on mental_health_splits_no_stress/test.csv\n",
    "\n",
    "import os, re, json, time, math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ===== inputs already prepared earlier =====\n",
    "# df_test -> loaded from Data_Warehouse/mental_health_splits_no_stress/test.csv\n",
    "# build_full_prompt(...) -> from the previous message\n",
    "\n",
    "# ===== user knobs =====\n",
    "llm = \"DeepSeek\"            # \"GPT\" or DeepSeek\n",
    "BATCH_SIZE = 5        # items per request\n",
    "TEMPERATURE = 0.0\n",
    "TOP_P = 1.0\n",
    "RUN_TAG = \"few_shot_withstress_test\"\n",
    "\n",
    "OUT_DIR = Path(\"llm_runs_few_shot\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===== model routing exactly as you shared =====\n",
    "if llm == \"GPT\":\n",
    "    api_key = os.getenv(\"GPT_API_KEY\")\n",
    "    api_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    model_name = \"gpt-5\"\n",
    "else:\n",
    "    api_key = os.getenv(\"DS_API_KEY\")\n",
    "    api_url = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "    model_name = \"deepseek-chat\" #deepseek-reasoner\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "844f7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== helpers =====\n",
    "def normalize_label(s: str) -> str:\n",
    "    return \" \".join(str(s).strip().lower().split())\n",
    "\n",
    "def canonicalize(pred_label: str, allowed: list[str]) -> str | None:\n",
    "    \"\"\"map a predicted label string to one of the allowed labels\"\"\"\n",
    "    if pred_label is None:\n",
    "        return None\n",
    "    norm = normalize_label(pred_label)\n",
    "    allowed_norm = {normalize_label(a): a for a in allowed}\n",
    "    if norm in allowed_norm:\n",
    "        return allowed_norm[norm]\n",
    "    # simple token overlap fallback\n",
    "    toks = set(norm.split())\n",
    "    if not toks:\n",
    "        return None\n",
    "    best, best_score = None, -1\n",
    "    for a in allowed:\n",
    "        atoks = set(normalize_label(a).split())\n",
    "        score = len(toks & atoks)\n",
    "        if score > best_score:\n",
    "            best, best_score = a, score\n",
    "    return best\n",
    "\n",
    "def build_test_block(df: pd.DataFrame, start: int, end: int) -> str:\n",
    "    lines = []\n",
    "    for i, r in df.iloc[start:end].reset_index(drop=True).iterrows():\n",
    "        txt = str(r[\"text\"]).replace(\"\\n\", \" \").strip()\n",
    "        lines.append(f\"{start+i} | {txt}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "line_pat = re.compile(r\"^\\s*(\\d+)\\s*\\|\\s*([^\\|]+?)\\s*\\|\\s*(.*)$\")\n",
    "\n",
    "def parse_output_to_df(raw_text: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    expects lines: id | predicted_label | text\n",
    "    returns a dataframe with columns: id, pred, text\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for line in raw_text.splitlines():\n",
    "        m = line_pat.match(line)\n",
    "        if not m:\n",
    "            continue\n",
    "        idx = int(m.group(1))\n",
    "        pred = m.group(2).strip()\n",
    "        txt  = m.group(3).strip()\n",
    "        rows.append({\"id\": idx, \"pred_raw\": pred, \"text_out\": txt})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a careful labeler. Classify mental health text. Do not give advice.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P\n",
    "    }\n",
    "    for attempt in range(5):\n",
    "        r = requests.post(api_url, headers=headers, json=data, timeout=120)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        time.sleep(1.5 * (attempt + 1))\n",
    "    raise RuntimeError(f\"LLM API error: {r.status_code} {r.text}\")\n",
    "\n",
    "def evaluate(y_true: list[str], y_pred: list[str], labels: list[str]) -> dict:\n",
    "    return {\n",
    "        \"n\": len(y_true),\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"report\": classification_report(y_true, y_pred, labels=labels, output_dict=True),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred, labels=labels).tolist(),\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7726eb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0-5 got reply in 27.1s\n",
      "Batch 5-10 got reply in 37.0s\n",
      "Batch 10-15 got reply in 15.4s\n",
      "Batch 15-20 got reply in 31.5s\n",
      "Batch 20-25 got reply in 16.8s\n",
      "Batch 25-30 got reply in 24.2s\n",
      "Batch 30-35 got reply in 17.3s\n",
      "Batch 35-40 got reply in 36.5s\n",
      "Batch 40-45 got reply in 18.5s\n",
      "Batch 45-50 got reply in 35.8s\n",
      "Batch 50-55 got reply in 18.7s\n",
      "Batch 55-60 got reply in 27.6s\n",
      "Batch 60-65 got reply in 24.1s\n",
      "Batch 65-70 got reply in 20.3s\n",
      "Batch 70-75 got reply in 12.5s\n",
      "Batch 75-80 got reply in 45.1s\n",
      "Batch 80-85 got reply in 26.2s\n",
      "Batch 85-90 got reply in 21.9s\n",
      "Batch 90-95 got reply in 21.5s\n",
      "Batch 95-100 got reply in 42.7s\n",
      "Batch 100-105 got reply in 31.5s\n",
      "Batch 105-110 got reply in 25.0s\n",
      "Batch 110-115 got reply in 32.1s\n",
      "Batch 115-120 got reply in 35.5s\n",
      "Batch 120-125 got reply in 30.7s\n",
      "Batch 125-130 got reply in 32.2s\n",
      "Batch 130-135 got reply in 17.9s\n",
      "Batch 135-140 got reply in 36.2s\n",
      "Batch 140-145 got reply in 49.9s\n",
      "Batch 145-150 got reply in 47.3s\n",
      "Batch 150-155 got reply in 22.5s\n",
      "Batch 155-160 got reply in 31.4s\n",
      "Batch 160-165 got reply in 44.8s\n",
      "Batch 165-170 got reply in 61.7s\n",
      "Batch 170-175 got reply in 57.8s\n",
      "Batch 175-180 got reply in 36.6s\n",
      "Batch 180-185 got reply in 13.4s\n",
      "Batch 185-190 got reply in 15.3s\n",
      "Batch 190-195 got reply in 20.9s\n",
      "Batch 195-200 got reply in 47.5s\n",
      "Batch 200-205 got reply in 37.0s\n",
      "Batch 205-210 got reply in 31.5s\n",
      "Batch 210-215 got reply in 18.6s\n",
      "Batch 215-220 got reply in 42.1s\n",
      "Batch 220-225 got reply in 31.9s\n",
      "Batch 225-230 got reply in 37.4s\n",
      "Batch 230-235 got reply in 25.6s\n",
      "Batch 235-240 got reply in 14.2s\n",
      "Batch 240-245 got reply in 25.6s\n",
      "Batch 245-250 got reply in 26.6s\n",
      "Batch 250-255 got reply in 22.4s\n",
      "Batch 255-260 got reply in 47.0s\n",
      "Batch 260-265 got reply in 39.6s\n",
      "Batch 265-270 got reply in 47.4s\n",
      "Batch 270-275 got reply in 17.9s\n",
      "Batch 275-280 got reply in 41.5s\n",
      "Batch 280-285 got reply in 39.3s\n",
      "Batch 285-290 got reply in 31.8s\n",
      "Batch 290-295 got reply in 32.9s\n",
      "Batch 295-300 got reply in 18.1s\n",
      "Batch 300-305 got reply in 23.9s\n",
      "Batch 305-310 got reply in 26.5s\n",
      "Batch 310-315 got reply in 30.9s\n",
      "Batch 315-320 got reply in 26.3s\n",
      "Batch 320-325 got reply in 45.1s\n",
      "Batch 325-330 got reply in 50.5s\n",
      "Batch 330-335 got reply in 42.7s\n",
      "Batch 335-340 got reply in 18.4s\n",
      "Batch 340-345 got reply in 52.4s\n",
      "Batch 345-350 got reply in 37.7s\n",
      "Batch 350-355 got reply in 44.9s\n",
      "Batch 355-360 got reply in 38.3s\n",
      "Batch 360-365 got reply in 14.6s\n",
      "Batch 365-370 got reply in 31.2s\n",
      "Batch 370-375 got reply in 36.0s\n",
      "Batch 375-380 got reply in 55.6s\n",
      "Batch 380-385 got reply in 31.0s\n",
      "Batch 385-390 got reply in 18.1s\n",
      "Batch 390-395 got reply in 18.6s\n",
      "Batch 395-400 got reply in 42.8s\n",
      "Batch 400-405 got reply in 30.2s\n",
      "Batch 405-410 got reply in 29.7s\n",
      "Batch 410-415 got reply in 28.6s\n",
      "Batch 415-420 got reply in 17.3s\n",
      "Batch 420-425 got reply in 22.8s\n",
      "Batch 425-430 got reply in 29.9s\n",
      "Batch 430-435 got reply in 31.5s\n",
      "Batch 435-440 got reply in 47.3s\n",
      "Batch 440-445 got reply in 43.7s\n",
      "Batch 445-450 got reply in 44.8s\n",
      "Batch 450-455 got reply in 20.7s\n",
      "Batch 455-460 got reply in 26.7s\n",
      "Batch 460-465 got reply in 45.5s\n",
      "Batch 465-470 got reply in 17.6s\n",
      "Batch 470-475 got reply in 46.4s\n",
      "Batch 475-480 got reply in 43.0s\n",
      "Batch 480-485 got reply in 40.1s\n",
      "Batch 485-490 got reply in 19.5s\n",
      "Batch 490-495 got reply in 28.1s\n",
      "Batch 495-500 got reply in 22.2s\n",
      "Batch 500-505 got reply in 36.9s\n",
      "Batch 505-510 got reply in 28.9s\n",
      "Batch 510-515 got reply in 32.0s\n",
      "Batch 515-520 got reply in 21.4s\n",
      "Batch 520-525 got reply in 17.7s\n",
      "Batch 525-530 got reply in 35.5s\n",
      "Batch 530-535 got reply in 20.5s\n",
      "Batch 535-540 got reply in 24.2s\n",
      "Batch 540-545 got reply in 24.0s\n",
      "Batch 545-550 got reply in 40.1s\n",
      "Batch 550-555 got reply in 18.1s\n",
      "Batch 555-560 got reply in 43.2s\n",
      "Batch 560-565 got reply in 20.8s\n",
      "Batch 565-570 got reply in 21.1s\n",
      "Batch 570-575 got reply in 30.3s\n",
      "Batch 575-580 got reply in 32.4s\n",
      "Batch 580-585 got reply in 35.0s\n",
      "Batch 585-590 got reply in 47.0s\n",
      "Batch 590-595 got reply in 23.5s\n",
      "Batch 595-600 got reply in 27.5s\n",
      "Batch 600-605 got reply in 19.9s\n",
      "Batch 605-610 got reply in 16.2s\n",
      "Batch 610-615 got reply in 18.8s\n",
      "Batch 615-620 got reply in 21.1s\n",
      "Batch 620-625 got reply in 24.0s\n",
      "Batch 625-630 got reply in 24.1s\n",
      "Batch 630-635 got reply in 30.8s\n",
      "Batch 635-640 got reply in 32.7s\n",
      "Batch 640-645 got reply in 21.8s\n",
      "Batch 645-650 got reply in 49.7s\n",
      "Batch 650-655 got reply in 20.2s\n",
      "Batch 655-660 got reply in 30.6s\n",
      "Batch 660-665 got reply in 16.8s\n",
      "Batch 665-670 got reply in 47.5s\n",
      "Batch 670-675 got reply in 37.3s\n",
      "Batch 675-680 got reply in 28.6s\n",
      "Batch 680-685 got reply in 14.2s\n",
      "Batch 685-690 got reply in 15.7s\n",
      "Batch 690-695 got reply in 17.1s\n",
      "Batch 695-697 got reply in 23.1s\n",
      "Total wall time 4251.3s for 697 items\n",
      "\n",
      "Overall metrics\n",
      "{'accuracy': 0.594, 'f1_macro': 0.5774, 'f1_weighted': 0.5943}\n",
      "\n",
      "Label wise metrics on TEST\n",
      "     anxiety  p=0.369  r=0.738  f1=0.492  support=42\n",
      "  depression  p=0.738  r=0.595  f1=0.659  support=232\n",
      "        none  p=0.450  r=0.944  f1=0.609  support=71\n",
      "        ptsd  p=0.548  r=0.561  f1=0.554  support=41\n",
      "      stress  p=0.803  r=0.414  f1=0.547  support=227\n",
      "     suicide  p=0.517  r=0.726  f1=0.604  support=84\n"
     ]
    }
   ],
   "source": [
    "# ===== label list from split =====\n",
    "label_list = sorted(df_test[\"label_norm\"].astype(str).str.strip().unique().tolist())\n",
    "\n",
    "# ===== batch over test set =====\n",
    "n = len(df_test)\n",
    "all_preds = {}     # id -> predicted label\n",
    "all_conf = {}      # optional future use\n",
    "start_time = time.time()\n",
    "\n",
    "for b in range(0, n, BATCH_SIZE):\n",
    "    e = min(b + BATCH_SIZE, n)\n",
    "    test_block = build_test_block(df_test, b, e)\n",
    "    prompt = build_full_prompt(labels=label_list, test_data_for_prompt=test_block, is_few_shot=False)\n",
    "\n",
    "    t0 = time.time()\n",
    "    raw = call_llm(prompt)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"Batch {b}-{e} got reply in {dt:.1f}s\")\n",
    "\n",
    "    df_out = parse_output_to_df(raw)\n",
    "\n",
    "    # map back to predictions\n",
    "    for _, row in df_out.iterrows():\n",
    "        rid = int(row[\"id\"])\n",
    "        pred_norm = canonicalize(row[\"pred_raw\"], label_list)\n",
    "        if pred_norm is None:\n",
    "            pred_norm = label_list[0]  # fallback\n",
    "        all_preds[rid] = pred_norm\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Total wall time {elapsed:.1f}s for {n} items\")\n",
    "\n",
    "# ===== build final predictions frame aligned with ground truth =====\n",
    "df_eval = df_test.copy().reset_index(drop=True)\n",
    "df_eval[\"pred\"] = [all_preds.get(i, label_list[0]) for i in range(len(df_eval))]\n",
    "\n",
    "# ===== metrics =====\n",
    "metrics = evaluate(\n",
    "    y_true=df_eval[\"label_norm\"].tolist(),\n",
    "    y_pred=df_eval[\"pred\"].tolist(),\n",
    "    labels=label_list\n",
    ")\n",
    "\n",
    "# ===== save artifacts =====\n",
    "pred_out = OUT_DIR / f\"pred_{RUN_TAG}_{'gpt_5' if llm=='GPT' else 'deepseek'}.csv\"\n",
    "df_eval.to_csv(pred_out, index=False)\n",
    "\n",
    "metrics_out = OUT_DIR / f\"metrics_{RUN_TAG}_{'gpt_5' if llm=='GPT' else 'deepseek'}.json\"\n",
    "with open(metrics_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# ===== pretty print summary =====\n",
    "print(\"\\nOverall metrics\")\n",
    "print({k: round(v, 4) for k, v in metrics.items() if isinstance(v, float)})\n",
    "\n",
    "print(\"\\nLabel wise metrics on TEST\")\n",
    "rep = metrics[\"report\"]\n",
    "for lab in label_list:\n",
    "    if lab in rep:\n",
    "        lr = rep[lab]\n",
    "        print(f\"{lab:>12}  p={lr['precision']:.3f}  r={lr['recall']:.3f}  f1={lr['f1-score']:.3f}  support={int(lr['support'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc122e",
   "metadata": {},
   "source": [
    "### GPT-5 No Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.6745, 'f1_macro': 0.6729, 'f1_weighted': 0.6742}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.571  r=0.762  f1=0.653  support=42\n",
    "  depression  p=0.906  r=0.539  f1=0.676  support=232\n",
    "        none  p=0.600  r=0.972  f1=0.742  support=71\n",
    "        ptsd  p=0.880  r=0.537  f1=0.667  support=41\n",
    "     suicide  p=0.507  r=0.821  f1=0.627  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90cb80c",
   "metadata": {},
   "source": [
    "### GPT-4.1 No Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.6957, 'f1_macro': 0.6834, 'f1_weighted': 0.6958}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.574  r=0.738  f1=0.646  support=42\n",
    "  depression  p=0.873  r=0.595  f1=0.708  support=232\n",
    "        none  p=0.663  r=0.972  f1=0.789  support=71\n",
    "        ptsd  p=0.840  r=0.512  f1=0.636  support=41\n",
    "     suicide  p=0.527  r=0.810  f1=0.638  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c4aa5",
   "metadata": {},
   "source": [
    "### DeepSeek No Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.7043, 'f1_macro': 0.6878, 'f1_weighted': 0.7056}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.493  r=0.810  f1=0.613  support=42\n",
    "  depression  p=0.886  r=0.603  f1=0.718  support=232\n",
    "        none  p=0.719  r=0.972  f1=0.826  support=71\n",
    "        ptsd  p=0.759  r=0.537  f1=0.629  support=41\n",
    "     suicide  p=0.559  r=0.786  f1=0.653  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf63c84",
   "metadata": {},
   "source": [
    "### GPT-4.1 With Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.5839, 'f1_macro': 0.5715, 'f1_weighted': 0.5764}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.375  r=0.714  f1=0.492  support=42\n",
    "  depression  p=0.697  r=0.595  f1=0.642  support=232\n",
    "        none  p=0.446  r=0.986  f1=0.614  support=71\n",
    "        ptsd  p=0.561  r=0.561  f1=0.561  support=41\n",
    "      stress  p=0.888  r=0.348  f1=0.500  support=227\n",
    "     suicide  p=0.508  r=0.798  f1=0.620  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d691943",
   "metadata": {},
   "source": [
    "### GPT-5 with Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.5753, 'f1_macro': 0.5597, 'f1_weighted': 0.58}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.326  r=0.714  f1=0.448  support=42\n",
    "  depression  p=0.812  r=0.522  f1=0.635  support=232\n",
    "        none  p=0.402  r=0.986  f1=0.571  support=71\n",
    "        ptsd  p=0.600  r=0.512  f1=0.553  support=41\n",
    "      stress  p=0.858  r=0.401  f1=0.547  support=227\n",
    "     suicide  p=0.482  r=0.810  f1=0.604  support=84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955dd4b1",
   "metadata": {},
   "source": [
    "### DeepSeek with Stress\n",
    "\n",
    "Overall metrics\n",
    "{'accuracy': 0.594, 'f1_macro': 0.5774, 'f1_weighted': 0.5943}\n",
    "\n",
    "Label wise metrics on TEST\n",
    "     anxiety  p=0.369  r=0.738  f1=0.492  support=42\n",
    "  depression  p=0.738  r=0.595  f1=0.659  support=232\n",
    "        none  p=0.450  r=0.944  f1=0.609  support=71\n",
    "        ptsd  p=0.548  r=0.561  f1=0.554  support=41\n",
    "      stress  p=0.803  r=0.414  f1=0.547  support=227\n",
    "     suicide  p=0.517  r=0.726  f1=0.604  support=84"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_gpu_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
