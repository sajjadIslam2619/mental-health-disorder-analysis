{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45068561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aac231b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11a10d4c570>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Config & paths\n",
    "# ---------------------------\n",
    "def find_data_warehouse(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        dw = p / \"Data_Warehouse\"\n",
    "        if dw.exists():\n",
    "            return dw\n",
    "    raise FileNotFoundError(\"Could not locate a folder named Data_Warehouse\")\n",
    "\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "DATA_WAREHOUSE = find_data_warehouse(SCRIPT_DIR)\n",
    "SPLIT_DIR = DATA_WAREHOUSE / \"mental_health_splits_no_stress\"\n",
    "MODEL_BASE = SPLIT_DIR / \"all_roberta_large_v1_multiclass\"\n",
    "BEST_DIR = MODEL_BASE / \"best\"\n",
    "MODEL_DIR = BEST_DIR if BEST_DIR.exists() else MODEL_BASE\n",
    "\n",
    "# defaults (can override via CLI)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = 256             # shorter to save mem\n",
    "PRED_BATCH = 4\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75b65f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# IO helpers\n",
    "# ---------------------------\n",
    "def load_test_and_mapping():\n",
    "    test_path = SPLIT_DIR / \"test.csv\"\n",
    "    if not test_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing test.csv at {test_path}\")\n",
    "    df_test = pd.read_csv(test_path)\n",
    "\n",
    "    label_map_path = SPLIT_DIR / \"label_classes.csv\"\n",
    "    if label_map_path.exists():\n",
    "        df_map = pd.read_csv(label_map_path, header=None)\n",
    "        if df_map.shape[1] == 2:\n",
    "            class_to_id = {str(df_map.iloc[i, 0]).strip().lower(): int(df_map.iloc[i, 1]) for i in range(len(df_map))}\n",
    "        else:\n",
    "            class_to_id = {str(df_map.iloc[i, -2]).strip().lower(): int(df_map.iloc[i, -1]) for i in range(len(df_map))}\n",
    "    else:\n",
    "        uniq = sorted([lbl for lbl in df_test[\"label\"].astype(str).str.lower().unique() if lbl != \"none\"])\n",
    "        class_to_id = {lbl: i for i, lbl in enumerate(uniq)}\n",
    "        class_to_id[\"none\"] = 4\n",
    "\n",
    "    id_to_class = {v: k for k, v in class_to_id.items()}\n",
    "    if \"suicide\" not in class_to_id or \"depression\" not in class_to_id:\n",
    "        raise ValueError(f\"Mapping must contain 'suicide' and 'depression'. Found: {list(class_to_id.keys())}\")\n",
    "    return df_test, class_to_id, id_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc04d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Model + prediction\n",
    "# ---------------------------\n",
    "def load_model_and_tokenizer():\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "    mdl.to(DEVICE); mdl.eval()\n",
    "    return tok, mdl\n",
    "\n",
    "def predict_batch(texts, tokenizer, model, batch_size=PRED_BATCH):\n",
    "    preds, probs = [], []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, truncation=True, max_length=MAX_LEN,\n",
    "                        return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            p = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            y = p.argmax(axis=1)\n",
    "        preds.extend(y.tolist()); probs.extend(p.tolist())\n",
    "        # cleanup\n",
    "        del enc, logits\n",
    "        import gc; gc.collect()\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    return np.array(preds), np.array(probs)\n",
    "\n",
    "# ---------------------------\n",
    "# Text cleaning (optional)\n",
    "# ---------------------------\n",
    "ARTIFACT_RX = re.compile(r\"[^\\w\\s'’\\-\\.]+\")  # keep basic punctuation and apostrophes\n",
    "def clean_text(t: str) -> str:\n",
    "    if not isinstance(t, str): return \"\"\n",
    "    # fix common mojibake seen in your data\n",
    "    t = (t.replace(\"âĢĻ\", \"'\")\n",
    "           .replace(\"âĿ¤ï¸ı\", \"\")\n",
    "           .replace(\"Ċ\", \"\"))\n",
    "    t = ARTIFACT_RX.sub(\" \", t)\n",
    "    # collapse whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "# ---------------------------\n",
    "# KeyBERT extraction\n",
    "# ---------------------------\n",
    "def build_keybert(backend_model: str = \"all-MiniLM-L6-v2\"):\n",
    "    #st = SentenceTransformer(backend_model)\n",
    "    st = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    return KeyBERT(model=st)\n",
    "\n",
    "def extract_keyphrases_for_docs(\n",
    "    docs,\n",
    "    kb: KeyBERT,\n",
    "    top_n=10,\n",
    "    ngram_range=(1, 3),\n",
    "    use_mmr=True,\n",
    "    diversity=0.7,\n",
    "    min_df_len=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Run KeyBERT per doc, aggregate by phrase:\n",
    "      - average score\n",
    "      - total score\n",
    "      - frequency\n",
    "    Returns a DataFrame sorted by total_score desc.\n",
    "    \"\"\"\n",
    "    agg_score = defaultdict(float)\n",
    "    agg_count = Counter()\n",
    "\n",
    "    for d in docs:\n",
    "        d = clean_text(d or \"\")\n",
    "        if len(d.split()) < min_df_len:\n",
    "            continue\n",
    "        try:\n",
    "            kws = kb.extract_keywords(\n",
    "                d,\n",
    "                keyphrase_ngram_range=ngram_range,\n",
    "                stop_words=\"english\",\n",
    "                use_maxsum=False,\n",
    "                use_mmr=use_mmr,\n",
    "                diversity=diversity,\n",
    "                top_n=top_n\n",
    "            )\n",
    "            for phrase, score in kws:\n",
    "                phrase_c = phrase.strip().lower()\n",
    "                if not phrase_c: continue\n",
    "                agg_score[phrase_c] += float(score)\n",
    "                agg_count[phrase_c] += 1\n",
    "        except Exception:\n",
    "            # skip problematic doc, continue\n",
    "            continue\n",
    "\n",
    "    rows = []\n",
    "    for ph in agg_score:\n",
    "        rows.append({\n",
    "            \"phrase\": ph,\n",
    "            \"freq\": agg_count[ph],\n",
    "            \"avg_score\": agg_score[ph] / max(1, agg_count[ph]),\n",
    "            \"total_score\": agg_score[ph],\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values([\"total_score\", \"freq\", \"avg_score\"], ascending=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adbdd028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Buckets & main\n",
    "# ---------------------------\n",
    "def main(args):\n",
    "    print(\"Using model dir:\", MODEL_DIR)\n",
    "    out_dir = MODEL_DIR / \"keybert_phrases\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_test, class_to_id, id_to_class = load_test_and_mapping()\n",
    "    tokenizer, model = load_model_and_tokenizer()\n",
    "\n",
    "    texts = df_test[\"text\"].astype(str).tolist()\n",
    "    y_true = df_test[\"label\"].astype(str).str.lower().map(class_to_id).to_numpy()\n",
    "    y_pred, y_prob = predict_batch(texts, tokenizer, model, batch_size=args.pred_batch)\n",
    "\n",
    "    idx_dep = class_to_id[\"depression\"]\n",
    "    idx_su = class_to_id[\"suicide\"]\n",
    "\n",
    "    # Buckets requested\n",
    "    FN_suicide   = np.where((y_true == idx_su) & (y_pred == idx_dep))[0]   # suicide but predicted depression\n",
    "    TP_suicide   = np.where((y_true == idx_su) & (y_pred == idx_su))[0]   # suicide and predicted suicide\n",
    "    TN_depr      = np.where((y_true == idx_dep) & (y_pred == idx_dep))[0] # depression but predicted depression\n",
    "    FP_dep_to_su = np.where((y_true == idx_dep) & (y_pred == idx_su))[0]  # depression and predicted suicide\n",
    "\n",
    "    # sample to cap runtime if desired\n",
    "    def sample_idx(arr, k):\n",
    "        arr = list(arr)\n",
    "        random.shuffle(arr)\n",
    "        return arr[:min(len(arr), k)]\n",
    "\n",
    "    FN_suicide_s   = sample_idx(FN_suicide,   args.max_docs)\n",
    "    TP_suicide_s   = sample_idx(TP_suicide,   args.max_docs)\n",
    "    TN_depr_s      = sample_idx(TN_depr,      args.max_docs)\n",
    "    FP_dep_to_su_s = sample_idx(FP_dep_to_su, args.max_docs)\n",
    "\n",
    "    # build docs lists\n",
    "    docs_FN   = [texts[i] for i in FN_suicide_s]\n",
    "    docs_TP   = [texts[i] for i in TP_suicide_s]\n",
    "    docs_TN   = [texts[i] for i in TN_depr_s]\n",
    "    docs_FP   = [texts[i] for i in FP_dep_to_su_s]\n",
    "\n",
    "    # KeyBERT\n",
    "    kb = build_keybert(args.backend)\n",
    "\n",
    "    print(f\"KeyBERT on FN_suicide docs: {len(docs_FN)}\")\n",
    "    df_FN = extract_keyphrases_for_docs(\n",
    "        docs_FN, kb, top_n=args.topn, ngram_range=tuple(args.ngram), use_mmr=not args.no_mmr,\n",
    "        diversity=args.diversity, min_df_len=args.min_doc_len\n",
    "    )\n",
    "    df_FN.head(args.save_k).to_csv(out_dir / \"keybert_FN_suicide_top.csv\", index=False)\n",
    "\n",
    "    print(f\"KeyBERT on TP_suicide docs: {len(docs_TP)}\")\n",
    "    df_TP = extract_keyphrases_for_docs(\n",
    "        docs_TP, kb, top_n=args.topn, ngram_range=tuple(args.ngram), use_mmr=not args.no_mmr,\n",
    "        diversity=args.diversity, min_df_len=args.min_doc_len\n",
    "    )\n",
    "    df_TP.head(args.save_k).to_csv(out_dir / \"keybert_TP_suicide_top.csv\", index=False)\n",
    "\n",
    "    print(f\"KeyBERT on TN_depression docs: {len(docs_TN)}\")\n",
    "    df_TN = extract_keyphrases_for_docs(\n",
    "        docs_TN, kb, top_n=args.topn, ngram_range=tuple(args.ngram), use_mmr=not args.no_mmr,\n",
    "        diversity=args.diversity, min_df_len=args.min_doc_len\n",
    "    )\n",
    "    df_TN.head(args.save_k).to_csv(out_dir / \"keybert_TN_depression_top.csv\", index=False)\n",
    "\n",
    "    print(f\"KeyBERT on FP_depression_to_suicide docs: {len(docs_FP)}\")\n",
    "    df_FP = extract_keyphrases_for_docs(\n",
    "        docs_FP, kb, top_n=args.topn, ngram_range=tuple(args.ngram), use_mmr=not args.no_mmr,\n",
    "        diversity=args.diversity, min_df_len=args.min_doc_len\n",
    "    )\n",
    "    df_FP.head(args.save_k).to_csv(out_dir / \"keybert_FP_depression_to_suicide_top.csv\", index=False)\n",
    "\n",
    "    print(\"Saved CSVs to:\", out_dir.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bde0915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model dir: d:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_no_stress\\all_roberta_large_v1_multiclass\\best\n",
      "KeyBERT on FN_suicide docs: 19\n",
      "KeyBERT on TP_suicide docs: 64\n",
      "KeyBERT on TN_depression docs: 207\n",
      "KeyBERT on FP_depression_to_suicide docs: 23\n",
      "Saved CSVs to: D:\\Sajjad-Workspace\\PSS_XAI\\Data_Process\\Data_Warehouse\\mental_health_splits_no_stress\\all_roberta_large_v1_multiclass\\best\\keybert_phrases\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    #backend = \"all-MiniLM-L6-v2\"\n",
    "    backend = \"all-mpnet-base-v2\"\n",
    "    topn = 8\n",
    "    ngram = [1, 3]\n",
    "    diversity = 0.7\n",
    "    no_mmr = False\n",
    "    max_docs = 10000   # use all\n",
    "    save_k = 50\n",
    "    pred_batch = 4\n",
    "    min_doc_len = 10\n",
    "\n",
    "args = Args()\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_gpu_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
